{"pages":[{"title":"Image Processing Cheatsheet from PyImageSearch","url":"/2017/10/19/Learn-OpenCV-1/","text":"This blog summarizes image processing methods from pyimagesearch. All the source codes and pictures come from the blog and I won’t take any credit for anything. Image ProcessingReferencesBlur detection with OpenCV OpenCV Gamma Correction Codes Blur Detection 1cv2.Laplacian(image, cv2.CV_64F).var() Gamma Correction 123456789def adjust_gamma(image, gamma=1.0): # build a lookup table mapping the pixel values [0, 255] to # their adjusted gamma values invGamma = 1.0 / gamma table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(&quot;uint8&quot;) # apply gamma correction using the lookup table return cv2.LUT(image, table) Object DetectionReferencesDetecting Circles in Images using OpenCV and Hough Circles Detecting Barcodes in Images with Python and OpenCV Target acquired: Finding targets in drone and quadcopter video streams using Python and OpenCV Recognizing digits with OpenCV and Python Detecting machine-readable zones in passport images Bubble sheet multiple choice scanner and test grader using OMR, Python and OpenCV Codes Detecting Circles 1234567891011121314# detect circles in the imagecircles = cv2.HoughCircles(gray, cv2.cv.CV_HOUGH_GRADIENT, 1.2, 100) # ensure at least some circles were foundif circles is not None: # convert the (x, y) coordinates and radius of the circles to integers circles = np.round(circles[0, :]).astype(&quot;int&quot;) # loop over the (x, y) coordinates and radius of the circles for (x, y, r) in circles: # draw the circle in the output image, then draw a rectangle # corresponding to the center of the circle cv2.circle(output, (x, y), r, (0, 255, 0), 4) cv2.rectangle(output, (x - 5, y - 5), (x + 5, y + 5), (0, 128, 255), -1) Detect squares in a video 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# load the videocamera = cv2.VideoCapture(args[&quot;video&quot;]) # keep loopingwhile True: # grab the current frame and initialize the status text (grabbed, frame) = camera.read() status = &quot;No Targets&quot; # check to see if we have reached the end of the # video if not grabbed: break # convert the frame to grayscale, blur it, and detect edges gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) blurred = cv2.GaussianBlur(gray, (7, 7), 0) edged = cv2.Canny(blurred, 50, 150) # find contours in the edge map (cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) for c in cnts: # approximate the contour peri = cv2.arcLength(c, True) approx = cv2.approxPolyDP(c, 0.01 * peri, True) # ensure that the approximated contour is &quot;roughly&quot; rectangular if len(approx) &gt;= 4 and len(approx) &lt;= 6: # compute the bounding box of the approximated contour and # use the bounding box to compute the aspect ratio (x, y, w, h) = cv2.boundingRect(approx) aspectRatio = w / float(h) # compute the solidity of the original contour area = cv2.contourArea(c) hullArea = cv2.contourArea(cv2.convexHull(c)) solidity = area / float(hullArea) # compute whether or not the width and height, solidity, and # aspect ratio of the contour falls within appropriate bounds keepDims = w &gt; 25 and h &gt; 25 keepSolidity = solidity &gt; 0.9 keepAspectRatio = aspectRatio &gt;= 0.8 and aspectRatio &lt;= 1.2 # ensure that the contour passes all our tests if keepDims and keepSolidity and keepAspectRatio: # draw an outline around the target and update the status # text cv2.drawContours(frame, [approx], -1, (0, 0, 255), 4) status = &quot;Target(s) Acquired&quot; # draw the status text on the frame cv2.putText(frame, status, (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2) # show the frame and record if a key is pressed cv2.imshow(&quot;Frame&quot;, frame) key = cv2.waitKey(1) &amp; 0xFF # if the &apos;q&apos; key is pressed, stop the loop if key == ord(&quot;q&quot;): break# cleanup the camera and close any open windowscamera.release()cv2.destroyAllWindows() Detectin Texture (Barcode in this case) 12345678910111213141516171819202122232425262728293031323334353637383940# compute the Scharr gradient magnitude representation of the images# in both the x and y directiongradX = cv2.Sobel(gray, ddepth = cv2.cv.CV_32F, dx = 1, dy = 0, ksize = -1)gradY = cv2.Sobel(gray, ddepth = cv2.cv.CV_32F, dx = 0, dy = 1, ksize = -1) # subtract the y-gradient from the x-gradient# to find regions that have high horizontal and low vertical gradients.gradient = cv2.subtract(gradX, gradY)gradient = cv2.convertScaleAbs(gradient)# blur and threshold the image# smooth out high frequency noise in the gradient blurred = cv2.blur(gradient, (9, 9))(_, thresh) = cv2.threshold(blurred, 225, 255, cv2.THRESH_BINARY)# construct a closing kernel and apply it to the thresholded image# this kernel has a width that is larger than the height# thus close the gaps between vertical stripes of the barcodekernel = cv2.getStructuringElement(cv2.MORPH_RECT, (21, 7))closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)# perform a series of erosions and dilations# erode the white pixels in the image, thus removing the small blobs# dilate the remaining white pixels and grow the white regions back out.closed = cv2.erode(closed, None, iterations = 4)closed = cv2.dilate(closed, None, iterations = 4)# find the contours in the thresholded image, then sort the contours# by their area, keeping only the largest one(cnts, _) = cv2.findContours(closed.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)c = sorted(cnts, key = cv2.contourArea, reverse = True)[0]# compute the rotated bounding box of the largest contourrect = cv2.minAreaRect(c)box = np.int0(cv2.cv.BoxPoints(rect))# draw a bounding box arounded the detected barcode and display the# imagecv2.drawContours(image, [box], -1, (0, 255, 0), 3) Detect Digits Areas 1234567891011121314151617181920212223242526# extract the thermostat display, apply a perspective transform to itthresh = cv2.threshold(warped, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (1, 5))thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)# find contours in the thresholded image, then initialize the# digit contours listscnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if imutils.is_cv2() else cnts[1]digitCnts = [] # loop over the digit area candidatesfor c in cnts: # compute the bounding box of the contour (x, y, w, h) = cv2.boundingRect(c) # if the contour is sufficiently large, it must be a digit if w &gt;= 15 and (h &gt;= 30 and h &lt;= 40): digitCnts.append(c) # sort the contours from left-to-right, then initialize the# actual digits themselvesdigitCnts = contours.sort_contours(digitCnts, method=&quot;left-to-right&quot;)[0] Detect Machine Readable Zones 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# initialize a rectangular and square structuring kernelrectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (13, 5))sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (21, 21))image = cv2.imread(imagePath)image = imutils.resize(image, height=600)gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)# smooth the image using a 3x3 Gaussian, then apply the blackhat# morphological operator to find dark regions on a light backgroundgray = cv2.GaussianBlur(gray, (3, 3), 0)blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, rectKernel)# compute the Scharr gradient of the blackhat image and scale the# result into the range [0, 255]# extremely helpful in reducing false-positive MRZ detectionsgradX = cv2.Sobel(blackhat, ddepth=cv2.CV_32F, dx=1, dy=0, ksize=-1)gradX = np.absolute(gradX)(minVal, maxVal) = (np.min(gradX), np.max(gradX))gradX = (255 * ((gradX - minVal) / (maxVal - minVal))).astype(&quot;uint8&quot;)# apply a closing operation using the rectangular kernel to close# gaps in between letters -- then apply Otsu&apos;s thresholding methodgradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, rectKernel)thresh = cv2.threshold(gradX, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]# perform another closing operation, this time using the square# kernel to close gaps between lines of the MRZ, then perform a# series of erosions to break apart connected componentsthresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel)thresh = cv2.erode(thresh, None, iterations=4)# during thresholding, it&apos;s possible that border pixels were# included in the thresholding, so let&apos;s set 5% of the left and# right borders to zerop = int(image.shape[1] * 0.05)thresh[:, 0:p] = 0thresh[:, image.shape[1] - p:] = 0# find contours in the thresholded image and sort them by their# sizecnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)[-2]cnts = sorted(cnts, key=cv2.contourArea, reverse=True)# loop over the contoursfor c in cnts: # compute the bounding box of the contour and use the contour to # compute the aspect ratio and coverage ratio of the bounding box # width to the width of the image (x, y, w, h) = cv2.boundingRect(c) ar = w / float(h) crWidth = w / float(gray.shape[1]) # check to see if the aspect ratio and coverage width are within # acceptable criteria if ar &gt; 5 and crWidth &gt; 0.75: # pad the bounding box since we applied erosions and now need # to re-grow it pX = int((x + w) * 0.03) pY = int((y + h) * 0.03) (x, y) = (x - pX, y - pY) (w, h) = (w + (pX * 2), h + (pY * 2)) # extract the ROI from the image and draw a bounding box # surrounding the MRZ roi = image[y:y + h, x:x + w].copy() cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2) break Object TransformationReferences4 Point OpenCV getPerspective Transform Example How to Build a Kick-Ass Mobile Document Scanner in Just 5 Minutes Text skew correction with OpenCV and Python Seam carving with OpenCV, Python, and scikit-image Codes Four Point Transformation 1234567891011121314151617181920212223242526272829### Find Four Points and Call Function# convert the image to grayscale, blur it, and find edges# in the imagegray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)gray = cv2.GaussianBlur(gray, (5, 5), 0)edged = cv2.Canny(gray, 75, 200)# find the contours in the edged image, keeping only the# largest ones, and initialize the screen contour(cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5] # loop over the contoursfor c in cnts: # approximate the contour peri = cv2.arcLength(c, True) approx = cv2.approxPolyDP(c, 0.02 * peri, True) # if our approximated contour has four points, then we # can assume that we have found our screen if len(approx) == 4: screenCnt = approx break # show the contour (outline) of the piece of paperprint &quot;STEP 2: Find contours of paper&quot;cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def order_points(pts): # initialzie a list of coordinates that will be ordered # such that the first entry in the list is the top-left, # the second entry is the top-right, the third is the # bottom-right, and the fourth is the bottom-left rect = np.zeros((4, 2), dtype = &quot;float32&quot;) # the top-left point will have the smallest sum, whereas # the bottom-right point will have the largest sum s = pts.sum(axis = 1) rect[0] = pts[np.argmin(s)] rect[2] = pts[np.argmax(s)] # now, compute the difference between the points, the # top-right point will have the smallest difference, # whereas the bottom-left will have the largest difference diff = np.diff(pts, axis = 1) rect[1] = pts[np.argmin(diff)] rect[3] = pts[np.argmax(diff)] # return the ordered coordinates return rectdef four_point_transform(image, pts): # obtain a consistent order of the points and unpack them # individually rect = order_points(pts) (tl, tr, br, bl) = rect # compute the width of the new image, which will be the # maximum distance between bottom-right and bottom-left # x-coordiates or the top-right and top-left x-coordinates widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2)) widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2)) maxWidth = max(int(widthA), int(widthB)) # compute the height of the new image, which will be the # maximum distance between the top-right and bottom-right # y-coordinates or the top-left and bottom-left y-coordinates heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2)) heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2)) maxHeight = max(int(heightA), int(heightB)) # now that we have the dimensions of the new image, construct # the set of destination points to obtain a &quot;birds eye view&quot;, # (i.e. top-down view) of the image, again specifying points # in the top-left, top-right, bottom-right, and bottom-left # order dst = np.array([ [0, 0], [maxWidth - 1, 0], [maxWidth - 1, maxHeight - 1], [0, maxHeight - 1]], dtype = &quot;float32&quot;) # compute the perspective transform matrix and then apply it M = cv2.getPerspectiveTransform(rect, dst) warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight)) # return the warped image return warped Text Skew Correction 123456789101112131415161718192021222324252627282930313233343536# convert the image to grayscale and flip the foreground# and background to ensure foreground is now &quot;white&quot; and# the background is &quot;black&quot;gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)gray = cv2.bitwise_not(gray) # threshold the image, setting all foreground pixels to# 255 and all background pixels to 0thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]# grab the (x, y) coordinates of all pixel values that# are greater than zero, then use these coordinates to# compute a rotated bounding box that contains all# coordinatescoords = np.column_stack(np.where(thresh &gt; 0))angle = cv2.minAreaRect(coords)[-1] # the `cv2.minAreaRect` function returns values in the# range [-90, 0); as the rectangle rotates clockwise the# returned angle trends to 0 -- in this special case we# need to add 90 degrees to the angleif angle &lt; -45: angle = -(90 + angle) # otherwise, just take the inverse of the angle to make# it positiveelse: angle = -angle# rotate the image to deskew it(h, w) = image.shape[:2]center = (w // 2, h // 2)M = cv2.getRotationMatrix2D(center, angle, 1.0)rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE) Template MatchingReferencesMulti-scale Template Matching using Python and OpenCV Image Difference with OpenCV and Python Codes Robust Template Matching 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&quot;&quot;&quot;1. Loop over the input image at multiple scales (i.e. make the input image progressively smaller and smaller).2. Apply template matching using cv2.matchTemplate and keep track of the match with the largest correlation coefficient (along with the x, y-coordinates of the region with the largest correlation coefficient).3. After looping over all scales, take the region with the largest correlation coefficient and use that as your “matched” region.While we can handle variations in translation and scaling, our approach will not be robust to changes in rotation or non-affine transformations.If we are concerned about rotation on non-affine transformations we are better off taking the time to detect keypoints, extract local invariant descriptors, and apply keypoint matching.&quot;&quot;&quot;template = cv2.imread(args[&quot;template&quot;])template = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)template = cv2.Canny(template, 50, 200)(tH, tW) = template.shape[:2]image = cv2.imread(imagePath) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) found = None # loop over the scales of the image for scale in np.linspace(0.2, 1.0, 20)[::-1]: # resize the image according to the scale, and keep track # of the ratio of the resizing resized = imutils.resize(gray, width = int(gray.shape[1] * scale)) r = gray.shape[1] / float(resized.shape[1]) # if the resized image is smaller than the template, then break # from the loop if resized.shape[0] &lt; tH or resized.shape[1] &lt; tW: break # detect edges in the resized, grayscale image and apply template # matching to find the template in the image edged = cv2.Canny(resized, 50, 200) result = cv2.matchTemplate(edged, template, cv2.TM_CCOEFF) (_, maxVal, _, maxLoc) = cv2.minMaxLoc(result) # check to see if the iteration should be visualized if args.get(&quot;visualize&quot;, False): # draw a bounding box around the detected region clone = np.dstack([edged, edged, edged]) cv2.rectangle(clone, (maxLoc[0], maxLoc[1]), (maxLoc[0] + tW, maxLoc[1] + tH), (0, 0, 255), 2) cv2.imshow(&quot;Visualize&quot;, clone) cv2.waitKey(0) # if we have found a new maximum correlation value, then ipdate # the bookkeeping variable if found is None or maxVal &gt; found[0]: found = (maxVal, maxLoc, r) # unpack the bookkeeping varaible and compute the (x, y) coordinates # of the bounding box based on the resized ratio (_, maxLoc, r) = found (startX, startY) = (int(maxLoc[0] * r), int(maxLoc[1] * r)) (endX, endY) = (int((maxLoc[0] + tW) * r), int((maxLoc[1] + tH) * r)) # draw a bounding box around the detected result and display the image cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2) cv2.imshow(&quot;Image&quot;, image) Image Difference 12345678910111213141516171819202122# compute the Structural Similarity Index (SSIM) between the two# images, ensuring that the difference image is returned(score, diff) = compare_ssim(grayA, grayB, full=True)diff = (diff * 255).astype(&quot;uint8&quot;)print(&quot;SSIM: &#123;&#125;&quot;.format(score))# threshold the difference image, followed by finding contours to# obtain the regions of the two input images that differthresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if imutils.is_cv2() else cnts[1]# loop over the contoursfor c in cnts: # compute the bounding box of the contour and then draw the # bounding box on both input images to represent where the two # images differ (x, y, w, h) = cv2.boundingRect(c) cv2.rectangle(imageA, (x, y), (x + w, y + h), (0, 0, 255), 2) cv2.rectangle(imageB, (x, y), (x + w, y + h), (0, 0, 255), 2) ##Color Manipulation ReferencesFinding the Brightest Spot in an Image using Python and OpenCV OpenCV and Python K-Means Color Clustering Color Quantization with OpenCV using K-Means Clustering Codes Brightest color: 12345gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)gray = cv2.GaussianBlur(gray, (args[&quot;radius&quot;], args[&quot;radius&quot;]), 0)(minVal, maxVal, minLoc, maxLoc) = cv2.minMaxLoc(gray)image = orig.copy()cv2.circle(image, maxLoc, args[&quot;radius&quot;], (255, 0, 0), 2) Color Quantization: Color quantization limits the number of colors remained in one picture. For example if there is sky blue and dark blue, they might be combined into some color in the middle of their RGB value. It removes redundant color information thus saves storage spaces. It’s useful in image search problems. 123456789101112131415161718192021222324# convert the image from the RGB color space to the L*a*b*# color space -- since we will be clustering using k-means# which is based on the euclidean distance, we&apos;ll use the# L*a*b* color space where the euclidean distance implies# perceptual meaningimage = cv2.cvtColor(image, cv2.COLOR_BGR2LAB) # reshape the image into a feature vector so that k-means# can be appliedimage = image.reshape((image.shape[0] * image.shape[1], 3)) # apply k-means using the specified number of clusters and# then create the quantized image based on the predictionsclt = MiniBatchKMeans(n_clusters = args[&quot;clusters&quot;])labels = clt.fit_predict(image)quant = clt.cluster_centers_.astype(&quot;uint8&quot;)[labels] # reshape the feature vectors to imagesquant = quant.reshape((h, w, 3))image = image.reshape((h, w, 3)) # convert from L*a*b* to RGBquant = cv2.cvtColor(quant, cv2.COLOR_LAB2BGR)image = cv2.cvtColor(image, cv2.COLOR_LAB2BGR)","tags":""},{"title":"Learn CNN from Giants","url":"/2017/08/29/Deep-Learning-2/","text":"Let’s start with the best tutorials for deep learning and CNNs. Genreal Tutorials: An Intuitive Explanation of Convolutional Neural Networks by Ujjwal Karn Unsupervised Feature Learning &amp; Deep Learning Tutorial by Andrew NG CS231n Convolutional Neural Network for Visual Recognition by Feifei Li Deep Learning Tutorial by Theano Development Team Classic Papers: AlexNet by Alex Krizhevsky, Ilya Sutskever&amp; Geoffrey Hinton from University of Toronto VGG: Very Deep Convolutional Neural Networks for Large-Scale Image Recognition by Visual Geometry Group, University of Oxford GoogLeNet: Going Deeper with Convolutions by Google Inc ResNet: Deep Residual Learning for Image Recognition by Microsoft AlexNetAdit has a good summary of its importance: The one that started it all. 2012 marked the first year where a CNN was used to achieve a top 5 test error rate of 15.4% (Top 5 error is the rate at which, given an image, the model does not output the correct label with its top 5 predictions). The next best entry achieved an error of 26.2%, which was an astounding improvement that pretty much shocked the computer vision community. Safe to say, CNNs became household names in the competition from then on out. Statistics Year: 2012 Data: 1.2 million images from ImageNet LSVRC-2010 Top 1 Error Top 5 Error AlexNet 37.5% 17.0% Architecture ​ Image Source Layers Remarks 1 Convolutional: 96 kernels of 11*11*3, stride 4 response normalized &amp; pooled 2 Convolutional: 256 kernels of 5*5*48 response normalized &amp; pooled 3 Convolutional: 384 kernels of 3*3*256 4 Convolutional: 384 kernels of 3*3*192 5 Convolutional: 256 kernels of 3*3*192 6 Fully connected: 4096 neurons 7 Fully connected: 4096 neurons 8 Fully connected: 4096 neurons Output layers: 1000, softmax Note that, later in the more successful version ZF Net, the size of kernel is modified from 11*11*3 to 7*7*3 to capture more details in the image. training schema We use stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005. We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and ﬁfth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates the early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron biases in the remaining layers with the constant 0. Main Take-aways Save Time Increase Accuracy Reduce Overfitting ReLU 6 times faster than Tahn/ Sigmoid activations Multiple GPUs ☑️ Drop Out layer 1/2 of time required to converge ☑️ Local Response Normalization Top1: 1.4%; Top5: 1.2% Overlapping Pooling Top1: 0.4%; Top5: 0.3% Data Augmentation ☑️ Data Augmentation Methodologies Extracting random 224 × 224 patches (and their horizontal reﬂections) from the 256×256 images and training our network on these extracted patches 4. Altering the intensities of the RGB channels in training images…. We perform PCA on the set of RGB pixel values, To each training image, we add multiples of the found principal components, with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from a Gaussian with mean zero and standard deviation 0.1. VGGVGG is the first paper that discusses about the depth of CNN architecture. It extends the number of layers to 19 and uses very small (3*3) convolutional filters. It also states that VGG model could be used as a part in other machine learning pipeline as deep features. Statistics Year: 2014 Data: 1.3 million images from ImageNet ILSVRC-2012 Top 1 Error Top 5 Error AlexNet 37.5% 17.0% VGG 23.7% 7.3% Architecture ​ Image Source Note that, All hidden layers are equipped with the rectiﬁcation (ReLU (Krizhevsky et al., 2012)) non-linearity. the ReLU activation function is not shown for brevity. Changes from AlexNet AlexNet VGG Layers 5 Conv, 3 FC layers 16 Conv, 3 FC layers Convolutional Filters 11*11; stride = 4 a stack of conv layers with 3*3 kernels, stide =1 Padding None 1 Pooling Overlapping Max Pooling Non-overlapping Max Pooling Local Response Normalization Yes - it improves performance No - it doesn’t improve performance but increase memory &amp; computation time Training Schema AlexNet VGG Batch size 128 256 Momentum 0.9 0.9 Weight decay 0.0005 0.0005 Initialization N(0,0.01); bias = 1 N(0,0.01); bias = 0 Main Take-aways Deep CNN! Why would we use stack of multiple 3*3 Convolutional layers (without spatial pooling in between) instead of larger one layer kernel? Same receptive filed. Receptive filed is well explained by Novel Martis in this post. For example, in the below image, the input for B(2,2) is A(1:3, 1:3). The input for C(3,3) is B(2:4, 2:4) -&gt; A(1:5,1:5). The receptive field for 2 convolutional layers will be 5*5, and 3 convolutional layers will be 7*7. ​ Image Source More discriminative function. There are three ReLU layers instead of one. Less parameters. Suppose we have C channels. Stack of three 3*3 kernels will have 3(3^2) = 27 parameters, and one layer of 7\\7 kernel will have 7^2 = 49 parameters; which is 81% more. InceptionInception goes beyond the idea “we need to go deeper” but comes up with a “network-in-network” inception module. There are two drawbacks of the previous most popular deeper and wider neural network - that it might easily go overfitting especially when there’re no enough training examples, and that it takes up too much computational resources. The authors are motivated to build more efficient yet accurate (or more accurate) algorithms by replacing the fully connected layers with dense structures. Statistics Year: 2015 Data: 1.3 million images from ImageNet ILSVRC 2014 Top 1 Error Top 5 Error VGG 23.7% 7.3% Inception 6.7% Architecture ​ Image source Inception Module: So the green box above is an inception module, which could be presented as the picture below. The idea behind is that the authors try to find a dense structure that can best approximate the optimal local sparse structure. So the idea is well outlined in Adit’s blog: previously in stacked CNNs, different sizes of convolutional layers and max pooling layers are to be chosen; here we have them all. For example, if inside one picture, there is a person stands nearer the camera while there is a cat that is far away from the camera, it would be beneficial to have both of a larger image kernel to capture the nearer person and a smaller kernel to capture the cat. Therefore with the inception module, parameters are less, yet more powerful than simply stacked convolution. ​ Image source Main Take-Aways The idea of CNN doesn’t have to be stacked up sequentially. Get rid of fully connected layer (use average pooling instead) thus saves a lot of parameters and computational time. The massive usage of 1*1 convoluational kernel: Dimensional reduction The 1*1 kernel is reducing a great amount of image dimensions. For example, if there is 224*224*60 input that goes through a 1*1*10 image kernel, then the output size will just be 224*224*10. Less parameters, less chance of overfitting ResNetResNet aims to solve the problem of degradation. Basically, the authors of ReNet found that with increased number of layers, the accuracy get saturated thus degradation occurs. Previously the degragation was thought to be overfitting, but it isn’t: The training error increase rather than decrease. This is counterintuitive. The authors believe that, “the degradation problem (of training accuracy) suggests that the solvers might have difﬁculties in approximating identity mappings by multiple nonlinear layers.” Therefore he is motivated to create an easier way to optimize the deep CNNs. Statistics Year: 2015 Data: ILSVRC 2015 Top 5 Error Inception 6.7% ResNet 3.6% beats human recognition: 5%-10% Architecture ​ Image Source Residual Block Image Source It performs shortcut identity mapping. I will reference Adit’s wonderful explanation here: The idea behind a residual block is that you have your input x go through conv-relu-conv series. This will give you some F(x). That result is then added to the original input x. Let’s call that H(x) = F(x) + x. In traditional CNNs, your H(x) would just be equal to F(x) right? So, instead of just computing that transformation (straight from x to F(x)), we’re computing the term that you have to add, F(x), to your input, x. Basically, the mini module shown below is computing a “delta” or a slight change to the original input x to get a slightly altered representation (When we think of traditional CNNs, we go from x to F(x) which is a completely new representation that doesn’t keep any information about the original x). The authors believe that “it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping”. Another reason for why this residual block might be effective is that during the backward pass of backpropagation, the gradient will flow easily through the graph because we have addition operations, which distributes the gradient. Reference The 9 Deep Learning Papers You Need to Know About An Intuitive Explanation of Convolutional Neural Networks CS231n Convolutional Neural Network for Visual Recognition by Feifei Li AlexNet by Alex Krizhevsky, Ilya Sutskever&amp; Geoffrey Hinton from University of Toronto VGG: Very Deep Convolutional Neural Networks for Large-Scale Image Recognition by Visual Geometry Group, University of Oxford GoogLeNet: Going Deeper with Convolutions by Google Inc ResNet: Deep Residual Learning for Image Recognition by Microsoft","tags":"cnn machine-learning"},{"title":"Learn ML from Sklearn: Cross Validation","url":"/2017/08/16/Sklearn-CV/","text":"Overfitting in Two Ways Learn parameters and test the model in the same dataset Solution: train-test 1X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.4, random_state=0) Tune the hyperparameters and test the model in the same dataset When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. Solution: Train-validation-test, Cross validation 1scores = cross_val_score(model, iris.data, iris.target, cv=5, scoring=&apos;f1_macro&apos;) Note: from my point of view, cross validation is not a very clean solution for hyperparameter tuning. A small test set is still needed to see the generalization error. But it is a good way to see if the model is stable. If the validation error varies amongst different left out samples, then there might be some problems. Visualize Overfitting &amp; UnderfittingEffect of a hyper-parameter 12345import numpy as npfrom sklearn.model_selection import validation_curvefrom sklearn.linear_model import Ridgetrain_scores, valid_scores = validation_curve(Ridge(), X, y, &quot;alpha&quot;, np.logspace(-7, 3, 3)) Effect of the number of training samples 12345from sklearn.model_selection import learning_curvefrom sklearn.svm import SVCtrain_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=&apos;linear&apos;), X, y, train_sizes=[50, 80, 110], cv=5) K FoldsK-Fold KFold divides all the samples in k groups of samples, called folds ( if k=n this is equivalent to the Leave One Out strategy), of equal sizes (if possible). The prediction function is learned using k - 1 folds, and the fold left out is used for test. 12345from sklearn.model_selection import KFoldkf = KFold(n_splits=10, random_state = 1)for train_index, test_index in kf.split(X): X_train, y_train = X[train_index], y[train_index] Stratified K-FoldUse stratified K-Fold when the class is unbalanced. StratifiedKFold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set. 12345from sklearn.model_selection import StratifiedKFoldskf = StratifiedKFold(n_splits=10, random_state = 1)for train, test in skf.split(X, y): X_train, y_train = X[train_index], y[train_index] Group K-Fold An example would be when there is medical data collected from multiple patients, with multiple samples taken from each patient. And such data is likely to be dependent on the individual group. In our example, the patient id for each sample will be its group identifier. In this case we would like to know if a model trained on a particular set of groups generalizes well to the unseen groups. To measure this, we need to ensure that all the samples in the validation fold come from groups that are not represented at all in the paired training fold. 123456789from sklearn.model_selection import GroupKFoldX = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]y = [&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;d&quot;, &quot;d&quot;, &quot;d&quot;]groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]gkf = GroupKFold(n_splits=5)for train, test in gkf.split(X, y, groups=groups): print(&quot;%s %s&quot; % (train, test)) Time Series Split Time series data is characterised by the correlation between observations that are near in time (autocorrelation). However, classical cross-validation techniques assume the samples are independent and identically distributed, and would result in unreasonable correlation between training and testing instances (yielding poor estimates of generalisation error) on time series data. Therefore, it is very important to evaluate our model for time series data on the “future” observations least like those that are used to train the model. Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them. Also, it adds all surplus data to the first training partition, which is always used to train the model. 123456789from sklearn.model_selection import TimeSeriesSplitX = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])tscv = TimeSeriesSplit(n_splits=3)for train, test in tscv.split(X): print(&quot;%s %s&quot; % (train, test))#&gt;&gt;&gt; [0 1 2] [3]#&gt;&gt;&gt; [0 1 2 3] [4]#&gt;&gt;&gt; [0 1 2 3 4] [5] Tuning Hyper-parametersSo Scikit-learn provides tools to tune hyper-parameters. That’s to say, we don’t have start with train-validation-test and then input different hyper-parameter and then print out validation error. We can input the desire model, and a list of hyper-parameters to choose from, and then scikit-learn will iterate and gives the best combination. Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to “train” the parameters of the grid. When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process: it is recommended to split the data into a development set (to be fed to the GridSearchCV instance) and an evaluation set to compute performance metrics. There are two ways to tune hyper-parameters. Grid Search The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. 12345678910111213141516# Split the dataset in two equal partsX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0)# Set the parameters by cross-validationtuned_parameters = [&#123;&apos;kernel&apos;: [&apos;rbf&apos;], &apos;gamma&apos;: [1e-3, 1e-4], &apos;C&apos;: [1, 10, 100, 1000]&#125;, &#123;&apos;kernel&apos;: [&apos;linear&apos;], &apos;C&apos;: [1, 10, 100&#125;]clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring=&apos;precision&apos;)clf.fit(X_train, y_train)print (clf.best_params_)print (clf.cv_results_[&apos;mean_test_score&apos;])y_true, y_pred = y_test, clf.predict(X_test) Randomized Search RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search: A budget can be chosen independent of the number of parameters and possible values. Adding parameters that do not influence the performance does not decrease efficiency. 123456789101112131415161718192021X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0)# specify parameters and distributions to sample fromparam_dist = &#123;&quot;max_depth&quot;: [3, None], &quot;max_features&quot;: sp_randint(1, 11), &quot;min_samples_split&quot;: sp_randint(2, 11), &quot;min_samples_leaf&quot;: sp_randint(1, 11), &quot;bootstrap&quot;: [True, False], &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]&#125; clf = RandomForestClassifier(n_estimators=20)# run randomized searchrandom_search = RandomizedSearchCV(clf, param_dist, n_iter=20)random_search.fit(X, y)print (clf.best_params_)print (clf.cv_results_[&apos;mean_test_score&apos;])y_true, y_pred = y_test, clf.predict(X_test)","tags":"python-packages machine-learning model-selection"},{"title":"Community Detection in Python","url":"/2017/08/08/Community-Detection/","text":"NetworkX vs. IGraphImplementation NetworkX: implemented in Python IGraph: implemented in C PerformanceIGraph wins. IGraph NetworkX Single-source shortest path 0.012 s 0.152 s PageRank 0.093 s 3.949 s K-core 0.022 s 0.714 s Minimum spanning tree 0.044 s 2.045 s Betweenness 946.8 s (edge) + 353.9 s (vertex) (~ 21.6 mins) 32676.4 s (edge) 22650.4 s (vertex) (~15.4 hours) See ref No. of Community Detection AlgorithmsIGraph wins. NetworkX: only optimal modularity. IGraph: nine algorithms including optimal modularity; edge betweenness etc. Ease of ProgrammingNetworkX wins. Load a graph NetworkX can simply load a graph from a list of edge tuples. 12345edges = [(1,2),(2,3),(3,4),(4,5)]G = nx.Graph()# add edges from txt G.add_edges_from(edges) IGraph needs to first load vertices and then a list of edge tuples 1234567891011121314151617# create graphg = Graph()# in order to add edges, we have to add all the vertices first#iterate through edges and put all the vertices in a listvertex = []for edge in edges: vertex.extend(edge)g.add_vertices( list( set(vertex))) # add a list of unique vertices to the graphg.add_edges(edges) # add the edges to the graph. &quot;&quot;&quot;Note: add_edges is much quicker than add_edge. for every time add_edge is performed, the entire data structure in C has to be renewed with a new series ordering.&quot;&quot;&quot;print g Get information from existing graph NetworkX 123## get no. of neighbors of a node by node namenode_index = G.vs.index(target_node) neighbors = G.adjacency_list()[node_index] IGraph 123456789101112## get no. of neighbors of a node by node namenode_index = g.vs.find(target_node).index #find index by nodenameneighbors = []# g.incident(node_index) gives a series of edges that passes the target node index# iterate through the edges, and append the other node into neighbors listfor edge in g.es[g.incident(node_index)]: if edge.target == node_index: neighbors.append(g.vs[edge.source][&quot;name&quot;]) else: neighbors.append(g.vs[edge.target][&quot;name&quot;]) Community Detection AlgorithmsA list of algorithms available in IGraph include: Optimal Modularity Edge Betweenness (2001) Fast Greedy (2004) Walktrap (2005) Eigenvectors (2006) Spinglass (2006) Label Propagation (2007) Multi-level (2008) Info Map (2008) Summary For directed graph: go with Info Map. Else, pls continue to read. If compuational resources is not a big problem, and the graph is &lt; 700 vertices &amp; 3500 edges, go with Edge Betweenness; it yields the best result. If cares about modularity, any of the remaining algorithms will apply; If the graph is particularly small: &lt; 100 vertices, then go with optimal modularity; If you want a first try-on algorithm, go with fast greedy or walktrap If the graph is bigger than 100 vertices and not a de-generated graph, and you want something more accurate than fast greedy or walktrap, go with leading eigenvectors If you are looking for a solution that is similar to K-means clustering, then go for Spinglass ​ Optimal Modularity Definition of modularity: Modularity compares the number of edges inside a cluster with the expected number of edges that one would find in the cluster if the network were a random network with the same number of nodes and where each node keeps its degree, but edges are otherwise randomly attached. Modularity is a measure of the segmentation of a network into partitions. The higher the modularity, the denser in-group connections are and the sparser the inter-group connections are. Methodology: GNU linear programming kit. Evaluation: Better for smaller communities with less than 100 vertices for the reasons of implementation choice. Resolution limit: when the network is large enough, small communities tend to be combined even if they are well-shaped. Fast Greedy Methodology: Bottom up hierarchical decomposition process. It will merge two current communities iteratively, with the goal to achieve the maximum modularity gain at local optimal. Evaluation: Pretty fast, can merge a sparse graph at linear time. Resolution limit: when the network is large enough, small communities tend to be combined even if they are well-shaped. Walktrap Methodology: Similar to fast greedy. It is believed that when we walk some random steps, it is large likely that we are still in the same community as where we were before. This method firstly performs a random walk 3-4-5, and merge using modularity with methods similar to fast greedy. Evaluation: A bit slower than fast greedy; A bit more accurate than fast greedy. Multi-level Methodology: Similar to fast greedy, just that nodes are not combined, they move around communities to make dicision if they will contribute to the modularity score if they stay. Eigenvectors Methodology: A top down approach that seeks to maximize modularity. It concerns decomposing a modularity matrix. Evaluation: More accurate than fast greedy Slower than fast greedy Limitation: not stable on degenerated graphs (might not work!) Label Propogation Methodology: A bit like k-clustering, with initialization k different points. It uses an iterative method (again just like k-means): the target label will be assigned with the most “vote” of the lables from its neighbors; until the current label is the most frequent label. Evaluation: Very fast Like K-Means, random initialization yields different results. Therefore have to run multiple times (suggested 1000+) to achieve a consensus clustering. Edge Betweenness Definition of edge betweenness: Number of shortest path that passes the edge. It’s not difficult to imagin that, if there is an edge that connects two different groups, then that edge will has to be passed through multiple times when we count the shortest path. Therefore, by removing the edge that contains with the highest number of shortest path, we are disconnecting two groups. Methodology: Top down hierarchical decomposition process. Evalution: Generally this approach gives the most satisfying results from my experience. Pretty slow method. The computation for edge betweenness is pretty complex, and it will have to be computed again after removing each edge. Suitable for graph with less than 700 vertices and 3500 edges. It produces a dendrogram with no reminder to choose the appropriate number of communities. (But for IGraph it does a function that output the optimal count for a dendrogram). Spinglass Methodology: Complicated enough for me to ignore.. Evaluation: Not fast has a bunch of hyperparameters to tune from Infomap Methodology: It is based on information theoretic principles; it tries to build a grouping which provides the shortest description length for a random walk on the graph, where the description length is measured by the expected number of bits per vertex required to encode the path of a random walk. Evaluation: Used for directed graph analytics Reference Graph-tool performance comparison Python IGraph Manual Modularity (Networks)) Tamas’ answer to “What are the differences between community detection algorithms?”","tags":"python-packages network-analytics machine-learning"},{"title":"八月 从头始","url":"/2017/08/07/八月-从头始/","text":"07/08: 写于清晨。 早上醒来，刷着朋友圈，看到宇菲开了一个公众号，记录她的房车生涯。惊叹中不由羡慕，似乎她一直活在她的路上，而我已经模糊成芸芸街头的一张脸谱。我不指望我有些许改变，让曾经那么多爱我、甚至仰慕于我的人说“这才是我心中的Yoyo”，我不；因为我不是定式、我也对曾经的我的毛躁、不知所谓的热情也早已不再欣赏。然而我确实需要找到，我现在的样子，归路究竟在何方？ 大抵恋爱的人都会在comfort zone待着，或者这个锅恋爱不背，只有我眷眷于舒适、不费力的环境。所以我懒了，懒到将许多的放弃当成了谦卑，懒到将不愿付出当成了无力去爱。于是在一步一步的退让后，我将自己缩到了一个角落，然后再退，再退……不知读者是否看过《套中人》，大概从我高中第一次放弃自己判断人际关系、而依赖于完美闺蜜的时候，我已经把自己放在了第一个套子里，那个套子叫做“我不擅长人际但是我真性情啊“。 于是我用很多“不能”包围了自己。不再唱《给未来的自己》。 不管怎样，怎样都会受伤，可伤了又怎样；至少我很坚强我很坦荡。 也许我努力了，到最后我仍然是不能。可是不是现在。不是什么都还没有付出的时候。不是生活中的一步步都还未和自己的模样产生共鸣的时候。 所有的目标在活出来之前都只是鸡汤而已。为无限可能加油吧。也谢谢激励我的你。:) 08/08: 写于深夜。 在所有的话开始之前，先跟自己说一句，你的时光会为你说话。你的心在哪里，你的宝藏就在哪里。 坚持健身三天，小打个卡。惊讶于曾经总是要偷懒耍滑的系列动作其实完美完成起来并不难，惊讶于我其实是喜欢自己安静地做事，安静地拉筋的模样。我想说我一切的改变是从房子开始的，又想说那个好笑的女巫童话也许不完全是骗人的。:) “女人真正想要的是什么？” … 加温回答道：“既然你说女人真正想要的是主宰自己的命运，那么就由你自己决定吧！”女巫终于热泪盈眶: “我选择白天夜晚都是美丽的女人，因为我爱你！” 今天难得的晚睡了，也是难得回家后自觉加班到此刻，就为了一篇学术博文。但成就感满满呀。那就原谅我此刻无人倾诉自发牢骚的呓言呓语吧。 18/09: 在慢慢进行自我改造计划。 上一个月的愿望是不再抱怨。抱怨其实只是一种习惯。但它确实是一根丑陋的拐杖，拄着它就无法飞翔 by维斯。 这一个月的愿望是自然地赞美。我要把我曾经小太阳的能力都拿回来，抵挡我内心轻易涌现的比较，然后，嫉妒。其实很奇怪，我是多么容易地欣赏别人。比我性子软糯的，我羡慕别人的温柔以及好人缘，比我性子刚硬的，我羡慕她们的独立及成熟。我似乎欣赏了天下的所有人，但我却没法欣赏我自己。还是我根本意识不到我自己的样子，所以才总会有患得患失以及不自信？ 还有一个没有想到的转变是渐渐不看小说了。终于这个爱好不争气了起来，它的写手逐渐低龄化到我无法哄骗自己走入她们营造的世界，哪怕是做梦也觉得荒谬。这一次晋江帐号突然出现bug，疼惜里面的人民币的同时我居然感到如释重负，啊，我真的该换爱好了。再见了陪我20年的小说。你陪我度过我最童真的时代，带给我无数关于爱情友情的瑰梦，但如今我们要告别了。大概余下的时间我只是随心把自己填满吧。看看食谱，做做手工，健健身，然后学着哄自己开心而不是任凭自己沉浸在作品的余韵中悲伤得不能自已。 19/09: 渐渐立了工作目标。循序渐进地做好每一个项目的同时，做公司内的deep learning专家。要广，也要专精。 也许工作没有贵贱之分，难道我会deep learning，万总不会，我就真的比他做的好吗？初心是帮助一个公司做到它们想象中才能做到的事情，让所有人更方便，这点来说，繁杂而底层的工作，像清洁工，像管道维修工，比我们这些领着高薪，写代码的人更被社会需要。工资付的是智商钱，竟不是劳力钱。 渐渐学习更多，终于能领会到学习的美妙。原来并不是死记硬背，而是站在巨人的肩膀上看先贤如何一个个解决在我无能为力的事情。这样的智慧令人心折。 今天要去保养头发了。突然觉得变成一个美丽的自己多么重要。为什么要容忍次一程度的生活和容貌呢。做一个最好的自己。","tags":""},{"title":"Learn ML from Sklearn Preprocessing","url":"/2017/06/25/Sklearn-Preprocessing/","text":"PreprocessingEncoding Categorical Features Integer representation can not be used directly with scikit-learn estimators, as these expect continuous input, and would interpret the categories as being ordered, which is often not desired (i.e. the set of browsers was ordered arbitrarily). One possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K or one-hot encoding, which is implemented in OneHotEncoder This estimator transforms each categorical feature with m possible values into m binary features, with only one active. 12345678from sklearn import preprocessingenc = preprocessing.OneHotEncoder()X_encoded = enc.fit_transform(X) #if X contains missing categorical features, one has to explicitly set n_valuesenc = preprocessing.OneHotEncoder(n_values=[1, 2, 3, 4])X_encoded = enc.fit_transform(X) Imputation of Missing Values A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. Basic strategies for imputing missing values: either using the mean, the median or the most frequent value of the row or column in which the missing values are located. 123from sklearn.preprocessing import Imputerimp = Imputer(missing_values=&apos;NaN&apos;, strategy=&apos;mean&apos;, axis=0)X_imputed = imp.fit(X) Standardization It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features. To address this issue you can use PCA with whiten=True to further remove the linear correlation across features. Standardization Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. 12from sklearn import preprocessingX_scaled = preprocessing.scale(X) Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) should be learnt from a training set and applied to held-out data for prediction: 123456from sklearn import preprocessingX_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.4)# instead of preprocessing.scale, it&apos;s recommended to use scalerscaler = preprocessing.StandardScaler().fit(X_train)X_train_transformed = scaler.transform(X_train)X_test_transformed = scaler.transform(X_test) Scaling sparse dataSummary: do the standardization without centering. Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it can make sense to scale sparse inputs without centering, especially if features are on different scales. 1234567from sklearn import preprocessingmax_abs_scaler = preprocessing.MaxAbsScaler()X_scaled = max_abs_scaler.fit_transform(X)## alternativescaler = preprocessing.StandardScaler(with_means = False)X_scaled = scaler.fit(X) Scaling data with outliersSummary: use median instead of mean, use IQR instead of standard deviation. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results. 123from sklearn.preprocessing import RobustScalerrobust_scaler = RobustScaler()X_scaled = robust_scaler.fit_transform(X) Normalization Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples. This assumption is the base of the Vector Space Model often used in text classification and clustering contexts. 123from sklearn import preprocessingX_l2_normalized = preprocessing.normalize(X, norm=&apos;l2&apos;)X_l1_normalized = preprocessing.normalize(X, norm=&apos;l1&apos;) Binarization Feature binarization is the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution. 123from sklean import preprocessing binarizer = preprocessing.Binarizer(threshold=1.1)binarizer.transform(X)","tags":"python-packages machine-learning preprocessing"},{"title":"学医偶得 一","url":"/2017/06/24/学医偶得/","text":"总纲次序 有的朋友每天要按摩很多穴位，还要刮痧、拔罐、练功，总觉得运用的方法越多，治疗的效果越好。其实并非如此，我们的气血就那么多，我们需集中力量，逐个解决身体的问题；切不可将气血分散各处，无的放矢，这样越治问题会越多，终将失去信心和耐心。记住：简单才有效，顺势才迅捷。 手法 酸和痛都表示经络尚通畅，但在该处狭窄或有拥堵，流通不畅快。酸多表示气血虚弱，需要补，不可采用过强手法。而刺痛则表明那地方有气血在，却堵住了，气血正在努力冲撞，此时则稍微用力度大的手法帮助疏通。 肺经时辰 人的气血在夜里3点到5点（也就是寅时）开始冲击肺经，所以此时若出现症状，我们通常要考虑到肺是不是有问题。曾治过一个妇女，每到冬季总是在凌晨4点钟左右躁热出汗，白天则畏寒怕冷。诊断她为风寒束表，心火内盛，典型的“冰包火”。但其发病的根源是肺气不足，无力助心火以驱散风寒，必借寅时肺经气盛才能发汗解表。所以我用补中益气汤补肺而助其宣发之力，顺势而为，一剂而愈。 功用 上可疏解肝经之郁结，中可运化脘腹之湿浊，下可补肾中之亏虚。 “诸气者，皆属于肺。”《内经》的话句句都是金玉良言，须仔细体悟才行。所以，气虚的培补、气逆的顺调、浊气的排放、清气的灌溉，都可以通过调节肺的功能来实现。 “肺主宣发肃降，肺是水上之源，肺开窍于鼻，肺主皮毛，诸气愤郁，皆属于肺，在志为忧悲，在液为涕，在体合皮毛，在窍为鼻。” 实例 少商：咽痛 治嗓子痛效果最佳，尤其是对急性咽喉肿痛有特效。 但是这个穴必须得强刺激才行，过去这些井穴（末梢的穴都叫井穴）通常都需要用三棱针来点刺，放一滴血，当时就会见效，但好多人就怕放血，这时不妨用指甲使劲掐一掐。 鱼际：心火 定喘的效果很好，只需按揉即可。夜里咳嗽，尤其是两三点钟咳嗽、睡不着觉，这种现象非常普遍，通常就是肝火引起的。这时候按摩鱼际穴就会缓解。 善于退热的要穴。当您心里有火、夜间爱咳嗽、比较烦热、睡不着觉时，按揉鱼际穴特别管用。 调节小孩的肠胃功能。在中医院小儿科，鱼际穴又叫板门穴。“板门”就是木板的房门，此穴是专门调理小孩不爱吃东西、肠胃功能不好的，一揉“板门”，胃口的门就打开了。 在大拇指下肉肚最高点。 太渊：补气、脉之会 有人总觉得气不够使，有吸不上气的感觉，就点揉太渊穴，此穴为肺经原穴，补气效果极佳。 喘气费劲（吸入氧气不够）、爬一会儿山甚至动一动就一头汗或者气不足、大便时老觉得没劲或使不上劲，这样的人就要补气。有的人是偏血虚，有的人是偏气虚。偏气虚的人要想直接补血是补不上的，一定要先补气，气足了血才能生。如果火气很旺，那就是气实，不需要直接补气。而气虚的人都是虚寒体质，都需要补气。 其为脉之会，就是体内所有脉都归它控制。心脏跳动异常、早搏、房颤，只要跟心血管有关系的，都是太渊穴的适应范围。静脉曲张、脉管炎这些跟血管、脉络有关系的病症，按揉太渊穴都有效果。 太渊穴正好在腕横纹上，很深。在揉它的时候，一定要把指甲剪平，也可以用大拇指内侧硌它。 经渠：气不顺 “经”是经络，“渠”是水渠。“经渠”的意思是经络到此就水到渠成了。经渠穴是治疗气不顺的。好多虚弱体质的人就是因为气乱了，而这个穴是一个慢慢调养的穴，它可以使肺气逐渐增强，最后达到一个水到渠成的目的。也就是说，每天都能给您补充一点点精力，而且对实证、虚证都管用。 按这个穴位的时候不要按到骨头上，而要按到骨头内侧缘。不要往下按，要往外按，搓着按就能找到这个穴了。平常可以经常揉一揉它，觉得气有点儿不太顺或者气接不上来都可以揉，而且无论是热性、寒性咳嗽都可以揉， 列缺：头颈不适 “头项寻列缺”，脖子落枕、感冒引起的头痛，都跟风寒有关，平时您可要多揉列缺穴。 孔最：扁桃体炎 “孔”是毛孔，“最”是最大。孔最穴的意思就是身体里所有跟孔有关的问题都归它来管理。上至鼻窍，下至肛门，都跟孔有关。对风寒感冒引起的咳嗽和扁桃体炎效果不错。有的人发烧不出汗，赶紧揉孔最穴可以帮助发汗。 尺泽：泻肺补肾 给肾以恩泽，给肾以浇灌，最好的补肾穴，通过降肺气而补肾，最适合上实下虚的人。肺属金，肾属水，而金能生水，就是肺气足了就可以补肾。所以，揉尺泽穴就能把肺经多余的能量补到肾经上去。尺泽穴又是合穴，属水，所以这种补肾方法叫做泻肺补肾法。其实，泻只是能量的一种转化，是把肺经多余的能量转换到肾经上去了，因为上焦的能量过多、淤积住了，反而让人觉得不舒服，老有火气，老想吃点儿凉的或者祛火的东西，而同时却两脚冰凉。这是火气都用到上边去了，没有留些到下面来，形成了上实下虚之证。 侠白：解气郁恐惧 “侠”是侠客，“白”是白色。肺属金，金在五行的颜色为白，因此，这里“白”代表肺的意思。而“侠白”就是有个侠客在保护肺，给我们补足肺气，让我们无所畏惧。所以它可以治疗肺气不足造成的经常恐惧、心跳过速。为什么人会恐惧呢？就是先有忧虑，忧虑解不开了就会恐惧。而且一忧虑就会气郁，常常气串两肋，所以按揉侠白穴还可以治疗肋间神经痛，即两肋痛。 天府：通鼻窍 鼻窍通于天，天府穴暗含着这个意思，就是能治鼻子的各种疾患。像过敏性鼻炎、慢性鼻炎、经常流鼻血等鼻子的疾病，揉天府穴效果非常好。 在腋下3寸，此穴可以用一种特殊的方法来找到。两臂张开，掌心相对平伸，在鼻尖上涂上一点墨水，用鼻尖点臂上，点到处就是此穴。 云门：浊气宣发 这里是一个气体宣发的地方。很多人爱生气，气完就憋在那里了，宣发不出去，于是循着肺经走到四肢，就会造成四肢烦热、特别燥、心里堵闷、掌心热等症状。这时，使劲点揉云门穴，一般就会打嗝，气就发出去了。打通经络，其中的一个主要目的就是排除浊气。好多人一揉这个肺经就老打嗝，这是非常好的现象。 中线任脉旁开６寸，锁骨下缘处。两手叉腰时，此处会有一个三角窝。 中府：调理中气 “中”指中气，就是脾肺之气，脾和肺合起来的气叫中气。如果经常觉得气不够使，喘不上气来，或者大便的时候无力，以及吃一点东西肚子就胀，这就是中气不足了。中府穴就是专门调治中气不足的。中府穴是肺经的一个募穴，也是脾肺两经交会的一个穴，这个穴调气最好。如果人体的气乱了，就爱咳嗽、哮喘、堵闷，会经常觉得上气不接下气，这时一定要多揉中府穴。 在云门下面一寸。推的时候，大拇指按着中府穴，然后向上推云门穴，一般这里会很痛。把痛的地方给推开，浊气就会散掉，您就会觉得胸里面非常舒服。 制约 肺功能不好就是肝里的浊气熏蒸肺造成的，主要的源头在肝，所以从肝上来调节才是治本的方法。 肺本是娇脏，最怕攻伐，所以“调诸脏即是治肺”实乃真知灼见。如咳喘症，也很少由肺经直接引起，多是其他脏波及。由肝火引起的叫“木火刑金”，祛肝火就好；由肾虚引起的叫“肾不纳气”，补肾气辄效；由脾虚引起的叫“痰湿蕴肺”，健脾祛湿最佳。还有外感咳嗽，多由风寒引起，那就赶走膀胱经之风寒好了。通常咳喘的病总会迁延不愈，古时便有“内科不治喘”之说，其实多是因见肺治肺，有痰化痰，宣来降去，不治根本，才成痼疾。 大肠经功用 肺与大肠相表里，所以肺脏上面有什么疾患，都可以通过大肠经来调理，它主治皮肤病，也管便秘、腹泻等肠道疾病。 皮肤病可以说是最让人心烦意乱的疾病了，荨麻疹、神经性皮炎、日光性皮炎、牛皮癣、疥疮、丹毒、疖肿、皮肤瘙痒症……都让人痛苦不堪。在百治无效之际，取大肠经刮痧，通常都会得到不同程度的缓解。大肠经为多气多血之经，阳气最盛，用刮痧和刺络的方法，最善祛体内热毒。若平日常常敲打，可清洁血液通道，预防青春痘。大肠经对现代医学所讲的淋巴系统有自然保护功能，经常刺激可增强人体免疫力，防止淋巴结核病的生成。 实例 三间：通经行气 最善通经行气，上可通达头面，治疗三叉神经痛、齿痛、目痛、喉肿痛和肩膀痛；下能通腹行气，泻泄可止，便秘可通。另外，有研究指出此穴有消炎、止痛、抗过敏的功效。 位于食指近拇指侧根部，第二掌指关节后。常用大拇指内侧指节横向硌揉此穴，效果甚佳。","tags":"肺经"},{"title":"暑月 新鲜事","url":"/2017/05/09/暑月-新鲜事/","text":"05/09: 在看小说、学术、和写博客之间拉扯，最终还是决定先把思绪倾倒。我着实是一个很奇怪的人，当没有朋友的时候会自怜，当交际多的时候会自厌。穿着一身22岁时候的面具，说着没长进的话语，看到自己空洞的灵魂。好……无力。所以我想多写，我问自己是谁的时候只能多写，让我的重心通过笔端泄入地平面。 人的浮躁往往来源于说的委实太多，做与思考却太少。今天早晨和万胖聊天是关于为何我不能做一个开心而高情商的宝宝，可真正的答案我想想其实也知道。这样的谈话就是浮夸。爱圣经。“我亲爱的弟兄，这是你们所知道的。但你们各人要快快地听，慢慢地说，慢慢地动怒，因为人的怒气不能成就神的义。”（雅各书1:19-20）“惟用爱心说诚实话，凡事长进，连于元首基督。“（以弗所书4:15）”不轻易发怒的，胜过勇士；治服己心的，强如取城。“（箴言16：32）我不想逃，我想定个目标。一年一年时光过去，我不信我不能长成基督的模样。今年，说话这件小事，放进祷告中。 说来，这个月的两件大事，即将毕业和商定订婚。很幸运，在今年的CMU学年中还能见证到上帝的恩典。我为雨桐祷告过，为詹婧祷告过，为我自己祷告过。不管是不是基督徒，是不是明白上帝应许祷告是一样怎样的事情，但我明白，我见证，我记住。原来我在一个群体中是一个代祷的管道，原来即便事情也许在福音的版图中无关紧要上帝也是关注的。毕竟上帝是爱，祂怜悯照顾了我们的感受。所以这份工作又是上帝给的。不确定是不是我眼中的最好，但是上帝眼中的最好，我一定能一点一点体会到。愿我继续能成为祝福的管道。 说到订婚，也有很多话要说。这两个月甜蜜流于嘴边，这样不好。何必徒徒挂在嘴边惹人艳羡，不如独自感受、刻骨铭心。很感激万胖，在长久以来对我的包容，直到我醒来，我知道，我明白他对我的包容意味着什么。从最开始说翻脸就翻脸的脾气，到后来时不时的低落，和永无止休的吃醋，我女生的情绪性的坏毛病渐渐好转，我是终于能有锚定住船。我们之间有那么多的故事，一点一滴的小瞬间，构成我爱你。未来几十年和一个人过也许是一件很可怕的事情，但如果那是和你，我想我还是很期待的。我希望你幸福，也很开心你未来的幸福有我参与。林林种种，构成一句话，余生请多多指教啦。 最后，还想诘问自己，除了自我剖析，除了谈情说爱，除了学术汇报，我的笔下还能不能有新的主题了？还是看的书太少呀。希望以后能客观点。嘤嘤嘤！ 06/09: 最近的重心一直在未来的家居设计上。想要学习家居的灵感来自于在Buffalo住的Airbnb。大大高高又舒适的床，地毯，小摆件，抱枕，壁画，甚至厕所里摆着一束小草。对我而言追求美并不是一件容易的事情，大概是因为长得丑吧（咦）。搭配家居与搭配衣服给我的感觉一样吃力。不过还是很愿意学习，为了罗尼~比心~ 看了小一周后给自己两个原则： 先确定功能再思考布局 先确定配色再挑选家居 以下是目前进展~ 首先上我的户型图图～ （图片来源：waterford landing） 由于是一个人住，这样的房型算是很均衡啦，感觉空间足够大，能够瞎发挥儿。 第一站是卧室。在仔细思索我个人生活习惯后，我决定了卧室的基础摆设： 大大大大大床 两个藤木脏衣篓 化妆柜和全身铜镜 基调是森林，绿色和木头色。决定留着那个墙壁上的小植物。一些小物，包括一台手机，一筒挂式纸巾（鼻炎患者的悲哀），一个香薰加湿器（更森林一点）就是极好。 （图片来源：h&amp;m home） 第二站是客厅。一人居……我的客厅并不需要沙发，也不需要电视。需要一台很长的桌子，一半办公，一半吃饭，显示屏在中间，吃饭的时候也可以看看电影。另一个方向，大概需要很多柜子，几个软座，一片绿意和一点留白。外加门口一个舒服得不得了的软软的座椅，可以坐着换双鞋。 （图片来源：pinterest) 第三站是dining room。既然我的客厅已经完美履行了吃饭的功能，我的dining room也许就适合做一个小小的练功房。瑜伽垫，小哑铃，几面镜子，还要想办法把ipad挂在墙上。 （图片来源：pinterest) 好啦。计划完毕！希望有足够的资金嘤嘤嘤～ 06/11: 今天……没有去主日。也很久……没有灵修了。抵不住歉意，打开了一个听道视频，说到的经文是：“你们所听的要留心。你们用什么量器量给人，也必用什么量器量给你们，并且要多给你们。因为有的，还要给他，没有的，连他所有的也要夺去。” 不敢看圣经的原因是每一句都很扎心。仔细寻思，脑海里有很多做错的事，却真的觉得需要千钧力才愿去改。小组合作的计较，与人相处的苦毒，甚至亲人之间的不饶恕，都在把我的生命往泥潭里拉。我害怕的有好多，如果我不为自己发声就要被人利用了，如果我不表明自己的立场我就要被人无视了，如果我放开心胸接纳别人我又要逼到不可能保留自己的界线了。我向上帝呼求爱的能力，上帝马上把我放进挑战的环境里。这个挑战年复一年我都fail了，现在条件反射地想用冷漠武装自己。主啊，我不靠近你，我怎么能有能力？而主啊，当我靠近你，我却看见我生命满满的瑕疵却没力气去改。我的挣扎逼着我远离，可是我时时刻刻又需要着上帝。主啊，求你拯救。 06/19: 终于知道人和人之间是独立的个体。亲缘关系也只是关系，并不妨碍独立。你有缺点，并不表示我有意见呀。 我想了很久什么是boundary。现在想来不过是过好自己。和是否take shit并无联系。哪怕爱心都是修行。 所谓的人生意义，活得出来是硬道理，活不出来只是鸡汤而已。外人说了一百遍，立不起来的还是自己，所以不妨不干涉也不说，只做最后关头的点拨。 回想自己：下的决心，睡过去的光阴，小说什么只是麻痹自己而已。当什么假正经。 ——感悟于和闺蜜及妈妈的相处。 06/24: 我们家里的人都不爱洗碗。每天中午把碗收起来午休，都偷偷摸摸地躲在房门后，一个赛一个醒得晚。谁先憋不住出来了，很好，十分钟内其他两扇门都打开。晚饭的惯例是爸爸做饭。食完晚餐，就到了每天我和妈妈的眼神对决。两个女人，今天一个皮肤病，明天一个姨妈痛。有时候爸爸哭笑不得去洗碗了，两个女人又开始用眼神互相指责，却动也不愿意动一步。 07/13: 零零星星又攒下一些思绪想写的。 我有一个好主意，如果测试两个人是否相熟，可以让一个人帮另一个人找眼镜。 大概是半个多月前回到老家。姥姥喊我上楼看一下她在天台种的花。站在门外，往通向天台的黑黑的楼梯甬道望去，似乎爷爷早年佝偻着s背一步一步往上走的身影猝不及防地在眼前浮现。不由内心大骇。也理解了姥姥即使无人相伴左右也要回老家的坚持。 看小说这么久，女生为主的小说的套路无外乎是即使我不优秀，我也有一个好情人／我非常优秀，所以我有一个好情人。而男生小说刚刚触及一两本，套路却大不相同。基本都是我资质很差，但是我特别牛逼／ 我资质很好，而且我特别牛逼。一言以蔽之，女生总是需要很多很多的爱，男生总是需要很多很多的征服感。是什么让我们这样呢？是谁限定了“诶这个太重了，让男孩子来”，或者“你是一个男孩子，居然想妈妈哭鼻子羞羞“这样的性别潜意识呢？ 雪：尽道丰年瑞，丰年事若何。长安有贫者，为瑞不宜多。by罗隐 07/29: 我的拖延症好像在搬新居的过程中痊愈了。再也不用过打折的生活了。 最近遇到的Uber司机都非常的有意思。与其说尬聊是最大的收获，不如说感谢他们让我短暂地了解他们的人生。","tags":""},{"title":"偶有所得","url":"/2017/04/10/偶有所得/","text":"遗言软件/网站如果我死了，我不希望我默默无名地死去。不希望该说出口的爱意仍被沉默吞没，不希望抱歉也只能随着骸骨而去，不希望我的葬礼上亲人好友想起和我最后对话的惋惜。我还想亲自定葬礼的形式呢。可是这个网站有一定风险- 怎么确定一个人死了呢？如果还没有死遗嘱就发了不是很尴尬吗？ air穿衣App。像Uber和Airbnb一样出借多余的“审美”。 Machine Learning Literature Review网站。摘录最新方向的ML进展，以知识树形式呈现。两个维度： 作者影响力：类似于PageRank 时间：纯粹时间线，或论文“族谱” （引用关系） 苏轼的一生网站。时间轴标明苏轼年份大事记，重大转折点标红。 年份旁鼠标移位可见诗作关键意象（NLP) 及 代表作。 中医版Keep健身类App。体质测试/五脏状况自测/舌苔图像匹配 。健身计划打卡 Prayers’ Circle教堂/小组社交App。 发布祷告事项，旁人可点Prayer。发布者可看到代祷人数。代祷人姓名不可见。上帝回应则划过，代祷人皆收到通知。按月/半年/年度统计基础数字，关键词。小组负责人可见组内关键主题汇总，及相关资料推荐。 灵修笔记圣经类App。可以引用经文、方便分类的灵修笔记 。日期/主题view 。","tags":"未来梦"},{"title":"四月 芳菲尽","url":"/2017/04/08/四月-芳菲尽/","text":"04/09: 每个月初都是最为激情澎波的时候。月末的懒怠堆积成的文字，新开一篇博文的激动，都汇成一种仪式感，所以开博文之前恨不得多洗几遍手再挺直腰。 这个月是丰收的一个季度。奇怪我的心竟然没有得到满足。又或者没什么奇怪，人不就是这样吗，就像是脖子前套上胡萝卜的蠢驴，一辈子的奔忙、从一个目标赶到下一个目标，心仍不过是空空如也，好像除了追逐本身并没有别的意义。大概所谓人性。 但是我仍然要感谢上帝。这是一个见证。在我最茫然、无助、并且压抑的时刻，一个从天而降的offer无疑把我从自我怀疑中狠狠解救出来，过了几天日日笙歌的云端日子。然而这一切并不会就从此完美结局，比较的压力仍然是在的，公司头衔、工资多少，以及方方面面的比对迟早会把人压得喘不过气。生活中各个方面总是有比较，唯一逃脱的办法就是做那个自己。正如今天听道所闻，有些事是desire，有的事是expectation。前者来自于人类的妄求，后者来自于上帝的美意。多少次我一头栽进无边欲海中就这样一秒伤了自己，又久久沉溺。 于是谈谈朋友这件事情。很幸运的学期，每个周末的狼人会是我全身上下每个细胞的击掌尖叫。所以我骨子里还是外向的，即便我总是喜欢定义自己为一张不爱社交而内向的牌，然而我确实喜欢有人可爱并且被爱的感觉。然而我却有负担的，每每聊天过后偶尔想起，想想自己说出的话浮夸，我就并不喜欢自己。就像我就算当下有感而发的朋友圈，事后我总觉得那不纯净，就要把它删掉一样。也许我是一根擎天古木，却在枝枝桠桠开出了变异的奇特的花。噢那也是我，何尝不是，只是我不想以那些花的样子被人记住——无论是美是丑——而我执着于做一棵木讷古板的大树。大概与人之间的距离，永远不要企图比与上帝的近，要么就会迷失。 还是给自己定一个计划。爱上帝，爱自己，爱邻居 ❤ 即将27岁，向expectation启程。 04/16: 想写两个濒死瞬间。也许没有那么夸张，且听我细细说来。 大概是这周二的时候，在上爵士课。也不过是平常都做的Leap，那一天也许屏息凝气的缘故，起跳落地后心脏却止不住地狂跳，似是不受控制、几近跳出胸腔。我毫无心脏病史，却在那一刻以为自己是心脏病突发：说不出话，不敢大声喘，站立不动任由思绪洗刷。奇怪的是那一刻，思绪很简单：真的……这就是我的一生了吗……才拿到offer，还未来得及赚钱，好遗憾。真的，就要去见上帝了吗……想想是否有所亏欠，心下顿时一安。若是就这样见主面也未尝不好吧。所幸缓和。 周四再上Jazz。还是相同的跳跃，仍然出现类似的体验，虽然好点。想问，真的，这么年轻就让我去了吗……是不是因为我在世上已是顶峰，爱主的，愿意为主做事的我是不是在我看不见的未来已经变成了丑恶自私的嘴脸，所以上帝让我停留在最美的年华。真的好遗憾，21岁信主，如今，带多少人归主了吗？我的双亲，我的挚爱，如今仍有懵懂不识祂。 写在这里，是因为有一个祷告。无论我将来路如何，请主管教。若我真的作恶，不尽主内忠心，求主收去我的生命。无论结局如何，我都愿意承受。 另外，我的愿望太多，愿意付出的时间却太少。到底如何制服这智障的己身啊啊啊。 04/24: 过了一个27岁的生日，从此把“我27岁”挂在嘴边。所以有点喜欢自己的坦荡荡。岁月于我大概并不是一把无情的杀猪刀 —— 我爱27岁的自己，于18岁的自己更甚。曾经的纯真如今并未完全失去，只是如今渐渐学会妥协，这并不是不美。 于是我还是需要展望一下27岁这一年是怎样的生活。大概要开始工作了，大概渐渐要考虑成家了。大概我还是不会变，爱的爱好收住，家里要有一块很大的羊绒毯子和小熊靠背，看看书，学学中医，躺在沙发上记笔记。不想再有一搭没一搭地爱着上帝，做一个愿意委身的基督徒，不要再不冷不热下去。然后做一个爱读书的人，有专业的领域。 所以大概就是这样吧，大概这样就能让自己满意。新的一岁，Yoyo加油。","tags":""},{"title":"Optimizing SQL Statements","url":"/2017/03/27/Optimizing-SQL/","text":"PART I Database LevelChoice of Storage EngineThe most commonly used storage engines are: InnoDB: Default setting for MySQL. Pros: A transactional storage engine. That means it supports rollback / commit (All or Nothing), rather writing every change directly into disk (though by default auto commit is on, but you can use SET AUTOCOMMIT=0; ). Row-level locking. That means, it allows concurrent access to one table. Better to use when you will have multiple read/ write sections simultaneously. Foreign key constraints. It automatically checks foreign integrity. Supports large buffer pool for both DATA and index. While MyISAM only supports buffer for index. Cons: Can’t be compressed for fast access in system table space. But it could have compress tables in general table space or file per table space. No full text indexing MyISAM: a non-transactional storage engine. Pros: It is designed for FAST READ. In a situation that the read is frequent while the write is little (read-write-ratio&lt;15%). Especially good for extensive SELECT queries. It’s frequently used in data warehousing. Full text indexing Cons: No foreign key constraints. Non-transactional. thus no roll-back capability. Row limit: $2^{32}$, about 4.3 billion records at maximum. Besides, other available storage engines include: CSV engine: stores data in CSV. Easily integrated with other applications. Archive storage engine: optimized for high speed inserting task. To see support engines in the database, 1show storage engines; To see engines currently used for each table in the database, 1SHOW TABLE STATUS FROM $YourDatabaseName\\G Sample Output: 123456789101112131415161718192021222324mysql&gt; SHOW TABLE STATUS FROM airline\\G*************************** 1. row *************************** Name: DIM_AIRLINE Engine: InnoDB Version: 10 Row_format: Dynamic Rows: 16256 Avg_row_length: 97 Data_length: 1589248Max_data_length: 0 Index_length: 1835008 Data_free: 0 Auto_increment: NULL Create_time: 2017-03-18 21:54:52 Update_time: NULL Check_time: NULL Collation: latin1_swedish_ci Checksum: NULL Create_options: Comment: *************************** 2. row *************************** Name: DIM_AIRPORT Engine: InnoDB(and more) To alter engine, 1ALTER TABLE $YourTableName ENGINE=&apos;MyISAM&apos;; Optimizing Buffering and CachingThe caching size should be large enough to hold frequently queried data and smaller than the memory size of the instance. InnoDB: Default caching size: 128 MB Show the current caching size: 1SELECT @@innodb_buffer_pool_size/1024/1024/1024; Sample results: 123456+------------------------------------------+| @@innodb_buffer_pool_size/1024/1024/1024 |+------------------------------------------+| 0.125000000000 |+------------------------------------------+1 row in set (0.00 sec) Change the caching size: 1mysqld --innodb_buffer_pool_size=8G --innodb_buffer_pool_instances=16 MyISAM: Key cache buffer is designed for index only and allows concurrent access. Show the current key cache buffer size: 1234567mysql&gt; SHOW VARIABLES LIKE &apos;key_buffer_size&apos;;+-----------------+----------+| Variable_name | Value |+-----------------+----------+| key_buffer_size | 16777216 |+-----------------+----------+1 row in set (0.00 sec) Change the caching size: 1SET GLOBAL key_buffer_size = 26777218; ​ A suggested strategy for MyISAM is, though, to divide cache into three parts: hot cache for read intensive index, cold cache for write intensive tables, and a default warm cache for other operations. ​ MySQL Query Cache: Stores query results. Useful when data tables doesn’t change a lot and queries are similar, such as web services. Show the current query cache size: 1234567mysql&gt; SHOW VARIABLES LIKE &apos;query_cache_size&apos;;+------------------+----------+| Variable_name | Value |+------------------+----------+| query_cache_size | 16777216 |+------------------+----------+1 row in set (0.00 sec) Change the caching size: 1SET GLOBAL query_cache_size = 46777216; PART II Table CreationTable CompressionTable compression is very useful for read-intensive applications. It helps particularly when there is a lot of character string columns. Why? “Because data compression enables smaller database size, reduced I/O, and improved throughput, at the small cost of increased CPU utilization.” Two ways to use compressed table for InnoDB: Upon table creation As compression is not enabled in the system table space (that is where there contains system files and we normally create tables), we need to create a general table space first. 1234-- Create a table spaceCREATE TABLESPACE `ts2` ADD DATAFILE &apos;ts2.ibd&apos; FILE_BLOCK_SIZE = 8192 Engine=InnoDB;-- Create a compressed tableCREATE TABLE t4 (c1 INT PRIMARY KEY) TABLESPACE ts2 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8; Alter table 1Alter table t4 ROW_FORMAT=COMPRESSED; For MyISAM engine, compressed table is generated with myisampack tool. Right IndexingWith clustered index, the table is sorted when being stored on disk. A non-clustered index is a row locator (RiD), it’s good to use in small table, but it finds a row required iterating through the whole table which is very bad for large table. In default, MySQL makes primary key as clustered index. Else MySQL will find the first all not null column as index (might be pretty long though!). According to MySQL documentation, “Accessing a row through the clustered index is fast because the index search leads directly to the page with all the row data. If a table is large, the clustered index architecture often saves a disk I/O operation when compared to storage organizations that store row data using a different page from the index record. (For example, MyISAM uses one file for data rows and another for index records.)” Other than clustered indexing (primary key), we could still set secondary indexing. The indexing will then contain data for the column which it indexes to, and the primary key. As you can see, without indexing, where clause has to go over the whole table to look up value. 123456789101112131415mysql&gt; explain select avg(carrier_delay) from FACT_FLIGHT_3_SAMPLE where unique_carrier = &apos;b6&apos; group by unique_carrier\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: FACT_FLIGHT_3_SAMPLE partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 50000 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) When indexing is created for the column of interests, the where clause is executed without accessing the actual table. 12345678910111213141516171819mysql&gt; CREATE INDEX carrier_ix ON FACT_FLIGHT_3_SAMPLE (UNIQUE_CARRIER ASC);Query OK, 50000 rows affected (0.10 sec)Records: 50000 Duplicates: 0 Warnings: 0mysql&gt; explain select avg(carrier_delay) from FACT_FLIGHT_3_SAMPLE where unique_carrier = &apos;b6&apos; group by unique_carrier\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: FACT_FLIGHT_3_SAMPLE partitions: NULL type: refpossible_keys: carrier_ix key: carrier_ix key_len: 9 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) According to experiment on data table with one billion entries, the time spent with indexing is only 50% without indexing. PART III Query LevelUse EXPLAINExplain is used to show a query execution plan. It helps understand what we are trying to do: how many operations are needed and how many rows are affected. Examples could be found in previous section. Queries Isolate “Isolate and tune any part of the query, so that a function could be called once for every row in the result set rather than in the whole table.” Aggregate functions: “The most efficient way to process GROUP BY is when an index is used to directly retrieve the grouping columns. With this access method, MySQL uses the property of some index types that the keys are ordered (for example, BTREE). This property enables use of lookup groups in an index without having to consider all keys in the index that satisfy all WHERE conditions. “ Reference MySQL Storage Engines When to use MyISAM and InnoDB SQL query performance killers – understanding poor database indexing Optimization Note* All quotes are from Reference 4 Optimization - the official documentation of MySQL’s.","tags":"mysql data-warehousing"},{"title":"三月 浮世绘","url":"/2017/03/21/三月-浮世绘/","text":"03/21: 之前写了满满一篇，但不知为何没有保存。想来就觉得怠惰，不想补上当时的思绪。但我的话总是那么多的，没有出口再憋半个月好为难我，还是再开一篇吧。只是我想大概Vegas的内容就不发了，一段美好却筋疲力竭的旅行多嚼两遍，怕日后回忆起的只会是未流淌于笔下的酷暑和不小心吃到的不新鲜的生蚝。多不好啊。文字的存在只是记叙那些无法用力在大脑保存下来的东西。 于是还是要从Vegas说起。有一部电影，亦或是有一句话叫闻香识女人，最近才可以体会到。前几年在上海商场一见钟情的Oh!Lola，如今已经沦为空气清新剂了。大概是过了那个年龄，就不再喜欢没有底蕴一味甜腻的香型。那日在免税店闻到一株Black Orchid,心内种了一片大草原。有时候香水这个东西也是蛮烦人的，价格高却偏偏割舍不得，喷得浓了也不见得身边都是同好不过对牛弹琴，喷得淡了又觉得拳头打在棉花上，自己闻不到呢大抵别人也闻不到，喷了一瓶自欺欺人。 但是我还是没有买。毕竟现在作为穷学生，也作为找工作焦头烂额如火如荼的穷学生，要留一下喜欢的东西励志用，“当你成功时”。 近期是我人生中第一次主动追一篇悬疑小说，《这个电影我穿过》。小说具体内容也不必提，大概好几年后我都不会忘记边看边给舍友发短信让她早点回家的害怕。大概我下次看恐怖小说是真的要猴年马月了。于是把小说和香水的事件放在一起提，体现了散文的中心思想，形散神不散，主题都是我长大了，啊哈哈哈哈。 二月时写道他乡的月亮别样圆，三月就换一句，the grass is always greener on the other side of the fence. 和教堂姐姐谈起为什么出国的原因，其中包括了宗教自由，她也和我渐渐说了美国最近在关心的一些问题。比如转性别厕所。这个话题大爆是一个妈妈在facebook上发了一个可爱女孩子的照片，副题——你忍心让这样可爱的女孩子一辈子只能上男厕所吗？于是开始了，媒体出动，明星秀出动，政治家出动，甚至电视剧出动，呼吁大家正确看待变性人。教堂姐姐还和我提到她在的学校已经有小学生变性了。我不喜欢这样的事件，很不喜欢。美国对于自由的追求已经过了头，只要是和关键字眼沾边，就会有大票的无脑支持。可是有时候，这样的自由意味着自我不接纳。多小的小孩啊，因为看了电视剧，因为看了身边有朋友变性，然后就做出了这样的大选择，抛弃自己原有的性别，一辈子。大概是我真的不能懂。我赞同对于已经变性的人的接纳，但我不赞同大肆宣扬变性尤其是儿童变性，上帝原先造我们的样子其实都是美好的，为什么不能接受呢。记得小时候我也埋怨过我黑我胖我瘦我情商不高，但渐渐学着欣赏自己，和自己玩，因为自己开心，真的是很美好的事情。可是快餐时代没有等待。 三月，不知何时桃花开？入三月的那天躺在床上想着三月桃花诗，想了半天只想到一首“人间四月芳菲尽，山寺桃花始盛开”。于是怅然。大抵在都市待久了的人心底都不再有四季的变化，所以古人换季吟诗出门吟诗喝酒吟诗，现代人只能继续“哈哈哈”和“666”。真是没意思透顶了。遇到树，不知是何树，遇到花，不欣赏什么花，网络世界把我们的脑海牢牢吸住，但回头一看空空如也。但，诚然，如果让我过原始人生活，我也绝对不干，哈哈哈哈。 吐槽有瘾。终于清空内存，专心学术去啦。 03/23: 黑人大概都是天生的演讲家。 昨晚打了一辆Uber，上车的时候是一个黑人司机，内心打了一个寒颤。典型黑人给我的印象：高，壮，放着黑人的饶舌音乐，痞痞的样子。我不知道要聊什么，甚至连how are you都不说。大概是觉察到我的害怕或者尴尬，他突然开口了，其实也没什么特别的，就抱怨了一下路上太坑坑洼洼。只是黑人说话语气就和唱歌似的，特别有韵律感，又或者他的表达方式逗极了，哈哈哈哈哈，所以我从他开口的那一瞬间就笑到我下车，中间狂喝三口水因为笑得口渴。 事后想想自己内心最初的地图炮，都想打自己一巴掌。 03/27: 昨晚玩了一场酣畅淋漓的狼人杀。也不是昨晚，大概是下午3点半到晚上10点半；也不是一场，大概是六场。 真的很喜欢这个游戏，玩了这么久终于摸到了一些边，做一个狼人小总结~ 大部分人抿人是看状态，看站队，和除非明显聊爆了的逻辑。当狼最重要的就是打一盆鸡血，该开心的时候开心，该沉重的时候沉重，心思不要太重。 一个时间不能踩(得罪)太多人。民，甚至是神发言肯定也都会有逻辑漏洞了，抓准一两个点进行煽动发言。就算无法获得场上所有人的同意，你也可以被当成愚民混过一轮，安心举票。 神的特点，责任感，或者特别强势，而且关注自己的person of interest （比如女巫和猎人全场在找要毒或带的狼，守卫在找真的预言家或神）。民的特点，认民（或首轮认神），不悍，可能踩错人，逻辑线不清晰。 变票可以，要有收益。如果变票出神了不吃毒，白天大部分精力要放在表水上，说清自己的心路历程。 03/28: 还未从斩断了尾巴的二月中醒过神来，三月又被上旬的期盼和中旬的浪漫淹没，回过神来，又是一个月要过完了。四月份，就不得不惶恐——离毕业也只剩下一个半月了，然而工作还没有着落。连万事不上心得万胖胖也急了，亲自操刀上马整理了达拉斯所有的工作发给我。身边的朋友也渐渐在问我为什么找工作进度这么落后，开始想要帮助我。我也慌过了，考虑过转码了。然而今日在此写下这一段文字，希望回首看起的时候不觉得自己太天真。我想我拥有某方面的天赋和热忱并非偶然。也许在最开始，上帝已经设定好了我未来的可能性。然而我不希望被压力和恐惧打败，就算进度慢我也想一步一个脚印最自己擅长也喜欢的事情。加油yoyo :)","tags":""},{"title":"Kalman Filter and Its Applications","url":"/2017/03/20/Kalman-Filter/","text":"IntroductionKalman Filter, an artificial intelligence technology, has been widely applied in driverless car navigation and robotics. It works well in presence of uncertainty information in dynamic systems. It is a linear solution based on Bayesian Inference especially for state space models. – This is the definition in the hard way. Lets skip the first paragraph and look at a little story. Source: Kalman Filter: Theories and ApplicationsA group of young men were standing, under their feet there was a twisty narrow road to a very big tree. One man walked to the tree, asking, “can any of you walk to the tree with your eyes closed?” “That’s simple, I used to serve in the army.” Mike said. He closed his eyes and walked to the tree like a drunk man. “Well, maybe I haven’t practiced for long.” He murmured. - Depending on prediction power alone. “Hey, I have GPS!!” David said. He held the GPS and closed his eyes, but he also walked like a drunk man. “That’s very bad GPS!” He shouted, “it’s not accurate!!” - Depending on measurement which has big noises “Let me try.” Simon, who also served at the army before, grabbed the GPS and then walked to the tree with his eyes closed. - Depending on both of prediction power and measurement After reaching the tree, he smiled and told everyone, “I am Kalman.” In the story above, a good representation of the walking state at time k is the velocity and position. $$ X_k = (p, v) $$ So there are two ways to measure where you are. you predict based on your own command system - it records every commands sent to you, but only some of commands are executed exactly as what they were - wheels may slip or wind may affect; you measure by your GPS system - it measures where you are based on satellite, but it can’t be as accurate as in meters and sometimes signals lost. With Kalman Filter, we will get better understanding of where you are and how fast you go than either of the prediction or measurement. That is, we update our belief of where you are and how fast you go by incorporating the two sources of predictions and measurements using Bayesian inference. Understanding Kalman FilterNote: all the knowledge and photos for this section come from Ref 2. This is a study note only. The whole idea of Kalman Filter can be represented by a single picture. It might look complicated at this moment, but we will understand everything after this article (if not, read Ref 2 - it’s a much nicer article I believe). [source: ref 2] Prediction Phase: $X_k, P_k, F_k$$X_k$: Position and VelocityRemember in our scenario, we want to know the position and velocity. We represent the state of the walking people at time k as $X_k = [position_k; velocity_k]$. $F_k$: Prediction MatrixWe have $ Position_k = Velocity_{k-1} * t + Position_{k-1} $$ Velocity_k = Velocity_{k-1} $ The above two formulas could be written as: $ \\begin{bmatrix} Position_k \\\\ Velocity_k \\end{bmatrix} = \\begin{bmatrix} 1 &amp; t \\\\ 0 &amp; 1 \\end{bmatrix} * \\begin{bmatrix} Position_{k-1} \\\\ Velocity_{k-1} \\end{bmatrix} $ We represent the prediction matrix $\\begin{bmatrix} 1 &amp; t \\\\ 0 &amp; 1 \\end{bmatrix} ​$ as $F_k​$. We have then $ X_k = F_k * X_{k-1} ​$ $P_k$: Covariance MatrixSince in our case, the faster the robot walks, the further the position might be. Therefore velocity and position should be correlated; the covariance matrix is $P_k$ . When prediction matrix updates X_k, the change will also reflect on covariance matrix. Therefore $ P_k = F_k * P_{k-1} F_k^T $ . Measurement Phase: $Z_k, H_k$$H_k$: Measurement FunctionSometimes the GPS reading is not having the same units with the prediction states. For example, we use km/h in prediction states and we use m/s in GPS reading for velocity. So we need a transformation with matrix $H_k$: $$ X_k = H_k X_k $$ $$ P_k = H_k P_kH_k^T $$ $Z_k$: MeasurementRemember the GPS reading is not very reliable and might have some variations. So it is represented as a Gaussian distribution with mean $Z_k = [PositionReading_k, VelocityReading_k]$. So in below picture the pink circle represents the prediction distribution while the green circle represents the measurement distribution. The bright bubble insight represents the belief distribution of position and velocity. [source: ref 2] External Factors: $R_k, Q_k, B_k, U_k $$R_k, Q_k$: noises$Q_k$ is the transition covariance for the prediction phase. $ P_k = F_kP_{k-1}F_k^T + Q_k $. The idea is that there would always be some uncertainty. Therefore we kept an assumption - points might move a bit outside its original path. In practice, the value is often set very slow, for example: 12delta = 1e-5Q = np.array([delta/1-delta, 0; 0, dealta/1-delta]) $R_k$ is the observation covariance for the measurement phase. $ P’_k = P’_{k-1} + R_k $. Default set as 1. $B_k, U_k$: External influenceFor example, if the people are going down from the mountain, he might walker quicker because of the gravity. Therefore we have $$ Position_k = Velocity_{k-1} * t + Position_{k-1} + gt^2 /2 $$ $$ Velocity_k = Velocity_{k-1} + gt $$ Therefore we have $$ X_k = F_k X_{k-1} + \\begin{bmatrix} t^2/2 \\\\ t \\end{bmatrix} g = F_k X_{k-1} + B_k U_k $$ where $ B_k $ is the control matrix for external influence and $U_k​$ is the control vector. The Big Boss: Kalman Gain ?We are almost done explaining all the variables in Kalman Filter, except a very important term: Kalman Gain. This is a bit complicated, but luckily, this is not something we need to calculate or input. Now let’s go back to the measurement phase once more. When we multiplying the prediction distribution and measurement distribution, the new mean and variance go like this: $$ u’ = u_0 + \\sigma_0^2 (u_1 - u_0)/ (\\sigma_0^2 + \\sigma_1^2) $$ $$ \\sigma’^2 = \\sigma_0^2 - \\sigma_0^4/(\\sigma_0^2 + \\sigma_1^2) $$ so we make: $ k = \\sigma_0^2/ (\\sigma_0^2 + \\sigma_1^2) $ where k is the kalman gain, therefore we can simplify the above equations to: $$ u’ = u_0 + k(u_1 - u_0) $$ $$ \\sigma’^2 = (1-k)\\sigma_0^2 $$ Therefore Kalman gain helps updating the new $X_k, P_k$ value after seeing the measurement. So what is an intuitive explanation of Kalman Gain? It actually calculates the uncertainty in the prediction phase to the measurement phase, so it tells how much we should trust the measurement when updating $X_k, P_k$. Python ImplementationI’d love to recommend a great post which gives applications of Kalman Filter in financial predictions with codes posted on its Jupyter Notebook. It demonstrates why we should use Kalman Filter comparing to linear regression just in one picture: [source: ref 3] Parameter MappingRecall: Kalman Filter measures uncertain information in a dynamic systems. In this case, we want to know the hidden state slope and intercept. Let’s map all the inputs from theoretical to practical settings. Prediction Phase State: $ X_k = \\begin{bmatrix} \\alpha_k \\\\ \\beta_k \\end{bmatrix} $ Prediction Matrix: $ F_k = \\begin{bmatrix} 1&amp;0 \\\\ 0&amp;1 \\end{bmatrix} $. This is because we assumes the slope and intercept aren’t correlated. Intial State Covariance $ P_0 = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 1 \\end{bmatrix} $ Measurement Phase Measurement Function $ H_k = \\begin{bmatrix}EWA &amp; 1 \\end{bmatrix} $. The measurement we have is EWC. It’s obvious that it doesn’t share the same measuring units with slope and intercept. Since we have EWC = EWA*slope + intercept, therefore the measurement function should be [EWA 1]. Measurement Mean $Z_k = EWC$. External Factors Transition Covariance $Q = \\begin{bmatrix} 1e-5/ (1-1e-5) &amp; 0 \\\\ 0 &amp;1e-5/(1-1e-5) \\end{bmatrix} $ Observation Covariance R = $1$ Note that, the selection of Q and R here means the author wants to trust more in the prediction phase rather than the measurement phase. Or, as suggested in PyKalman documentation, values for $P_0, Q, R$ could be initialized using: 12kf = KalmanFilter(em_vars=[&apos;initial_state_covariance&apos;, &apos;transition_covariance&apos;, &apos;observation_covariance&apos;])kf.em(X, n_iter=5) Implementation CodesHere is the code source. I copied it here only for easy reading. 123456789101112131415161718192021import numpy as npfrom pykalman import KalmanFilter### construct the covariance matrix Q and measurement function Hdelta = 1e-5trans_cov = delta / (1 - delta) * np.eye(2)obs_mat = np.vstack([data.EWA, np.ones(data.EWA.shape)]).T[:, np.newaxis]### construct Kalman Filterkf = KalmanFilter(n_dim_obs=1, n_dim_state=2, initial_state_mean=np.zeros(2), initial_state_covariance=np.ones((2, 2)), transition_matrices=np.eye(2), observation_matrices=obs_mat, observation_covariance=1.0, transition_covariance=trans_cov)### get resultsstate_means, _ = kf.filter(data.EWC.values)slope = state_means[:, 0]intercept = state_means[:,1] Application in Dynamic RoISimilarly, diminishing marketing RoI could be measured in this way. We always write $$ Sales = Marketing Investment * RoI + Intercept $$ However, as time past by, the RoI should also be diminishing. So with Kalman Filter, the changing RoI could be captured. Meanwhile, Intercept also composite of historical sales, industry trends, buzz news etc and could be analyzed deeper. Reference Kalman Filter: Theories and Applications How a Kalman Filter works, in pictures Online Linear Regression using a Kalman Filter Estimating the Half Life of Advertisement, Prasad Naik, 1999 How to understand Kalman Gain intuitively pykalman documentation","tags":"machine-learning kalman-filter"},{"title":"Deep Learning Review Notes","url":"/2017/03/19/Deep-Learning-1/","text":"Key Layers in a CNN Network Convolutional neural networks make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies). - AlexNet Convolutional LayerA convolutional layer compromise multiple convolutional kernels (image kernels). Below we will introduce what is an image kernel, why there are multiple image kernels than one, some computational details, and why a smaller image kernel is preffered in most recent research and models. What is an image kernel ​ image source So above picture demonstrates perfectly how a kernel of 3*3 with stride =1 works. Basically it slides through the picture one pixel a time (thus stride = 1), and performs element wise multiplication. Why do we need this? Here is a more straightforward summary of what a kernel can do: ​ image source Besides the above demonstrations, image kernels could also retain certain colors of the image, or bottom sobel etc. Here is a link you can play with and get more understanding towards image kernels. Why there are multiple image kernels in a convolutional layer?Here is a good answer (by Prasoon Goyal) for the first question: But clearly, why would you want only one of those filters? Why not all? Or even something that is a hybrid of these? So instead of having one filter of size 5×55×5 in the convolution layer, you have kk filters of the same size in the layer. Now, each of these is independent and hopefully, they’ll converge to different fitlers after learning. Here, k is the number of output channels. kk could be taken anywhere between few tens to few thousands. How to compute?So, the input of a convolutional neural network ususally has three dimensions: long, wide, and color dimension (if grey scale then two dimensions). Here is a good demo of how things work. Explained in text, if the input source is 7*7*3, and we have 2 kernels of size 3*3*3 with stride 2: The input could be seen as 3 stacked 2d 7*7 matrices; and each kernel could be seen as 3 stacked 3*3 matrix; Each stacked layer of the kernel will slide through the corresponding stacked layer of input with step of 2 pixels each time, and the element sum will be furthur added together. The output after each kernel sliding will then be a 33 2d matrix. Here is why: (input width - kernel width + 2 zero padding)/stride + 1 = (7-3+0)/2 + 1 = 3. As we have two kernels, so the output volume will be 3\\3*2. Smaller kernel size preferredAnother important thing to know is that, smaller kernels capture more details of pictures. This helps to understand the evolvement on CNNs. A demonstration is as below: Kernel size: 3*3 Kernel size: 10*10 image source Pooling LayerPooling layer is often used after convolutional layer for down sampling. It reduces the amount of parameters carried forward while retaining the most useful information; thus it also prevents overfitting. A demonstration for max pooling could be shown in following picture: ​ Image source Some visualizations could be found below: Feature Map After Pooling image source Another example (image source): Fully Connected LayersFor fully connected layers we have $$ output = activation(dot(input, kernel) + bias) $$ Some of the key activation functions are as follows: SigmoidSigmoid function pushes large positive numbers to 1 while large negative numbers to 0. However it has two fallbacks: 1) It will kill the gradient. If the value of a neuron is either 0 or 1, the gradient for the neuron will become so closed to zero that, it will “kill” the multiplication results for all gradients in back propagation computation. 2) The sigmoid output are all positive. It will cause the gradient on weights become all positive or all negative. [source: 3] TanhTanh activation is a scaled version of sigmoid function: $$tanh(x)=2σ(2x)−1tanh⁡(x)=2σ(2x)−1$$ Therefore it is zero centered with range [-1,1]. It still have the problem of killing gradient, but generally it is preferred to sigmoid activation. ReLUShort for Rectified Linear Units. A popular choice. It threshold upon 0. $$ max (0, x) $$ Comparing to the previous two activation methods, it’s much quicker to converge and involves much less computation time due to linearity. And it doesn’t have the issue of non-zero centered. However, it should be noted, if the learning rate is set to be high, part of the neurons will “die” - they will be not activated during the whole training phase. With the learning rate set to be smaller, it won’t be much an issue. And below is a demonstration of how ReLU activation looks like: ​ image source SoftMaxA very common choice for multi-class output activation. Batch Normalization Layer This type of layer turns out to be useful when using neurons with unbounded activations (e.g. rectified linear neurons), because it permits the detection of high-frequency features with a big neuron response, while damping responses that are uniformly large in a local neighborhood. It is a type of regularizer that encourages “competition” for big activities among nearby groups of neurons.” - AlexNet Batch normalization is a common practice in deep learning. In machine learning tasks, scaling with zero mean and one standard deviation will make the performance better. However, in deep learning, even if we normalize the data at the very beginning, the data distribution will change a lot in deeper layers. Therefore with batch normalization layer, we could always do data preprocessing again. It is often used right after the fully connected layer or convolutional layer, before the non-linear layers. It makes a significant difference and becomes much more robust to bad initializations. Note that, Fei-Fei Li claims the contributions of batch normalization is minimal. And the use of local response normalization could hardly be seen in recent year models. Drop Out LayerDrop out layer is a common choice to prevent over fitting. It’s fast and effective. It will keep some neurons activated or 0 according to probabilities. Sample Architecture and CodesSample architecture for convolutional neural network is as follows: [source: 3] Sample codes for MNIST solution using keras deep learning as follows: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122'''Transfer learning toy example:1- Train a simple convnet on the MNIST dataset the first 5 digits [0..4].2- Freeze convolutional layers and fine-tune dense layers for the classification of digits [5..9].Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_transfer_cnn.pyGet to 99.8% test accuracy after 5 epochsfor the first five digits classifierand 99.2% for the last five digits after transfer + fine-tuning.'''from __future__ import print_functionimport datetimeimport kerasfrom keras.datasets import mnistfrom keras.models import Sequentialfrom keras.layers import Dense, Dropout, Activation, Flattenfrom keras.layers import Conv2D, MaxPooling2Dfrom keras import backend as Know = datetime.datetime.nowbatch_size = 128num_classes = 5epochs = 5# input image dimensionsimg_rows, img_cols = 28, 28# number of convolutional filters to usefilters = 32# size of pooling area for max poolingpool_size = 2# convolution kernel sizekernel_size = 3if K.image_data_format() == 'channels_first': input_shape = (1, img_rows, img_cols)else: input_shape = (img_rows, img_cols, 1)def train_model(model, train, test, num_classes): x_train = train[0].reshape((train[0].shape[0],) + input_shape) x_test = test[0].reshape((test[0].shape[0],) + input_shape) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # convert class vectors to binary class matrices y_train = keras.utils.to_categorical(train[1], num_classes) y_test = keras.utils.to_categorical(test[1], num_classes) model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy']) t = now() model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) print('Training time: %s' % (now() - t)) score = model.evaluate(x_test, y_test, verbose=0) print('Test score:', score[0]) print('Test accuracy:', score[1])# the data, shuffled and split between train and test sets(x_train, y_train), (x_test, y_test) = mnist.load_data()# create two datasets one with digits below 5 and one with 5 and abovex_train_lt5 = x_train[y_train &lt; 5]y_train_lt5 = y_train[y_train &lt; 5]x_test_lt5 = x_test[y_test &lt; 5]y_test_lt5 = y_test[y_test &lt; 5]x_train_gte5 = x_train[y_train &gt;= 5]y_train_gte5 = y_train[y_train &gt;= 5] - 5x_test_gte5 = x_test[y_test &gt;= 5]y_test_gte5 = y_test[y_test &gt;= 5] - 5# define two groups of layers: feature (convolutions) and classification (dense)feature_layers = [ Conv2D(filters, kernel_size, padding='valid', input_shape=input_shape), Activation('relu'), Conv2D(filters, kernel_size), Activation('relu'), MaxPooling2D(pool_size=pool_size), Dropout(0.25), Flatten(),]classification_layers = [ Dense(128), Activation('relu'), Dropout(0.5), Dense(num_classes), Activation('softmax')]# create complete modelmodel = Sequential(feature_layers + classification_layers)# train model for 5-digit classification [0..4]train_model(model, (x_train_lt5, y_train_lt5), (x_test_lt5, y_test_lt5), num_classes)# freeze feature layers and rebuild modelfor l in feature_layers: l.trainable = False# transfer: train dense layers for new classification task [5..9]train_model(model, (x_train_gte5, y_train_gte5), (x_test_gte5, y_test_gte5), num_classes) code source: keras documentation Parameter TuningLosses Regression: Mean_squared_error, Mean_absolute_error, mean_absolute_percentage_error, mean_squared_logarithmic_error Classification: two most commonly used: squared_hinge, cross entropy for softmax output1) hinge: hinge, squared_hinge2) cross entropy: categorical_crossentropy, sparse_categorical_crossentropy, binary_crossentropy Optimizers Batch Size: It means the number of training examples in one forward-backward training phase. If the batch size is small, it requires less memory and the network trains faster (the parameters will be updated once a batch). If the batch size is large, the training takes more time but will be more accurate. Learning Rate: If the learning rate is small, it will takes so long to reach the optimal solution; if the learning rate is large, it will stuck at some points and fail to reach optimal. So the best practice is to use a time based learning rate - it will decrease after each epoch. How? Use parameter decay - a common choice is 1e-2. $$ lr = self.lr (1. / (1. + self.decay self.iterations)) $$ Momentum (used in SGD optimizer): It helps accelerating convergence and avoid local optimal. A typical value is 0.9 . Comparison between Deep Learning Frameworks Theano: a pretty old deep learning framework written in Java. Raw Theano might not be perfect but It has many easy-to-use APIs built on top of it, such as Keras and Lasagne. (+) RNN fits nicely (-) Long compile time for large models (-) Single GPU support (-) Many Bugs on AWS TensorFlow: a newly created machine learning framework to replace Theano - but TensorFlow and Theano share some amount of the same creators so they are pretty similar. (+) Supports more than deep learning tasks - can do reinforcement learning (+) Faster model compile time than Theano (+) Supports multiple GPU for data and model parallelism (-) Computational graph is written in Python, thus pretty slow Caffe: mainly used for visual recognition tasks. (+) Large amount of existing models (+) CNN fits nicely (+) Good for image processing (+) Easy to tune or train models (-) needs to write extra codes for GPU models (-) RNN doesn’t fit well so not good for text or sound applications Deeplearning4J: a deep learning library written in Java. It includes distributed version for Hadoop and Spark. (+) Supports distributed parallel computing (+) CNN fits nicely (-) Takes 4X computation time than the other three frameworks Keras: An easy-to-use Wrapper API for Theano, TensorFlow and Deeplearning4J. It supports all the functionality that TensorFlow supports! Pre-trained ModelsWhat are Pre-trained Models?Pre-trained models are those models that people train on a very large datasets, such as ImageNet (it has 1.2 million images and 1000 categories). We could either use it as a start point for the deep learning tasks to raise accuracy, or use them as a feature extraction tool and feed the features generated with pre-train models into other machine learning models (e.g. SVM). Pre-trained Models: A ComparisonSome of the pre-trained models for image tasks include: ResNet, VGG, AlexNet, GoogLeNet. We use top-1-error and top-5-error to represent the accuracy on ImageNet. Top-1-error is just 1- accuracy; top-5-error measures if the true label resides in the 5-most-probable labels predicted. Release Model Name Top-1 Error Top-5 Error Images per second 2015 ResNet 50 24.6 7.7 396.3 2015 ResNet 101 23.4 7.0 247.3 2015 ResNet 152 23.0 6.7 172.5 2014 VGG 19 28.7 9.9 166.2 2014 VGG 16 28.5 9.9 200.2 2014 GoogLeNet 34.2 12.9 770.6 2012 AlexNet 42.6 19.6 1379.8 Tensorflow has a recent package Slim that implements more advanced models including Inception V4 etc. Right now the lowest top-1-error is 19.6% by Inception-Resnet-V2. Reference Comparing Frameworks: Deeplearning4j, Torch, Theano, TensorFlow, Caffe, Paddle, MxNet, Keras &amp; CNTK Deep Learning with Theano, Torch, Caffe, TensorFlow, and Deeplearning4J: Which One Is the Best in Speed and Accuracy? CS231n Convolutional Neural Networks for Visual Recognition Pretrained Models An overview of gradient descent optimization algorithms Keras: Deep Learning library for TensorFlow and Theano An Intuitive Explanation of Convolutional Neural Networks ​","tags":"machine-learning tensorflow cnn"},{"title":"This is my prayer","url":"/2017/03/10/my-prayer/","text":"Open my eyes that I may see wonderful things in your law. Psalms 119:18 If any of you lacks wisdom, you should ask God, who generously give to all without finding fault, and it will be given to you. James 1:5 Blessed are the pure in heart, for they will see God. Matthew 5:8","tags":""},{"title":"DS Interview: Classification","url":"/2017/03/09/Interview-Classification/","text":"Definition Explain what logistic regression is. How do we train a logistic regression model? How do we interpret its coefficients? Logistic Regression is mapping inputs to probabilities. The sigmoid function turns a non-differentiable cost function to a convex one. So training a logistic regression model is a matter of optimization problem, we can deploy gradient descent ( cost function) or gradient ascent ( likelihood function). The coefficient means unit of increase of the log odd [log P(y=1) - log P(y=0)] for one unit increase of a predictor, given other predictors constant. How do random forests work (in layman’s terms)? Random forest works in the following way: First, the random forests compromise of decision trees (that’s why it is a random forest!). The decision trees learn and VOTE; output with majority vote will be taken. Second, the random forests takes an tree bagging (boostrap aggregation) process: each of the decision trees are built with only a subset of the original train data - it samples with replacement. Third, random forests takes a feature bagging process: each of the decision trees will only train on a subset of all features. What is the maximal margin classifier? How this margin can be achieved and why is it beneficial? How do we train SVM? Data with different labels are separated by a hyperplane. Maximal margin classifier minimizes the total distance from the points to the hyperplane. Maximal margin classifier tolerate more noises thus more robust to overfitting. We can train it either using linear programming or gradient descent. What is a kernel? How to choose kernel? Explain the kernel trick. Kernel is legal definition of dot product. Choosing kernel requires domain knowledge, or we can simply apply cross validation. When the data are not seperable in the current feature space, a common approach is to map them into high-dimensional space and then find the hyperplane. The kernel trick is, we don’t have to compute the mapping coordinates of those features in high dimensional space - we only perform inner product multiplication in the current space and then raise to the power of n. Thus it saves a lot of computational resources. Why is naive Bayes so bad? How would you improve a spam detection algorithm that uses naive Bayes? Naive bayes has its name because it assumes all the features to be independent from each other. This is not the case, for example in NLP tasks. Ways to improve Naive Bayes: Complement Naive Bayes: also count words NOT IN the category; Feature engineering, such as retaining words with high kappa, stemming etc; PCA / covariance matrix could be employed. What is an Artificial Neural Network? What is back propagation?It is a classification method that model brain neuras to solve complex classification problems. Back propagation is a “learn from mistake” algorithm for neural network training. Back propagation, in its nature, is gradient descent. It compares the output with the label, and “propagate” the error back onto the previous layers and adjust the weights accordingly. Model Comparison What are the advantages of different classification algorithms? (Decision tree, Naive Bayes, logistic regression, SVM, Random Forest) Decision Tree: Pros: 1, Easy to interpret; 2, more robust to outliers; 3, works for non-linear separable data Cons: 1, Tree structures change; 2, Easy to overfit; 3, Performs relatively worse in linear separable tasks Naive Bayes:Pros: 1, Easy to train, converge quickly; 2, Gives generative story; Cons: 1, Constraint: Independence assumption; 2, Easily affected by outliers Logistic Regression: Pros: 1, Can apply regularization to prevent overfitting; 2, Easy to interpret with probabilities; 3, Well-performed; Cons: 1, Linear separable; 2, Complex for multi-class problem SVM: Pros: 1, Good performance; 2, Can apply different kernel (linear/ non-linear separable) Cons: 1, Slow to train, memory intensive (especially with cross validation for kernel selection); 2, complicated for multi-class problem Random Forests:Pros: 1, Very good performance; 2, Handles large numbers of features; 3, Tells which features matter more; 4, Deals well with missing data Cons: 1, Unlike decision tree, the process is hard to interpret; 2, Was observed to be overfitting on some datasets; 3, Slow to predict How can I classify supervised data that is probabilistic rather than deterministic? Either logistic regression, or linear regression with normalization. Metrics How to define/select metrics? What are benefits and weaknesses of various binary classification metrics? What is the best out-of-sample accuracy test for a logistic regression and why? Provide examples when false positives are more important than false negatives, false negatives are more important than false positives False positive is more important during early stage health scanning. At this moment the cost of delaying small problems are not big as false alarms bringing unnecessary worries to many patients. False negative is more important during cancer related examinations. Misses will cost lives. Cross validation What is cross-validation? Cross validation is a technique to assess how well the model perform on unseen data. It is used for either, model selection, or model performance estimation. Basically it randomly seperates the data into k-folds and train on k-1 folds and test on the last fold. k is always chosen to be 3, 5, 10. What are the pitfalls on relying on cross-validation to select models? Generally we use Cross validation for two purpose, model selection&amp; estimate model accuracy for future use. For model selection, cross validation actually measures “accuracy”. Then it will easily falls into accuracy paradox - that increasing accuracy doesn’t lead to a more desired model (the email spam filter could increase accuracy by setting the rule “no email is spammed”, as TP &lt; FP). So CV is not robust in this sense. For estimate model accuracy - normally it is training and test data falls in the same pattern, but the future data might not have the same pattern as your training/ test data. Potentially data shift exists.","tags":"machine-learning data-science-interviews"},{"title":"DS Interview: Linear Regression","url":"/2017/03/09/Interview-Linear-Regression/","text":"Definition How would linear regression be described and explained in layman’s terms? Linear regression is to find the best fitting line given a bunch of points. The distance to the line is the “error” - because ideally we prefer every point is on that line. Mathematically, we want a line that produces as small total error as possible. That’s what we do in linear regression. What is an intuitive explanation of a multivariate regression? Normally regression only has one outcome (Y), and several predictor variables (X). In Multivariate regression, there are more than one outcomes (Y). What is gradient descent method? Will gradient descent methods always converge to the same point?Gradient descent is an interative algorithm to find optimal solution. Sometimes, for example, in k-means clustering it will converge to different local optimals with different initializations. In regression, yes, it will always converge to the global optimal point. What is the difference between linear regression and least squares? Linear regression is a statistical inference problem / machine learning model; and least squares describes one way to achieve the solution to the problem/ model. Alternatively, we can use MAE as measurement and gradient descent to find the solution to linear regression model. Statistical Perspective What are the assumptions required for linear regression? What if some of these assumptions are violated? First, variables are normal distributed. Can check with histogram/ QQ plot. Second, relationship is linear between independent and dependent variables (not polynomial etc.). Scatter plot is helpful in two dimensional case. Solution: change the model to include polynomial terms. Third, no linear dependency between predictors (no multi-collinearity). Use correlation/ covariance matrix to detect. Solution: dimensional reduction (PCA), select one particular variable from highly correlated variable set, or ridge regression (without expecting the coefficient of one single variable explain much) Fourth, there is no correlation between error terms (no autocorrelation). This means, dependent variables (y) are independent of each other. Use residual plot or Durbin – Watson test. Solution: time-series modelling Last, homoscedasticity. It means the residual remains same as x changes. Scatter plot with x,y,fitting line, residuals is a good measurement. Solution: log transformation, box-cox transformation What is collinearity and what to do with it? How to remove multicollinearity? This means there are at least two predictors are highly correlated. It will make interpretation of coefficients, lead to overfitting, or even failure of inverting the matrix.To remove multicollinearity, there are generally four ways:1, drop affected variables; 2, dimensional reduction (PCA); 3, ridge regression; 4, partial least square regression. Can you derive the ordinary least square regression formula? Regularization What is an intuitive explanation of regularization? Regularization is a technique to prevent overfitting. It enables the model to learn just the right amount of information about the data. Basically it penalizes the model if it goes too detailed that even learn specific patterns that will not necessarily present in the future. What is the difference between L1 and L2 regularization? L1 regularization gives sparse estimation by giving a Lapace prior for coefficients. It will force most of coefficients to be zero.L2 regularization will retain all predictors with Gaussian prior for coefficients. Some of the coefficients get larger and some get smaller. It’s easier to compute. What are the benefits and drawbacks of specific methods, such as ridge regression and lasso? Ridge regression is faster than lasso; lasso regression is a feature selection method itself. But we can’t claim which method to use without observing the distribution of data. Model Tuning/ Selection How do you choose the right number of predictors? One lazy method is lasso regression. We can also go through a feature selection process: First we decide the metric, such as MSE or R square. Then we can use greedy search and cross validation to build models with different combinations of predictors; and evaluate the models with test set. How to check if the regression model fits the data well? Adjusted R square, MSE What is the difference between squared error and absolute error? and Are there instances where root mean squared error might be used rather than mean absolute error? and How would a model change if we minimized absolute error instead of squared error? What about the other way around? Squared error/ root mean squared error - penalize more on big errors; while MAE is more robust to outliers. This is because taking square of a small item (&lt; 1) is even smaller, square of a large item is even larger. Others What are the drawbacks of linear model? Are you familiar with alternatives (Lasso, ridge regression, boosted trees)? Drawbacks: Rigid assumptions; overfitting problem; linearity; see previous regularization part; boosted trees turn a series of weak prediction models into a strong prediction by voting. Assume you need to generate a predictive model using multiple regression. Explain how you intend to validate this modelCross validation; R^2 or MSE; Residual analysis (see statistical part) Do we always need the intercept term in a regression model? Yes. Intuitively speaking, without the constant, the regression line has to cross the origin, but it’s not always the case. Statistically speaking, it allows the mean of residuals to be 0. Provide three examples of relevant phenomena that have long tails. Why are they important in classification and regression problems? Natural language (Zipf’s law); customer purchase; wealth. The problem associated include it violates the normal distribution and linearity assumption for linear model and create accuracy paradox in classification problem. We will need to perform log transformation for long tailed data.","tags":"machine-learning data-science-interviews"},{"title":"Install Pentaho with GUI on EC2 Linux Instance","url":"/2017/03/09/Pentaho/","text":"In this post I will introduce how to install and visit the GUI interface from VNC client. I have been sitting here for 4 hours and were so frustrated for all the BUGS I saw.. Hopefully this post will help you get this done within one hour. Now let’s get started!! Security GroupWhen you run an instance, please edit the inbound traffic rules of security group:Protocol: Customed TCP/IP, Port 5901. This step is critical for setting up VNC server which we will introduce later. See ref. Install JavaPentaho runs on Java. Installation instruction goes here.Please, please check you’ve installed the SAME version of JDK and JRE! If you get error message “Unsupported major.minor version 52.0” then it’s very likely the version doesn’t match. At least it was the problem for me. Well it’s also possible that there is something wrong with yout /etc/environment : either you didn’t set up JAVA_HOME and Path; or you forgot to run source /etc/environment to confirm your setup. Download the binFile is available at here . If you can’t download with wget; try transfer downloads to your instance using filezilla. It took only half an hour to download and transfer to instance.After downloading (&amp; transfering), runchmod a+x pentaho-business-analytics-7.0.0-x64.bin./pentaho-business-analytics-7.0.0-x64.bin Install VNC server on server sideSteps as in Reference:12345678910sudo useradd -m pentahosudo passwd pentahosudo usermod -aG admin pentahosudo apt-get updatesudo apt-get install ubuntu-desktopsudo apt-get install vnc4serversu - pentahovncservervncserver -kill :1vim /home/pentaho/.vnc/xstartup and then change the document xstartup tp:12345678910111213141516#!/bin/shexport XKL_XMODMAP_DISABLE=1unset SESSION_MANAGERunset DBUS_SESSION_BUS_ADDRESS[ -x /etc/vnc/xstartup ] &amp;&amp; exec /etc/vnc/xstartup[ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresourcesxsetroot -solid greyvncconfig -iconic &amp;gnome-panel &amp;gnome-settings-daemon &amp;metacity &amp;nautilus &amp;gnome-terminal &amp; start VNC server again. Very Important: user name to start VNC server should be the same as username to run ./spoon.sh. Else will encounter org.eclipse.swt.SWTError. Download VNC clientI found RealVNC simple and good to use. After installation, open:${your public DNS}:1 Open PentahoIn VNC client, open terminal.Navigate usingcd /computer/home/pentaho/Pentaho/design-tools/data-integration/Final step: run./spoon.sh DONE!!!!","tags":"data-warehousing linux"},{"title":"Yoga! Yoga!","url":"/2017/03/09/Yoga/","text":"这学期报了一个Yoga班。坦白来说，不过是为了学费回本并应付罗尼先生的“要运动要出汗”的要求，但每周有三天时间能荒废一小时在无用却美好的事情上，却觉得很值。虽然大腿战战，时常力竭，但身体似乎有好转！不知道是因为难得的脑子不动肢体却动的时光；还是真的如我猜测：瑜伽是一门作用于筋与脊柱的运动。而肝主筋，肾主骨，从中医的角度，瑜伽恰恰是对身体很好的一门锻炼。 文下记载课上常用Routine。有些衔接动作搜索不到相应图片，那就略过。因博主脑容量小，常常记忆紊乱，此帖长期更新。","tags":"yoga"},{"title":"生活即诗","url":"/2017/03/09/生活即诗/","text":"突然想写一些文字，献给中华诗词大会。我突然很想问自己读古诗的时候在读什么。我喜欢流淌的文字，以及文字里流淌的意境。那意境，也许是月下柳梢头，也许是大江东去浪淘尽，我喜欢的正是诗人的眼睛，看到的世界很美。古人的生活也许很无聊。没有电视，没有手机，资讯只能是朝堂上的刀光剑影，或是街头巷尾的闲话长短。也许还不得志。也许是受尽了万般委屈，于是歌以咏志。没有动车，没有飞机。咫日可达的地方在古代也许就是半个月的颠簸。所以才有了少小离家老大回的无奈感伤，才有了劝君更尽一杯酒的离愁别绪。所以我佩服的诗人，往往把生活过程了一首诗。竹杖芒鞋，举杯邀月，哪怕是落寞都显得很美。即便是繁重的日子，上马击狂胡，下马草军书，也铿锵有力地汇成诗的韵律。我恨的是空虚。孤单了只有孤独寂寞冷，敬佩了只有666和厉害了。近不内省自身，远不仰俯天地。在我张口结舌地看着他人对答如流的时候，在我再也想不起来一个贴切的比喻描绘细腻的情感时候，我在问自己，我的心中是否还有诗，我的生活是否还是一首诗。","tags":""},{"title":"二月 众生相","url":"/2017/02/09/二月-众生相/","text":"15/02：昨天从Jenny家回来的时候，搭上一个印度姐姐的Uber。提起有一个儿子在匹大读书，提起她从来都是去寺庙但有朝一日想进教堂看看。我问她你要去教堂？她说是啊，I like God, I like every God. 16/02：朋友圈又见留学生自杀，无意间瞄到抑郁自测。很意外地每一条都中了，证据就是现今只能用消瘦而非苗条形容的身材，还有一些口不能言的后遗症。但我很感恩，上帝给我的恩典是够用的，在那段时间陪我同行，如今祂也与我同在。证据就是，在繁忙又无望的找工作季，我还觉得感恩，从心底想微笑。ps 今天走在路上雪花飘进了鼻孔里 :( :p 17/02:今天思考的主题叫“而我何去何从”。起床的时候刷Quora，刷到一题如何在谷歌面试中保持镇静。给出答案的基本都是谷歌engineer，有说，就把面试官当成日常同事正在向你寻求帮助，有说，有绝对实力碾压的时候你自然镇定。而我倾向于后者，这也终于解释了为何我这段时间忐忐忑忑以至于英语都说不好。在上学期需要努力的时候，我选择了退后，于是我没有统计基础；在我需要刷题的时候，我选择借口，于是我没有代码基础；仅仅凭着一点machine learning的知识和少得可怜的实践经验（我甚至没把握能tune CNN！），我拿什么去自信呢？是啊，这学期身体似乎好转了，估计也能如预期一般顺利毕业了，只是我观自己，不过是随便应付的一年。可我怎么办？主啊，给我智慧，给我一条路。乱入：今天在过马路的时候旁边的人一口飞痰划过身侧，吓死个人，原来美国人也不都是有素质的啊！不过是sample bias带来的“它乡月亮格外圆”，想想国外人民在陆家嘴南京西也不见得能见到随地吐痰的人啊。下午的时候仍然在想找工作的事情。诚然，一方面能做的就是珍惜当下——过去追悔莫及已经无用了，现在刷题，趁着作业好好打好基本功这是其一；二，我内心的恐惧来自于这个行业的日新月异，害怕平台不对就彻底被时代抛弃；而我既没有做research的能力，也感觉够不着大公司的门槛。突然有一个想法，做一个literature review/ knowledge share的网站：就像一棵树，所有的论文都是基于一些论文的改进及延伸。感觉会是毕业之后能持久做下去的事情呢，一定要找对合伙人! 19/02:昨天是没有故事的一天。大概是困在家里，也没有和人沟通，也没有和自己沟通，只是机械地跑程序、调程序、记录事故。但还是有点有趣的——这是我这学期以来过得最像周末的一天：逛泳衣，买春夏穿的衣服，胡思乱想。很喜欢Madewell的日常装和Sandro Paris的仙女裙。也不知道过几年点击这些链接是不是死的、我又会怎样想，但当下我刷了一通痛快的卡。突然又有一个想法，现在可以租房（airbnb）租车（uber），是不是也可以租一个网红回来帮我选衣衫。大抵是人以群分，一个nerd身边哪里有好的眼光（当然我很幸运，纵使被吐槽得要死也承蒙萱宝宝不弃）。而发现一个自卑阴郁的小蘑菇身上的闪光点，让她因为自己的美开怀大笑，也是一件很好的事吧。当然，话又说回来了，一个nerd真的需要搭配得好的衣衫吗？XDps 有人可以迷恋、想他想到半夜在床上打滚失眠是再幸福不过的事了呢 XD 24/02:补一则：前几天在路上看到一个中国女孩子正激动而快速地讲述着自己遇到的事情，而旁边，有一个明显外国人正在不停地点头。顺便说一句，用的是中文。这已经是我认识的第二个会说中文的外国人了，也许哪一天SHE唱的那句“全世界都在说中国话”会成真吧。 26/02:今天从教堂回家的路上，正谈着找工作形势艰难，收到了一条这样短信：Good morning! Ryan (my fiance) menthioned that after talking with you last week about your job search, that he would be happy to pass on your resume to his compnay. I know Pittsburgh isn’t your first choice but I wanted to let you know if you’re interested he works for a great company. The data sicentist that worked for them just left and they hire people from all over the world so he would be happy to send your resume to the chief technology officer.记录这个被善意淹没的时刻。","tags":""},{"title":"categories","url":"/categories/index.html","text":"","tags":""},{"title":"tags","url":"/tags/index.html","text":"","tags":""}]}