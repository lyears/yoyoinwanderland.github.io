{"pages":[{"title":"Algorithms - 1. Array","url":"/Algorithms-Array/","text":"LoopingEasy1 Two SumGiven an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. 12345678910111213def twoSum(self, nums, target): &quot;&quot;&quot; :type nums: List[int] :type target: int :rtype: List[int] &quot;&quot;&quot; sols = &#123;&#125; for i in xrange(len(nums)): if nums[i] in sols: return i, sols[nums[i]] else: sols[target - nums[i]] = i return None ####243 Shortest Word Distance Given a list of words and two words word1 and word2, return the shortest distance between these two words in the list. For example,Assume that words = [&quot;practice&quot;, &quot;makes&quot;, &quot;perfect&quot;, &quot;coding&quot;, &quot;makes&quot;]. Given word1 = “coding”, word2 = “practice”, return 3.Given word1 = &quot;makes&quot;, word2 = &quot;coding&quot;, return 1. Note:You may assume that word1 does not equal to word2, and word1 and word2 are both in the list. 12345678910111213141516def shortestDistance(self, words, word1, word2): &quot;&quot;&quot; :type words: List[str] :type word1: str :type word2: str :rtype: int &quot;&quot;&quot; index1 = index2 = dist = len(words) for i, w in enumerate(words): if w == word1: index1 = i dist = min(dist, abs(index2 - index1)) elif w == word2: index2 = i dist = min(dist, abs(index2 - index1)) return dist 66. Plus OneGiven a non-negative integer represented as a non-empty array of digits, plus one to the integer. You may assume the integer do not contain any leading zero, except the number 0 itself. The digits are stored such that the most significant digit is at the head of the list. 123456789101112131415def plusOne(self, digits): &quot;&quot;&quot; :type digits: List[int] :rtype: List[int] &quot;&quot;&quot; if len(digits) == 0: return [1] d = digits[-1] + 1 if d &lt; 10: digits[-1] = d return digits else: return self.plusOne(digits[:-1]) + [0] 12345678910def plusOne(self, digits): &quot;&quot;&quot; :type digits: List[int] :rtype: List[int] &quot;&quot;&quot; if len(digits) == 0: return [1] else: digits = reduce(lambda x,y : 10*x + y , digits) + 1 return map(int, str(digits)) 605 Can Place FlowersSuppose you have a long flowerbed in which some of the plots are planted and some are not. However, flowers cannot be planted in adjacent plots - they would compete for water and both would die. Given a flowerbed (represented as an array containing 0 and 1, where 0 means empty and 1 means not empty), and a number n, return if n new flowers can be planted in it without violating the no-adjacent-flowers rule. Example 1: 12Input: flowerbed = [1,0,0,0,1], n = 1Output: True Example 2: 12Input: flowerbed = [1,0,0,0,1], n = 2Output: False Note: The input array won’t violate no-adjacent-flowers rule. The input array size is in the range of [1, 20000]. n is a non-negative integer which won’t exceed the input array size. 123456789101112131415def canPlaceFlowers(self, flowerbed, n): &quot;&quot;&quot; :type flowerbed: List[int] :type n: int :rtype: bool &quot;&quot;&quot; for i, x in enumerate(flowerbed): if n &lt;= 0: return True if (not x) and (i == 0 or not flowerbed[i-1]) and (i == len(flowerbed) - 1 or not flowerbed[i+1]): n -= 1 flowerbed[i] = 1 return n &lt;= 0 Medium####162 Find Peak Element A peak element is an element that is greater than its neighbors. Given an input array where num[i] ≠ num[i+1], find a peak element and return its index. The array may contain multiple peaks, in that case return the index to any one of the peaks is fine. You may imagine that num[-1] = num[n] = -∞. For example, in array [1, 2, 3, 1], 3 is a peak element and your function should return the index number 2. def findPeakElement(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; for i in xrange(len(nums)): if (i == 0 or nums[i] &gt; nums[i-1]) and (i == len(nums) - 1 or nums[i] &gt; nums[i+1]): return i 73 Set Matrix ZeroesGiven a m x n matrix, if an element is 0, set its entire row and column to 0. Do it in place. def setZeroes(self, matrix): &quot;&quot;&quot; :type matrix: List[List[int]] :rtype: void Do not return anything, modify matrix in-place instead. &quot;&quot;&quot; if not matrix or not matrix[0]: return rows = len(matrix) cols = len(matrix[0]) row_zeros = [] col_zeros = [] for i in xrange(rows): for j in xrange(cols): if matrix[i][j] == 0: row_zeros.append(i) col_zeros.append(j) for i in row_zeros: matrix[i] = [0]*cols for i in xrange(rows): for j in col_zeros: matrix[i][j] = 0 ####238 Product of Array Except Self Given an array of n integers where n &gt; 1, nums, return an array output such that output[i] is equal to the product of all the elements of nums except nums[i]. Solve it without division and in O(n). For example, given [1,2,3,4], return [24,12,8,6]. Follow up:Could you solve it with constant space complexity? (Note: The output array does not count as extra space for the purpose of space complexity analysis.) def productExceptSelf(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[int] &quot;&quot;&quot; n = len(nums) p = 1 output = [] for num in nums: output.append(p) p *= num p = 1 for j in xrange(n-1, -1, -1): output[j] *= p p *= nums[j] return output 78 SubsetsGiven a set of distinct integers, nums, return all possible subsets (the power set). Note: The solution set must not contain duplicate subsets. For example,If nums = [1,2,3], a solution is: 12345678910[ [3], [1], [2], [1,2,3], [1,3], [2,3], [1,2], []] def subsets(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[List[int]] &quot;&quot;&quot; ans = [[]] for i in nums: ans = ans + [l + [i] for l in ans] return ans 280 Wiggle SortGiven an unsorted array nums, reorder it in-place such that nums[0] &lt;= nums[1] &gt;= nums[2] &lt;= nums[3].... For example, given nums = [3, 5, 2, 1, 6, 4], one possible answer is [1, 6, 2, 5, 3, 4]. def wiggleSort(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: void Do not return anything, modify nums in-place instead. &quot;&quot;&quot; for i in range(len(nums) - 1): nums[i:i+2] = sorted(nums[i:i+2], reverse = i%2) 48 Rotate ImageYou are given an n x n 2D matrix representing an image. Rotate the image by 90 degrees (clockwise). Note:You have to rotate the image in-place, which means you have to modify the input 2D matrix directly. DO NOT allocate another 2D matrix and do the rotation. Example 1: 12345678910111213Given input matrix = [ [1,2,3], [4,5,6], [7,8,9]],rotate the input matrix in-place such that it becomes:[ [7,4,1], [8,5,2], [9,6,3]] Example 2: 123456789101112131415Given input matrix =[ [ 5, 1, 9,11], [ 2, 4, 8,10], [13, 3, 6, 7], [15,14,12,16]], rotate the input matrix in-place such that it becomes:[ [15,13, 2, 5], [14, 3, 4, 1], [12, 6, 8, 9], [16, 7,10,11]] def rotate(self, matrix): &quot;&quot;&quot; :type matrix: List[List[int]] :rtype: void Do not return anything, modify matrix in-place instead. &quot;&quot;&quot; if matrix: matrix[:] = list(zip(*matrix[::-1])) 54 Spiral MatrixGiven a matrix of m x n elements (m rows, n columns), return all elements of the matrix in spiral order. For example,Given the following matrix: 123[[ 1, 2, 3 ], [ 4, 5, 6 ], [ 7, 8, 9 ]] You should return [1,2,3,6,9,8,7,4,5] def spiralOrder(self, matrix): &quot;&quot;&quot; :type matrix: List[List[int]] :rtype: List[int] &quot;&quot;&quot; if not matrix or not matrix[0]: return matrix return list(matrix.pop(0)) + self.spiralOrder(zip(*matrix)[::-1]) 15 Three SumGiven an array S of n integers, are there elements a, b, c in S such that a + b + c = 0? Find all unique triplets in the array which gives the sum of zero. Note: The solution set must not contain duplicate triplets. 1234567For example, given array S = [-1, 0, 1, 2, -1, -4],A solution set is:[ [-1, 0, 1], [-1, -1, 2]] def threeSum(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[List[int]] &quot;&quot;&quot; if len(nums) &lt; 3: return [] if len(nums) == 3: return [] if sum(nums) != 0 else [nums] nums = sorted(nums) solutions = [] for i in range(len(nums) - 1): if nums[i] &gt; 0 : return solutions if i &gt; 0 and nums[i] == nums[i-1]: continue sols = self.twoSum(nums[i+1:], -nums[i]) solutions.extend(sols) return solutions def twoSum(self, nums, target): sols = {} sub_dict = {} for num in nums: if num in sub_dict: sols[num] = target-num else: sub_dict[target - num] = 1 sols = [[-target, key, val] for key, val in sols.items()] return sols Two PointersEasy283 Move ZerosGiven an array nums, write a function to move all 0‘s to the end of it while maintaining the relative order of the non-zero elements. For example, given nums = [0, 1, 0, 3, 12], after calling your function, nums should be [1, 3, 12, 0, 0]. Note: You must do this in-place without making a copy of the array. Minimize the total number of operations. 123456def moveZeroes(self, nums): zero = 0 # records the position of &quot;0&quot; for i in xrange(len(nums)): if nums[i] != 0: nums[i], nums[zero] = nums[zero], nums[i] zero += 1 88 Merge Sorted ArrayGiven two sorted integer arrays nums1 and nums2, merge nums2 into nums1 as one sorted array. Note:You may assume that nums1 has enough space (size that is greater or equal to m + n) to hold additional elements from nums2. The number of elements initialized in nums1 and nums2 are m and n respectively. def merge(self, nums1, m, nums2, n): &quot;&quot;&quot; :type nums1: List[int] :type m: int :type nums2: List[int] :type n: int :rtype: void Do not return anything, modify nums1 in-place instead. &quot;&quot;&quot; while m &gt; 0 and n &gt; 0: if nums1[m-1] &gt; nums2[n-1]: nums1[m+n-1] = nums1[m-1] m -=1 else: nums1[m+n-1] = nums2[n-1] n -= 1 if n &gt; 0: nums1[:n] = nums2[:n] Medium75 Sort ColorsGiven an array with n objects colored red, white or blue, sort them so that objects of the same color are adjacent, with the colors in the order red, white and blue. Here, we will use the integers 0, 1, and 2 to represent the color red, white, and blue respectively. def sortColors(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: void Do not return anything, modify nums in-place instead. &quot;&quot;&quot; i = j = 0 for k in xrange(len(nums)): n = nums[k] nums[k] = 2 if n &lt; 2: nums[j] = 1 j += 1 if n == 0: nums[i] = 0 i += 1 Dynamic ProgrammingEasy53 Maximum SubarrayFind the contiguous subarray within an array (containing at least one number) which has the largest sum. For example, given the array [-2,1,-3,4,-1,2,1,-5,4],the contiguous subarray [4,-1,2,1] has the largest sum = 6. 1234567891011121314151617181920def maxSubArray(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int Example: [-2,1,-3,4,-1,2,1,-5,4] 1. if added_sum + current number &lt; current sum: abandon added_sum 2. keep track the largest sum in previous sublists &quot;&quot;&quot; if not nums: return 0 if len(nums) == 1: return nums[0] prev_max = current_max = nums[0] for n in nums[1:]: current_max = max(current_max + n, n) prev_max = max(current_max, prev_max) return prev_max ####121 Best Time to Buy and Sell Stock Say you have an array for which the ith element is the price of a given stock on day i. If you were only permitted to complete at most one transaction (ie, buy one and sell one share of the stock), design an algorithm to find the maximum profit. Example 1:Input: [7, 1, 5, 3, 6, 4]Output: 5 max. difference = 6-1 = 5 (not 7-1 = 6, as selling price needs to be larger than buying price) Example 2:Input: [7, 6, 4, 3, 1]Output: 0 In this case, no transaction is done, i.e. max profit = 0. def maxProfit(self, prices): &quot;&quot;&quot; :type prices: List[int] :rtype: int &quot;&quot;&quot; min_prices = float(&apos;inf&apos;) max_profit = 0 for i in prices: min_prices = min(i, min_prices) max_profit = max( i - min_prices, max_profit) return max_profit 76 Min Cost Climbing StairsOn a staircase, the i-th step has some non-negative cost cost[i] assigned (0 indexed). Once you pay the cost, you can either climb one or two steps. You need to find minimum cost to reach the top of the floor, and you can either start from the step with index 0, or the step with index 1. Example 1:Input: cost = [10, 15, 20]Output: 15Explanation: Cheapest is start on cost[1], pay that cost and go to the top. Example 2:Input: cost = [1, 100, 1, 1, 1, 100, 1, 1, 100, 1]Output: 6Explanation: Cheapest is start on cost[0], and only step on 1s, skipping cost[3]. Note: cost will have a length in the range [2, 1000]. Every cost[i] will be an integer in the range [0, 999]. def minCostClimbingStairs(self, cost): &quot;&quot;&quot; :type cost: List[int] :rtype: int &quot;&quot;&quot; # if no stairs / only one stair, no cost by climbing 2 steps if len(cost) &lt; 2: return 0 # by reaching the second to last, or the last of the array, we can finish the whole journey # we proceed like this: # step 1, step 2 # step 2, step 3 # step 3, step 4 # on each of stairs, we try to find the minimal cost to reach there # the final minimum cost is the min ( cost_of_two_steps_away, cost_of_one_step_away) two_steps_away_cost, one_step_away_cost = cost[0], cost[1] ## i is the step to reach for i in range(2, len(cost)): two_steps_away_cost, one_step_away_cost = one_step_away_cost, min(two_steps_away_cost, one_step_away_cost) + cost[i] return min(two_steps_away_cost, one_step_away_cost) Medium152 Maximum Product SubarrayFind the contiguous subarray within an array (containing at least one number) which has the largest product. For example, given the array [2,3,-2,4],the contiguous subarray [2,3] has the largest product = 6. def maxProduct(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; prev_min = prev_max = max_val = nums[0] for i in nums[1:]: prev_min, prev_max = min(prev_min*i, prev_max*i, i), max(prev_max*i, prev_min*i, i) max_val = max(max_val, prev_max) return max_val Binary SearchMedium34 Search for a RangeGiven an array of integers sorted in ascending order, find the starting and ending position of a given target value. Your algorithm’s runtime complexity must be in the order of O(log n). If the target is not found in the array, return [-1, -1]. For example,Given [5, 7, 7, 8, 8, 10] and target value 8,return [3, 4].1234567891011121314def searchRange(self, nums, target): &quot;&quot;&quot; :type nums: List[int] :type target: int :rtype: List[int] &quot;&quot;&quot; if not nums: return -1, -1 start = bisect.bisect_left(nums, target) if start &gt;= len(nums) or nums[start] != target : return -1, -1 end = bisect.bisect_right(nums, target, lo = start) return start, end - 1 123456789101112131415161718def searchRange(self, nums, target): if not nums: return [-1,-1] start = self.binarySearch(nums, target-0.5) if nums[start] != target: return [-1, -1] nums.append(float(&apos;inf&apos;)) end = self.binarySearch(nums, target+0.5)-1 return [start, end]def binarySearch(self, arr, target): start, end = 0, len(arr)-1 while start &lt; end: mid = (start+end)//2 if target &lt; arr[mid]: end = mid else: start = mid+1 return start 33 Search in Rotated Sorted ArraySuppose an array sorted in ascending order is rotated at some pivot unknown to you beforehand. (i.e., 0 1 2 4 5 6 7 might become 4 5 6 7 0 1 2). You are given a target value to search. If found in the array return its index, otherwise return -1. You may assume no duplicate exists in the array. def search(self, nums, target): &quot;&quot;&quot; :type nums: List[int] :type target: int :rtype: int &quot;&quot;&quot; if not nums: return -1 if len(nums) == 1: return int(nums[0] == target) - 1 left, right = 0, len(nums) - 1 while left &lt; right: mid = (left + right)/ 2 if nums[mid] == target: return mid elif nums[left] &lt;= nums[mid] &lt; target or target &lt; nums[left] &lt;= nums[mid] or nums[mid] &lt; target &lt; nums[left]: left = mid + 1 else: right = mid return left if target == nums[left] else -1 Mark Occurance Using ArrayEasy448 Find All Numbers Disappeared in an ArrayGiven an array of integers where 1 ≤ a[i] ≤ n (n = size of array), some elements appear twice and others appear once. Find all the elements of [1, n] inclusive that do not appear in this array. Could you do it without extra space and in O(n) runtime? You may assume the returned list does not count as extra space. Example: Input: [4,3,2,7,8,2,3,1]Output: [5,6] def findDisappearedNumbers(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[int] &quot;&quot;&quot; # return list(set(range(1, len(nums) + 1)) - set(nums)) for n in nums: ix = int(n) - 1 nums[ix] += 0.3 return [i + 1 for i, n in enumerate(nums) if n - int(n) == 0] DFSMedium79 Word SearchGiven a 2D board and a word, find if the word exists in the grid. The word can be constructed from letters of sequentially adjacent cell, where “adjacent” cells are those horizontally or vertically neighboring. The same letter cell may not be used more than once. For example, given board = 12345[ [&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;E&apos;], [&apos;S&apos;,&apos;F&apos;,&apos;C&apos;,&apos;S&apos;], [&apos;A&apos;,&apos;D&apos;,&apos;E&apos;,&apos;E&apos;]] word = “ABCCED”, -&gt; returns true word = “SEE”, -&gt; returns true word = “ABCB”, -&gt; returns false def exist(self, board, word): &quot;&quot;&quot; :type board: List[List[str]] :type word: str :rtype: bool &quot;&quot;&quot; if not board or not board[0] or not word: return False for i in xrange(len(board)): for j in xrange(len(board[i])): if self.dfs(board, word, i, j): return True return False def dfs(self, board, word, i, j): if not word: return True if board[i][j] != word[0]: return False if not word[1:]: return True board[i][j] = &quot;#&quot; if i-1 &gt;= 0 and self.dfs(board, word[1:], i-1, j): return True if j-1 &gt;= 0 and self.dfs(board, word[1:], i, j-1): return True if i+1 &lt; len(board) and self.dfs(board, word[1:], i+1, j): return True if j+1 &lt; len(board[i]) and self.dfs(board, word[1:], i, j+1): return True board[i][j] = word[0] return False","tags":"coding-interviews"},{"title":"快手午餐食堂","url":"/Lunch-Recipe/","text":"我不是一个吃货。我不会开车两小时去吃一道令人心动的菜，也从未抱怨过没有早茶和烤鱼的匹村是多么让人难以忍受。但，这并不代表着我能接受五分以下的菜肴好吗。。公司食堂偶尔供应的米饭居然和着醋蒸，月亮镇没有一家中餐馆能拿得出手，于是我选择了自救。生活中需要一点小确幸，de不出bug吃一顿好吃的午饭，跑不出模型再吃一顿好吃的午饭，又是元气满满可以大战三百年。 我也从不是一个大厨。偶尔宴请得赞一声好吃，已是道上的朋友给面子。今日厚颜发这一篇食谱，为了散落天涯的各路好友，以及念叨着不知作什么当午餐的万胖。 总体原则 一顿最多只做一道大菜 酒席中后上的大碗的菜，如全鸡、全鸭、肘子等。 —— 百度百科 所谓大菜，我简单粗暴地定义，从切、腌、炒等等需要人工参与的时间大于15分钟便是大菜。一道足矣。 前一天晚上做好食材准备 解冻。有一些需要特别腌制。一般而言只不过把一些东西从冰箱上层挪到下层，或者切两刀撒点酱油和胡椒，简单快捷，不超过十分钟。 要多买几个锅 如果您只有一口锅，甚至没有微波炉和高压锅，那您还是别看这篇文章了吧。 做好时间管理 我的午休60分钟，回家往返路程20分钟；电饭煲蒸米饭为15-20分钟。一般而言，我都能在蒸米饭的同时准备好所有的饭菜。举个例子。比如今日食谱是微波炉烤翅，以及炒生菜，我一般会按以下进行。 淘米、煮饭 （2mins) =&gt; 饭做好15分钟。 从冰箱拿出提前腌好的鸡翅 =&gt; 撒孜然粉、黑胡椒、lemon pepper (2mins) =&gt; 放入微波炉 =&gt; 8分钟熟。 剥蒜、剥生菜、洗生菜、炒生菜 (6mins) =&gt; 装盘，洗锅 (2mins) 鸡翅摆上桌、盛饭 =&gt; 这时候饭还剩下5分钟就能煮好；等待5分钟，菜微热的时候刚好就能吃了。 快手菜谱所有快手菜都是人工参与不需要10分钟；以及成菜时间不超过20分钟。是否需要腌制、是否需要微波炉我都特殊标出。 主食类 蛋炒饭 或将蛋改为冻蔬菜、半熟肉及火锅产品；加入随意酱料；饭下锅前与一勺生抽抓匀。 泡面 海鲜粉丝煲 快手荤菜 微波炉蒸蛋 【微波炉】 之前在我匹村茶屋吃过一道金针菇蒸蛋，非常惊艳。先捞金针菇，等微凉，放入蛋液。如此链接，用解冻模式10分钟，取出淋点麻油及酱油，高火模式1-2分钟。 微波炉烤翅【微波炉】【腌制】 小米椒爆鸡胗【腌制】 炸大虾【解冻】 这一道大概没有链接。有一日吃方便面时发现剩下了很多料包，就随意拿出一包和在虾仁上，再放点生粉，热油后下去炸。特别好吃。 蜜煎鸡胸【腌制】 蚝油牛柳 【解冻】【腌制】 榨菜肉丝汤【解冻】 培根蔬菜卷 【烤箱】 金针菇可替换为芦笋或者大虾。都很好吃。想念萱头。 电饭煲盐焗鸡 为什么这个菜谱步骤这么多……事情能不能简单点…… 厨房纸把鸡包一下再深入擦一下，抹匀盐和黑胡椒（或者直接黑胡椒盐嘛），不用腌制不用放油，直接放入电饭煲煮饭模式。第一次跳起加小半碗酒，第二次跳起就能吃，大概刚好20分钟成菜。 酸菜鱼汤 又是一道自创。回家第一件事儿切鱼片，放白胡椒、盐、料酒，剁椒腌制，过十几分钟（别的菜都做好的时候），炒姜、酸菜、鱼片（我喜欢用catfish，炒了以后挺好吃），最后加小半碗水，放点醋或者酸菜水儿，熟了就能吃了。需要的话随便哪一步放点辣椒。炒菜过程大概不超过5分钟。 快手素菜 麻婆豆腐【解冻】 这一道麻婆豆腐按工来说绝对不止10分钟。但有一个简化版：热油炒肉末、加豆瓣酱翻炒、加水和酱油、滚后加豆腐、大火烧到没什么汤时加半瓶盖醋、再滴几滴麻油、放点买的葱花。这样一来只需要最开始和最后看一下锅就好。 凉拌菠菜 我倒是有一个改良版，捞完菠菜放点青花椒油（或者麻油）、蚝油即可。省了不少工但也很好吃。 拌黄瓜 黄瓜竖着切四开，旁边挤一坨蚝油。真正的快手菜了。 蒸洋茄 好像又是自创菜谱。不用切开，直接蒸，淋点酱油和辣椒小青圈。 咸蛋拌茄子 首先应该是中国茄子，瘦长的那种；国外超市胖茄子我从未做成功过。茄子冲个水，入微波炉，10分钟。开火，放少量油，炒咸蛋黄，等其融化后，撕碎茄子，捏爆咸蛋白，拌一拌，可以直接出锅，也可加点蚝油。 醋溜白菜 买一罐蒜蓉，要么切蒜都能切出十分钟 :( 番茄蛋花汤 炒番茄、加半勺番茄酱、撒黑胡椒我认为一个都不可少。另外可以将蛋替换成任何火锅食材。鱼豆腐、牛肉片，我都觉得挺好吃的。 西瓜汁 有辣菜的时候，买一盒西瓜方儿，榨汁，不超过一分钟，但是真的好开心啊。 电高压锅类其实有了高压锅都不用管任何快手荤菜了。卤鸡爪、卤猪蹄、粉蒸排骨、卤牛筋、乌鸡汤等等，两周换着吃都不会重样。反正早上或者前天晚上花点时间放点东西就好。好吃又实惠。我都不想列菜谱了，累，反正都挺好吃的。","tags":""},{"title":"Improving Deep Neural Networks","url":"/Deep-Learning-4/","text":"Tuning Tips Importance of hyper-parameters: Most Important Learning Rate 2nd Momentum, Mini-batch size 3rd Hidden Units, Number of Layers, Learning rate decay Randomly choose hyper-parameter on a log scale. Don’t do grid search! 12r = -4 * np.random.rand() # r [-4, 0]theta = 10^r Mini-batch Small training set (m &lt;= 2000): Use batch gradient descent Large training set (m &gt; 2000): typical mini-batch size: 2^n, for example, 64, 128, 256, 512 Tensorflow Example 1234567891011121314151617181920212223242526## Cheatsheet from above linkdef (image_path_list, label_list): imagepaths = tf.convert_to_tensor(image_path_list, dtype=tf.string) labels = tf.convert_to_tensor(label_list, dtype=tf.int32) # Build a TF Queue, shuffle data # slice tensors into single instances and queue them up using threads. image, label = tf.train.slice_input_producer([imagepaths, labels], shuffle=True) # Read images from disk image = tf.read_file(image) image = tf.image.decode_jpeg(image, channels=CHANNELS) # Resize images to a common size image = tf.image.resize_images(image, [IMG_HEIGHT, IMG_WIDTH]) # Normalize image = image * 1.0/127.5 - 1.0 # Create batches X, Y = tf.train.batch([image, label], batch_size=batch_size, capacity=batch_size * 8, num_threads=4) return X, Y 1234567891011with tf.Session() as sess: # Run the initializer sess.run(init) # Start the data queue tf.train.start_queue_runners() # Training cycle for step in range(1, num_steps+1): sess.run(train_op) Keras Example 123model.fit(x_train, y_train, epochs=2000, batch_size=128) Adam OptimizerAdam optimizer combines the advantages of both Momentum and RMSprop thus enabling quick convergence. Parameters: Learning rate: needs to be tuned Beta1: 0.9 (dw) Beta2: 0.999 (dw^2) Epsilon: 10^-8 Tensorflow Example 12345678910111213“”“default setting:__init__( learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name=&apos;Adam&apos;)”“”optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) Keras Example 1keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) Learning Rate Decaylearning rate changes with current epoch number. Some common algorithms are as follows: 123lr = 0.95^epoch_num*alphalr = k / np.sqrt(epoch_num)*alphalr = alpha/(1+kt) TensorFlow Example 123456789101112# Optimizer: set up a variable that&apos;s incremented once per batch and controls the learning rate decay.batch = tf.Variable(0, dtype=data_type())learning_rate = tf.train.exponential_decay( 0.01, # Base learning rate. batch * BATCH_SIZE, # Current index into the dataset. train_size, # Decay step. 0.95, # Decay rate. staircase=True)# Use simple momentum for the optimization.optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch) Keras Example 1keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.95) InitializationThe reason we want to refine initialization is to avoid vanishing and exploding gradient in deep networks. For example, let’s say g(z) = z, b = 0; then in a 15-layers NN we will have y = (w^15) * x; thus we will have either very large or very small gradient. One way to avoid is to take into account number of features that will feed into the current layer. Some recommended initializations include: np.sqrt(2/ (num_features_last_layer)) np.sqrt( 2/ (number_features_last_layer + number_features_this_layer)) Tensorflow Example 1W = tf.get_variable(&apos;W&apos;, shape=(512, 256), initializer=tf.contrib.layers.xavier_initializer()) 12345678910111213141516tf.contrib.layers.fully_connected( inputs, num_outputs, activation_fn=tf.nn.relu, normalizer_fn=None, normalizer_params=None, weights_initializer=initializers.xavier_initializer(), weights_regularizer=None, biases_initializer=tf.zeros_initializer(), biases_regularizer=None, reuse=None, variables_collections=None, outputs_collections=None, trainable=True, scope=None) Keras Example 123456789101112model.add(Dense(64, kernel_initializer=&apos;random_uniform&apos;, bias_initializer=&apos;zeros&apos;))&quot;&quot;&quot;Other choices include:keras.initializers.Zeros()keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None)lecun_uniform(seed=None)glorot_normal(seed=None) # Xavier normal initializer....&quot;&quot;&quot; Tensorflow Tutorials Tensorflow for Deep Learning Research Tensorflow Python API Tensorflow Examples by aymericdamien Tensorflow Tutorials by HvassLabs","tags":"cnn machine-learning"},{"title":"How to Structure DL Project","url":"/Deep-Learning-3/","text":"If you have time for only one course from Andrew NG, I will recommend this one - Structuring Machine Learning Projects. It doesn’t go into too much technical details, but it provides a very useful way to structure and debug deep learning problems. This note won’t follow the same logistics as the original course, instead I will just note down the key steps to organize and then debug deep learning projects. Note: all the knowledge and examples come from Andrew’s lesson, and the quoted part come as the original quiz questions. I won’t take any credit for anything. Guideline Set up dev/ test set &amp; metric Build initial system quickly Use Bias/Variance analysis to prioritize next step Design the SystemMetric of SuccessOne important thing is to use a single number evaluation metrics. Case One Classifier Precision Recall A 95% 90% B 98% 85% This brings confusion which classifier to choose. You better use F1 if you care about both metrics. Case Two Classifier Accuracy Running Time A 90% 80 ms B 92% 90 ms C 95% 1,500 ms We can make accuracy as optimizing metric and running time as satisficing metric. Structuring the Data Small data set: 60-20-20 > 100,000 examples: 98-1-1 Mismatched training and dev/test set:: dev &amp; test set should be images of interest The distribution of data you care about contains images from your car’s front-facing camera; which comes from a different distribution than the images you were able to find and download off the internet. How should you split thedataset into train/dev/test sets? Mixall the 100,000 images with the 900,000 images you found online. Shuffleeverything. Split the 1,000,000 images dataset into 600,000 for the trainingset, 200,000 for the dev set and 200,000 for the test set. (No. This won’t measure how well the system performs on the images of interest - which is the images from car’s front facing cameras in this case.) Chooset he training set to be the 900,000 images from the internet along with 80,000 images from your car’s front-facing camera. The 20,000 remaining images will besplit equally in dev and test sets. Network ArchitectureTransfer Learning Two types: small set - retrain the last layers Enough data - initialize the last layer randomly and retrain all the network When to use transfer learning: Task a &amp; b have same input (all images, voice etc.) Have a lot more data for task A than task B Low level features from A could be helpful for learning B Multi-Task LearningUsed in anonymous driving (pedestrians, signals, stop signs, other vehicles etc.), and multi-object detections. When to use Multi-task learning: Training on a set of tasks that could benefit from having shared lower-level features Usually: amount of data you have for each task is quite similar Can train a big enough neural network to do well on all the tasks End-to-End LearningUsed in machine translation, audio transcription. But some tasks, like facial recognition (find face - compare face), handbone age estimation (estimate average length of handbone - look up age mapping), are better off using traditional two-pass methods. Pros: Let the data speaks Less hand designingof components needed Cons: Need large amount of data (x,y) Excludes potentially useful hand-designed components Bias/Variance AnalysisBenchmark: BEST Human Level Performance (ask people to label images if you don’t know!) Case One Human Level Performance 1% Training Error 8% Dev Error 10% Prioritize to reduce bias Case Two Human Level Performance 7.5% Training Error 8% Dev Error 10% Prioritize to reduce variance Case Three In this case, we have mismatched training and dev/test set. Human Level Performance 7.5% Training Error on mixed user images and internet images 8% Dev Error on only user images 10% We will have no idea if it’s variance or it’s the mismatched training and dev/test set. Instead, we analyze: Human Level Performance 7.5% Training Error on mixed user images and internet images 8% Training-dev Error on mixed user images and internet images 9.6% Dev Error on only user images 10% ​ If the result as above, then we know our model has high variance that we want to minimize. Human Level Performance 7.5% Training Error on mixed user images and internet images 8% Training-dev Error on mixed user images and internet images 8.1% Dev Error on only user images 10% If the result is as above, then we know our model has a high data mismatch problem. Error AnalysisHow to Perform Error AnalysisPick 100/ 500 mislabeled images, and then create a table as follows: No. Big Cat Dog Blurry Incorrectly Labeled Comments 1 Y 2 Y 3 Y 4 Y 5 Y 6 Y And then decide what to prioritize first. Incorrectly Labeled ImagesDefinition: Incorrectly labeled images: the labeler (people) got it wrong (input data to classifier have wrong labeles) Mislabled images: classifier got it wrong. When to fix it: In training set DL algorithms are quite robust to random errors in the training set. But not robust tosystematic errors. So for the first case, we can safely leave it in the training set. In dev/ test set the purpose of dev set is to pick the better classifier. If, for example, the incorrectly labeled training is 0.6%; we have the following two classifiers; It makes a significant differences to the performance on the dev set and test set. And we need to fix it. Classifier A Classifier B 1.9% error rate 2.1% error rate Improve PerformanceAt the end of the day we need to know what to do next. So below is the action plans: High Avoidable Bias: Bigger model Train longer Use better optimization algo (Adds momentum or RMS prop, or use a better algorithm like Adam) change NN architecture, hyperparam search High Variance: Bigger training set Regularization L2 Drop out Data augmentation NN architecture/ hyperparam search Data Mismatch Problem For example, the training set come from both in-car audio and audio from internet. We can add car noises to the audio from Internet and creates artificially synthesized in-car audio. You decide to use data augmentation to addressfoggy images. You find 1,000 pictures of fog off the internet, and “add” the mto clean images to synthesize foggy days. So long as the synthesized fog looks realistic to the human eye, you can be confident that the synthesized data is accurately capturing the distribution ofreal foggy images, since human vision is very accurate for the problem you’re solving. Incorrectly Labeled Problem Apply the same process (for example, if you hire someone to re-label data for you) on both dev and test set, to make sure they come from the same distribution Consider examples your algorithm got right as well as ones got wrong. Train and dev/test sets may now come from slightly different distributions. Other Problem from Error Analysis Train a classifier just for big cat/ white dog problem if necessary. Improve performance on blurry images. I think I can do data augmentation - create more blurry images WITH labels.","tags":"cnn machine-learning"},{"title":"Image Processing Cheatsheet from PyImageSearch","url":"/Learn-OpenCV-1/","text":"This blog summarizes image processing methods from pyimagesearch. All the source codes and pictures come from the blog and I won’t take any credit for anything. Image ProcessingReferencesBlur detection with OpenCV OpenCV Gamma Correction Codes Blur Detection 1cv2.Laplacian(image, cv2.CV_64F).var() Gamma Correction 123456789def adjust_gamma(image, gamma=1.0): # build a lookup table mapping the pixel values [0, 255] to # their adjusted gamma values invGamma = 1.0 / gamma table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(&quot;uint8&quot;) # apply gamma correction using the lookup table return cv2.LUT(image, table) Object DetectionReferencesDetecting Circles in Images using OpenCV and Hough Circles Detecting Barcodes in Images with Python and OpenCV Target acquired: Finding targets in drone and quadcopter video streams using Python and OpenCV Recognizing digits with OpenCV and Python Detecting machine-readable zones in passport images Bubble sheet multiple choice scanner and test grader using OMR, Python and OpenCV Codes Detecting Circles 1234567891011121314# detect circles in the imagecircles = cv2.HoughCircles(gray, cv2.cv.CV_HOUGH_GRADIENT, 1.2, 100) # ensure at least some circles were foundif circles is not None: # convert the (x, y) coordinates and radius of the circles to integers circles = np.round(circles[0, :]).astype(&quot;int&quot;) # loop over the (x, y) coordinates and radius of the circles for (x, y, r) in circles: # draw the circle in the output image, then draw a rectangle # corresponding to the center of the circle cv2.circle(output, (x, y), r, (0, 255, 0), 4) cv2.rectangle(output, (x - 5, y - 5), (x + 5, y + 5), (0, 128, 255), -1) Detect squares in a video 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# load the videocamera = cv2.VideoCapture(args[&quot;video&quot;]) # keep loopingwhile True: # grab the current frame and initialize the status text (grabbed, frame) = camera.read() status = &quot;No Targets&quot; # check to see if we have reached the end of the # video if not grabbed: break # convert the frame to grayscale, blur it, and detect edges gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) blurred = cv2.GaussianBlur(gray, (7, 7), 0) edged = cv2.Canny(blurred, 50, 150) # find contours in the edge map (cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) for c in cnts: # approximate the contour peri = cv2.arcLength(c, True) approx = cv2.approxPolyDP(c, 0.01 * peri, True) # ensure that the approximated contour is &quot;roughly&quot; rectangular if len(approx) &gt;= 4 and len(approx) &lt;= 6: # compute the bounding box of the approximated contour and # use the bounding box to compute the aspect ratio (x, y, w, h) = cv2.boundingRect(approx) aspectRatio = w / float(h) # compute the solidity of the original contour area = cv2.contourArea(c) hullArea = cv2.contourArea(cv2.convexHull(c)) solidity = area / float(hullArea) # compute whether or not the width and height, solidity, and # aspect ratio of the contour falls within appropriate bounds keepDims = w &gt; 25 and h &gt; 25 keepSolidity = solidity &gt; 0.9 keepAspectRatio = aspectRatio &gt;= 0.8 and aspectRatio &lt;= 1.2 # ensure that the contour passes all our tests if keepDims and keepSolidity and keepAspectRatio: # draw an outline around the target and update the status # text cv2.drawContours(frame, [approx], -1, (0, 0, 255), 4) status = &quot;Target(s) Acquired&quot; # draw the status text on the frame cv2.putText(frame, status, (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2) # show the frame and record if a key is pressed cv2.imshow(&quot;Frame&quot;, frame) key = cv2.waitKey(1) &amp; 0xFF # if the &apos;q&apos; key is pressed, stop the loop if key == ord(&quot;q&quot;): break# cleanup the camera and close any open windowscamera.release()cv2.destroyAllWindows() Detectin Texture (Barcode in this case) 12345678910111213141516171819202122232425262728293031323334353637383940# compute the Scharr gradient magnitude representation of the images# in both the x and y directiongradX = cv2.Sobel(gray, ddepth = cv2.cv.CV_32F, dx = 1, dy = 0, ksize = -1)gradY = cv2.Sobel(gray, ddepth = cv2.cv.CV_32F, dx = 0, dy = 1, ksize = -1) # subtract the y-gradient from the x-gradient# to find regions that have high horizontal and low vertical gradients.gradient = cv2.subtract(gradX, gradY)gradient = cv2.convertScaleAbs(gradient)# blur and threshold the image# smooth out high frequency noise in the gradient blurred = cv2.blur(gradient, (9, 9))(_, thresh) = cv2.threshold(blurred, 225, 255, cv2.THRESH_BINARY)# construct a closing kernel and apply it to the thresholded image# this kernel has a width that is larger than the height# thus close the gaps between vertical stripes of the barcodekernel = cv2.getStructuringElement(cv2.MORPH_RECT, (21, 7))closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)# perform a series of erosions and dilations# erode the white pixels in the image, thus removing the small blobs# dilate the remaining white pixels and grow the white regions back out.closed = cv2.erode(closed, None, iterations = 4)closed = cv2.dilate(closed, None, iterations = 4)# find the contours in the thresholded image, then sort the contours# by their area, keeping only the largest one(cnts, _) = cv2.findContours(closed.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)c = sorted(cnts, key = cv2.contourArea, reverse = True)[0]# compute the rotated bounding box of the largest contourrect = cv2.minAreaRect(c)box = np.int0(cv2.cv.BoxPoints(rect))# draw a bounding box arounded the detected barcode and display the# imagecv2.drawContours(image, [box], -1, (0, 255, 0), 3) Detect Digits Areas 1234567891011121314151617181920212223242526# extract the thermostat display, apply a perspective transform to itthresh = cv2.threshold(warped, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (1, 5))thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)# find contours in the thresholded image, then initialize the# digit contours listscnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if imutils.is_cv2() else cnts[1]digitCnts = [] # loop over the digit area candidatesfor c in cnts: # compute the bounding box of the contour (x, y, w, h) = cv2.boundingRect(c) # if the contour is sufficiently large, it must be a digit if w &gt;= 15 and (h &gt;= 30 and h &lt;= 40): digitCnts.append(c) # sort the contours from left-to-right, then initialize the# actual digits themselvesdigitCnts = contours.sort_contours(digitCnts, method=&quot;left-to-right&quot;)[0] Detect Machine Readable Zones 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# initialize a rectangular and square structuring kernelrectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (13, 5))sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (21, 21))image = cv2.imread(imagePath)image = imutils.resize(image, height=600)gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)# smooth the image using a 3x3 Gaussian, then apply the blackhat# morphological operator to find dark regions on a light backgroundgray = cv2.GaussianBlur(gray, (3, 3), 0)blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, rectKernel)# compute the Scharr gradient of the blackhat image and scale the# result into the range [0, 255]# extremely helpful in reducing false-positive MRZ detectionsgradX = cv2.Sobel(blackhat, ddepth=cv2.CV_32F, dx=1, dy=0, ksize=-1)gradX = np.absolute(gradX)(minVal, maxVal) = (np.min(gradX), np.max(gradX))gradX = (255 * ((gradX - minVal) / (maxVal - minVal))).astype(&quot;uint8&quot;)# apply a closing operation using the rectangular kernel to close# gaps in between letters -- then apply Otsu&apos;s thresholding methodgradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, rectKernel)thresh = cv2.threshold(gradX, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]# perform another closing operation, this time using the square# kernel to close gaps between lines of the MRZ, then perform a# series of erosions to break apart connected componentsthresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel)thresh = cv2.erode(thresh, None, iterations=4)# during thresholding, it&apos;s possible that border pixels were# included in the thresholding, so let&apos;s set 5% of the left and# right borders to zerop = int(image.shape[1] * 0.05)thresh[:, 0:p] = 0thresh[:, image.shape[1] - p:] = 0# find contours in the thresholded image and sort them by their# sizecnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)[-2]cnts = sorted(cnts, key=cv2.contourArea, reverse=True)# loop over the contoursfor c in cnts: # compute the bounding box of the contour and use the contour to # compute the aspect ratio and coverage ratio of the bounding box # width to the width of the image (x, y, w, h) = cv2.boundingRect(c) ar = w / float(h) crWidth = w / float(gray.shape[1]) # check to see if the aspect ratio and coverage width are within # acceptable criteria if ar &gt; 5 and crWidth &gt; 0.75: # pad the bounding box since we applied erosions and now need # to re-grow it pX = int((x + w) * 0.03) pY = int((y + h) * 0.03) (x, y) = (x - pX, y - pY) (w, h) = (w + (pX * 2), h + (pY * 2)) # extract the ROI from the image and draw a bounding box # surrounding the MRZ roi = image[y:y + h, x:x + w].copy() cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2) break Object TransformationReferences4 Point OpenCV getPerspective Transform Example How to Build a Kick-Ass Mobile Document Scanner in Just 5 Minutes Text skew correction with OpenCV and Python Seam carving with OpenCV, Python, and scikit-image Codes Four Point Transformation 1234567891011121314151617181920212223242526272829### Find Four Points and Call Function# convert the image to grayscale, blur it, and find edges# in the imagegray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)gray = cv2.GaussianBlur(gray, (5, 5), 0)edged = cv2.Canny(gray, 75, 200)# find the contours in the edged image, keeping only the# largest ones, and initialize the screen contour(cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5] # loop over the contoursfor c in cnts: # approximate the contour peri = cv2.arcLength(c, True) approx = cv2.approxPolyDP(c, 0.02 * peri, True) # if our approximated contour has four points, then we # can assume that we have found our screen if len(approx) == 4: screenCnt = approx break # show the contour (outline) of the piece of paperprint &quot;STEP 2: Find contours of paper&quot;cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def order_points(pts): # initialzie a list of coordinates that will be ordered # such that the first entry in the list is the top-left, # the second entry is the top-right, the third is the # bottom-right, and the fourth is the bottom-left rect = np.zeros((4, 2), dtype = &quot;float32&quot;) # the top-left point will have the smallest sum, whereas # the bottom-right point will have the largest sum s = pts.sum(axis = 1) rect[0] = pts[np.argmin(s)] rect[2] = pts[np.argmax(s)] # now, compute the difference between the points, the # top-right point will have the smallest difference, # whereas the bottom-left will have the largest difference diff = np.diff(pts, axis = 1) rect[1] = pts[np.argmin(diff)] rect[3] = pts[np.argmax(diff)] # return the ordered coordinates return rectdef four_point_transform(image, pts): # obtain a consistent order of the points and unpack them # individually rect = order_points(pts) (tl, tr, br, bl) = rect # compute the width of the new image, which will be the # maximum distance between bottom-right and bottom-left # x-coordiates or the top-right and top-left x-coordinates widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2)) widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2)) maxWidth = max(int(widthA), int(widthB)) # compute the height of the new image, which will be the # maximum distance between the top-right and bottom-right # y-coordinates or the top-left and bottom-left y-coordinates heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2)) heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2)) maxHeight = max(int(heightA), int(heightB)) # now that we have the dimensions of the new image, construct # the set of destination points to obtain a &quot;birds eye view&quot;, # (i.e. top-down view) of the image, again specifying points # in the top-left, top-right, bottom-right, and bottom-left # order dst = np.array([ [0, 0], [maxWidth - 1, 0], [maxWidth - 1, maxHeight - 1], [0, maxHeight - 1]], dtype = &quot;float32&quot;) # compute the perspective transform matrix and then apply it M = cv2.getPerspectiveTransform(rect, dst) warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight)) # return the warped image return warped Text Skew Correction 123456789101112131415161718192021222324252627282930313233343536# convert the image to grayscale and flip the foreground# and background to ensure foreground is now &quot;white&quot; and# the background is &quot;black&quot;gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)gray = cv2.bitwise_not(gray) # threshold the image, setting all foreground pixels to# 255 and all background pixels to 0thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]# grab the (x, y) coordinates of all pixel values that# are greater than zero, then use these coordinates to# compute a rotated bounding box that contains all# coordinatescoords = np.column_stack(np.where(thresh &gt; 0))angle = cv2.minAreaRect(coords)[-1] # the `cv2.minAreaRect` function returns values in the# range [-90, 0); as the rectangle rotates clockwise the# returned angle trends to 0 -- in this special case we# need to add 90 degrees to the angleif angle &lt; -45: angle = -(90 + angle) # otherwise, just take the inverse of the angle to make# it positiveelse: angle = -angle# rotate the image to deskew it(h, w) = image.shape[:2]center = (w // 2, h // 2)M = cv2.getRotationMatrix2D(center, angle, 1.0)rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE) Template MatchingReferencesMulti-scale Template Matching using Python and OpenCV Image Difference with OpenCV and Python Codes Robust Template Matching 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&quot;&quot;&quot;1. Loop over the input image at multiple scales (i.e. make the input image progressively smaller and smaller).2. Apply template matching using cv2.matchTemplate and keep track of the match with the largest correlation coefficient (along with the x, y-coordinates of the region with the largest correlation coefficient).3. After looping over all scales, take the region with the largest correlation coefficient and use that as your “matched” region.While we can handle variations in translation and scaling, our approach will not be robust to changes in rotation or non-affine transformations.If we are concerned about rotation on non-affine transformations we are better off taking the time to detect keypoints, extract local invariant descriptors, and apply keypoint matching.&quot;&quot;&quot;template = cv2.imread(args[&quot;template&quot;])template = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)template = cv2.Canny(template, 50, 200)(tH, tW) = template.shape[:2]image = cv2.imread(imagePath) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) found = None # loop over the scales of the image for scale in np.linspace(0.2, 1.0, 20)[::-1]: # resize the image according to the scale, and keep track # of the ratio of the resizing resized = imutils.resize(gray, width = int(gray.shape[1] * scale)) r = gray.shape[1] / float(resized.shape[1]) # if the resized image is smaller than the template, then break # from the loop if resized.shape[0] &lt; tH or resized.shape[1] &lt; tW: break # detect edges in the resized, grayscale image and apply template # matching to find the template in the image edged = cv2.Canny(resized, 50, 200) result = cv2.matchTemplate(edged, template, cv2.TM_CCOEFF) (_, maxVal, _, maxLoc) = cv2.minMaxLoc(result) # check to see if the iteration should be visualized if args.get(&quot;visualize&quot;, False): # draw a bounding box around the detected region clone = np.dstack([edged, edged, edged]) cv2.rectangle(clone, (maxLoc[0], maxLoc[1]), (maxLoc[0] + tW, maxLoc[1] + tH), (0, 0, 255), 2) cv2.imshow(&quot;Visualize&quot;, clone) cv2.waitKey(0) # if we have found a new maximum correlation value, then ipdate # the bookkeeping variable if found is None or maxVal &gt; found[0]: found = (maxVal, maxLoc, r) # unpack the bookkeeping varaible and compute the (x, y) coordinates # of the bounding box based on the resized ratio (_, maxLoc, r) = found (startX, startY) = (int(maxLoc[0] * r), int(maxLoc[1] * r)) (endX, endY) = (int((maxLoc[0] + tW) * r), int((maxLoc[1] + tH) * r)) # draw a bounding box around the detected result and display the image cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2) cv2.imshow(&quot;Image&quot;, image) Image Difference 12345678910111213141516171819202122# compute the Structural Similarity Index (SSIM) between the two# images, ensuring that the difference image is returned(score, diff) = compare_ssim(grayA, grayB, full=True)diff = (diff * 255).astype(&quot;uint8&quot;)print(&quot;SSIM: &#123;&#125;&quot;.format(score))# threshold the difference image, followed by finding contours to# obtain the regions of the two input images that differthresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if imutils.is_cv2() else cnts[1]# loop over the contoursfor c in cnts: # compute the bounding box of the contour and then draw the # bounding box on both input images to represent where the two # images differ (x, y, w, h) = cv2.boundingRect(c) cv2.rectangle(imageA, (x, y), (x + w, y + h), (0, 0, 255), 2) cv2.rectangle(imageB, (x, y), (x + w, y + h), (0, 0, 255), 2) Color ManipulationReferencesFinding the Brightest Spot in an Image using Python and OpenCV OpenCV and Python K-Means Color Clustering Color Quantization with OpenCV using K-Means Clustering Codes Brightest color: 12345gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)gray = cv2.GaussianBlur(gray, (args[&quot;radius&quot;], args[&quot;radius&quot;]), 0)(minVal, maxVal, minLoc, maxLoc) = cv2.minMaxLoc(gray)image = orig.copy()cv2.circle(image, maxLoc, args[&quot;radius&quot;], (255, 0, 0), 2) Color Quantization: Color quantization limits the number of colors remained in one picture. For example if there is sky blue and dark blue, they might be combined into some color in the middle of their RGB value. It removes redundant color information thus saves storage spaces. It’s useful in image search problems. 123456789101112131415161718192021222324# convert the image from the RGB color space to the L*a*b*# color space -- since we will be clustering using k-means# which is based on the euclidean distance, we&apos;ll use the# L*a*b* color space where the euclidean distance implies# perceptual meaningimage = cv2.cvtColor(image, cv2.COLOR_BGR2LAB) # reshape the image into a feature vector so that k-means# can be appliedimage = image.reshape((image.shape[0] * image.shape[1], 3)) # apply k-means using the specified number of clusters and# then create the quantized image based on the predictionsclt = MiniBatchKMeans(n_clusters = args[&quot;clusters&quot;])labels = clt.fit_predict(image)quant = clt.cluster_centers_.astype(&quot;uint8&quot;)[labels] # reshape the feature vectors to imagesquant = quant.reshape((h, w, 3))image = image.reshape((h, w, 3)) # convert from L*a*b* to RGBquant = cv2.cvtColor(quant, cv2.COLOR_LAB2BGR)image = cv2.cvtColor(image, cv2.COLOR_LAB2BGR)","tags":""},{"title":"Learn CNN from Giants","url":"/Deep-Learning-2/","text":"Let’s start with the best tutorials for deep learning and CNNs. Genreal Tutorials: An Intuitive Explanation of Convolutional Neural Networks by Ujjwal Karn Unsupervised Feature Learning &amp; Deep Learning Tutorial by Andrew NG CS231n Convolutional Neural Network for Visual Recognition by Feifei Li Deep Learning Tutorial by Theano Development Team Classic Papers: AlexNet by Alex Krizhevsky, Ilya Sutskever&amp; Geoffrey Hinton from University of Toronto VGG: Very Deep Convolutional Neural Networks for Large-Scale Image Recognition by Visual Geometry Group, University of Oxford GoogLeNet: Going Deeper with Convolutions by Google Inc ResNet: Deep Residual Learning for Image Recognition by Microsoft AlexNetAdit has a good summary of its importance: The one that started it all. 2012 marked the first year where a CNN was used to achieve a top 5 test error rate of 15.4% (Top 5 error is the rate at which, given an image, the model does not output the correct label with its top 5 predictions). The next best entry achieved an error of 26.2%, which was an astounding improvement that pretty much shocked the computer vision community. Safe to say, CNNs became household names in the competition from then on out. Statistics Year: 2012 Data: 1.2 million images from ImageNet LSVRC-2010 Top 1 Error Top 5 Error AlexNet 37.5% 17.0% Architecture ​ Image Source Layers Remarks 1 Convolutional: 96 kernels of 11*11*3, stride 4 response normalized &amp; pooled 2 Convolutional: 256 kernels of 5*5*48 response normalized &amp; pooled 3 Convolutional: 384 kernels of 3*3*256 4 Convolutional: 384 kernels of 3*3*192 5 Convolutional: 256 kernels of 3*3*192 6 Fully connected: 4096 neurons 7 Fully connected: 4096 neurons 8 Fully connected: 4096 neurons Output layers: 1000, softmax Note that, later in the more successful version ZF Net, the size of kernel is modified from 11*11*3 to 7*7*3 to capture more details in the image. training schema We use stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005. We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and ﬁfth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates the early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron biases in the remaining layers with the constant 0. Main Take-aways Save Time Increase Accuracy Reduce Overfitting ReLU 6 times faster than Tahn/ Sigmoid activations Multiple GPUs ☑️ Drop Out layer 1/2 of time required to converge ☑️ Local Response Normalization Top1: 1.4%; Top5: 1.2% Overlapping Pooling Top1: 0.4%; Top5: 0.3% Data Augmentation ☑️ Data Augmentation Methodologies Extracting random 224 × 224 patches (and their horizontal reﬂections) from the 256×256 images and training our network on these extracted patches 4. Altering the intensities of the RGB channels in training images…. We perform PCA on the set of RGB pixel values, To each training image, we add multiples of the found principal components, with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from a Gaussian with mean zero and standard deviation 0.1. VGGVGG is the first paper that discusses about the depth of CNN architecture. It extends the number of layers to 19 and uses very small (3*3) convolutional filters. It also states that VGG model could be used as a part in other machine learning pipeline as deep features. Statistics Year: 2014 Data: 1.3 million images from ImageNet ILSVRC-2012 Top 1 Error Top 5 Error AlexNet 37.5% 17.0% VGG 23.7% 7.3% Architecture ​ Image Source Note that, All hidden layers are equipped with the rectiﬁcation (ReLU (Krizhevsky et al., 2012)) non-linearity. the ReLU activation function is not shown for brevity. Changes from AlexNet AlexNet VGG Layers 5 Conv, 3 FC layers 16 Conv, 3 FC layers Convolutional Filters 11*11; stride = 4 a stack of conv layers with 3*3 kernels, stide =1 Padding None 1 Pooling Overlapping Max Pooling Non-overlapping Max Pooling Local Response Normalization Yes - it improves performance No - it doesn’t improve performance but increase memory &amp; computation time Training Schema AlexNet VGG Batch size 128 256 Momentum 0.9 0.9 Weight decay 0.0005 0.0005 Initialization N(0,0.01); bias = 1 N(0,0.01); bias = 0 Main Take-aways Deep CNN! Why would we use stack of multiple 3*3 Convolutional layers (without spatial pooling in between) instead of larger one layer kernel? Same receptive filed. Receptive filed is well explained by Novel Martis in this post. For example, in the below image, the input for B(2,2) is A(1:3, 1:3). The input for C(3,3) is B(2:4, 2:4) -&gt; A(1:5,1:5). The receptive field for 2 convolutional layers will be 5*5, and 3 convolutional layers will be 7*7. ​ Image Source More discriminative function. There are three ReLU layers instead of one. Less parameters. Suppose we have C channels. Stack of three 3*3 kernels will have 3(3^2) = 27 parameters, and one layer of 7\\7 kernel will have 7^2 = 49 parameters; which is 81% more. InceptionInception goes beyond the idea “we need to go deeper” but comes up with a “network-in-network” inception module. There are two drawbacks of the previous most popular deeper and wider neural network - that it might easily go overfitting especially when there’re no enough training examples, and that it takes up too much computational resources. The authors are motivated to build more efficient yet accurate (or more accurate) algorithms by replacing the fully connected layers with dense structures. Statistics Year: 2015 Data: 1.3 million images from ImageNet ILSVRC 2014 Top 1 Error Top 5 Error VGG 23.7% 7.3% Inception 6.7% Architecture ​ Image source Inception Module: So the green box above is an inception module, which could be presented as the picture below. The idea behind is that the authors try to find a dense structure that can best approximate the optimal local sparse structure. So the idea is well outlined in Adit’s blog: previously in stacked CNNs, different sizes of convolutional layers and max pooling layers are to be chosen; here we have them all. For example, if inside one picture, there is a person stands nearer the camera while there is a cat that is far away from the camera, it would be beneficial to have both of a larger image kernel to capture the nearer person and a smaller kernel to capture the cat. Therefore with the inception module, parameters are less, yet more powerful than simply stacked convolution. ​ Image source Main Take-Aways The idea of CNN doesn’t have to be stacked up sequentially. Get rid of fully connected layer (use average pooling instead) thus saves a lot of parameters and computational time. The massive usage of 1*1 convoluational kernel: Dimensional reduction The 1*1 kernel is reducing a great amount of image dimensions. For example, if there is 224*224*60 input that goes through a 1*1*10 image kernel, then the output size will just be 224*224*10. Less parameters, less chance of overfitting ResNetResNet aims to solve the problem of degradation. Basically, the authors of ReNet found that with increased number of layers, the accuracy get saturated thus degradation occurs. Previously the degragation was thought to be overfitting, but it isn’t: The training error increase rather than decrease. This is counterintuitive. The authors believe that, “the degradation problem (of training accuracy) suggests that the solvers might have difﬁculties in approximating identity mappings by multiple nonlinear layers.” Therefore he is motivated to create an easier way to optimize the deep CNNs. Statistics Year: 2015 Data: ILSVRC 2015 Top 5 Error Inception 6.7% ResNet 3.6% beats human recognition: 5%-10% Architecture ​ Image Source Residual Block Image Source It performs shortcut identity mapping. I will reference Adit’s wonderful explanation here: The idea behind a residual block is that you have your input x go through conv-relu-conv series. This will give you some F(x). That result is then added to the original input x. Let’s call that H(x) = F(x) + x. In traditional CNNs, your H(x) would just be equal to F(x) right? So, instead of just computing that transformation (straight from x to F(x)), we’re computing the term that you have to add, F(x), to your input, x. Basically, the mini module shown below is computing a “delta” or a slight change to the original input x to get a slightly altered representation (When we think of traditional CNNs, we go from x to F(x) which is a completely new representation that doesn’t keep any information about the original x). The authors believe that “it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping”. Another reason for why this residual block might be effective is that during the backward pass of backpropagation, the gradient will flow easily through the graph because we have addition operations, which distributes the gradient. Reference The 9 Deep Learning Papers You Need to Know About An Intuitive Explanation of Convolutional Neural Networks CS231n Convolutional Neural Network for Visual Recognition by Feifei Li AlexNet by Alex Krizhevsky, Ilya Sutskever&amp; Geoffrey Hinton from University of Toronto VGG: Very Deep Convolutional Neural Networks for Large-Scale Image Recognition by Visual Geometry Group, University of Oxford GoogLeNet: Going Deeper with Convolutions by Google Inc ResNet: Deep Residual Learning for Image Recognition by Microsoft","tags":"cnn machine-learning"},{"title":"Learn ML from Sklearn: Cross Validation","url":"/Sklearn-CV/","text":"Overfitting in Two Ways Learn parameters and test the model in the same dataset Solution: train-test 1X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.4, random_state=0) Tune the hyperparameters and test the model in the same dataset When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. Solution: Train-validation-test, Cross validation 1scores = cross_val_score(model, iris.data, iris.target, cv=5, scoring=&apos;f1_macro&apos;) Note: from my point of view, cross validation is not a very clean solution for hyperparameter tuning. A small test set is still needed to see the generalization error. But it is a good way to see if the model is stable. If the validation error varies amongst different left out samples, then there might be some problems. Visualize Overfitting &amp; UnderfittingEffect of a hyper-parameter 12345import numpy as npfrom sklearn.model_selection import validation_curvefrom sklearn.linear_model import Ridgetrain_scores, valid_scores = validation_curve(Ridge(), X, y, &quot;alpha&quot;, np.logspace(-7, 3, 3)) Effect of the number of training samples 12345from sklearn.model_selection import learning_curvefrom sklearn.svm import SVCtrain_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=&apos;linear&apos;), X, y, train_sizes=[50, 80, 110], cv=5) K FoldsK-Fold KFold divides all the samples in k groups of samples, called folds ( if k=n this is equivalent to the Leave One Out strategy), of equal sizes (if possible). The prediction function is learned using k - 1 folds, and the fold left out is used for test. 12345from sklearn.model_selection import KFoldkf = KFold(n_splits=10, random_state = 1)for train_index, test_index in kf.split(X): X_train, y_train = X[train_index], y[train_index] Stratified K-FoldUse stratified K-Fold when the class is unbalanced. StratifiedKFold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set. 12345from sklearn.model_selection import StratifiedKFoldskf = StratifiedKFold(n_splits=10, random_state = 1)for train, test in skf.split(X, y): X_train, y_train = X[train_index], y[train_index] Group K-Fold An example would be when there is medical data collected from multiple patients, with multiple samples taken from each patient. And such data is likely to be dependent on the individual group. In our example, the patient id for each sample will be its group identifier. In this case we would like to know if a model trained on a particular set of groups generalizes well to the unseen groups. To measure this, we need to ensure that all the samples in the validation fold come from groups that are not represented at all in the paired training fold. 123456789from sklearn.model_selection import GroupKFoldX = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]y = [&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;d&quot;, &quot;d&quot;, &quot;d&quot;]groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]gkf = GroupKFold(n_splits=5)for train, test in gkf.split(X, y, groups=groups): print(&quot;%s %s&quot; % (train, test)) Time Series Split Time series data is characterised by the correlation between observations that are near in time (autocorrelation). However, classical cross-validation techniques assume the samples are independent and identically distributed, and would result in unreasonable correlation between training and testing instances (yielding poor estimates of generalisation error) on time series data. Therefore, it is very important to evaluate our model for time series data on the “future” observations least like those that are used to train the model. Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them. Also, it adds all surplus data to the first training partition, which is always used to train the model. 123456789from sklearn.model_selection import TimeSeriesSplitX = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])tscv = TimeSeriesSplit(n_splits=3)for train, test in tscv.split(X): print(&quot;%s %s&quot; % (train, test))#&gt;&gt;&gt; [0 1 2] [3]#&gt;&gt;&gt; [0 1 2 3] [4]#&gt;&gt;&gt; [0 1 2 3 4] [5] Tuning Hyper-parametersSo Scikit-learn provides tools to tune hyper-parameters. That’s to say, we don’t have start with train-validation-test and then input different hyper-parameter and then print out validation error. We can input the desire model, and a list of hyper-parameters to choose from, and then scikit-learn will iterate and gives the best combination. Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to “train” the parameters of the grid. When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process: it is recommended to split the data into a development set (to be fed to the GridSearchCV instance) and an evaluation set to compute performance metrics. There are two ways to tune hyper-parameters. Grid Search The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. 12345678910111213141516# Split the dataset in two equal partsX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0)# Set the parameters by cross-validationtuned_parameters = [&#123;&apos;kernel&apos;: [&apos;rbf&apos;], &apos;gamma&apos;: [1e-3, 1e-4], &apos;C&apos;: [1, 10, 100, 1000]&#125;, &#123;&apos;kernel&apos;: [&apos;linear&apos;], &apos;C&apos;: [1, 10, 100&#125;]clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring=&apos;precision&apos;)clf.fit(X_train, y_train)print (clf.best_params_)print (clf.cv_results_[&apos;mean_test_score&apos;])y_true, y_pred = y_test, clf.predict(X_test) Randomized Search RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search: A budget can be chosen independent of the number of parameters and possible values. Adding parameters that do not influence the performance does not decrease efficiency. 123456789101112131415161718192021X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0)# specify parameters and distributions to sample fromparam_dist = &#123;&quot;max_depth&quot;: [3, None], &quot;max_features&quot;: sp_randint(1, 11), &quot;min_samples_split&quot;: sp_randint(2, 11), &quot;min_samples_leaf&quot;: sp_randint(1, 11), &quot;bootstrap&quot;: [True, False], &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]&#125; clf = RandomForestClassifier(n_estimators=20)# run randomized searchrandom_search = RandomizedSearchCV(clf, param_dist, n_iter=20)random_search.fit(X, y)print (clf.best_params_)print (clf.cv_results_[&apos;mean_test_score&apos;])y_true, y_pred = y_test, clf.predict(X_test)","tags":"python-packages machine-learning model-selection"},{"title":"Community Detection in Python","url":"/Community-Detection/","text":"NetworkX vs. IGraphImplementation NetworkX: implemented in Python IGraph: implemented in C PerformanceIGraph wins. IGraph NetworkX Single-source shortest path 0.012 s 0.152 s PageRank 0.093 s 3.949 s K-core 0.022 s 0.714 s Minimum spanning tree 0.044 s 2.045 s Betweenness 946.8 s (edge) + 353.9 s (vertex) (~ 21.6 mins) 32676.4 s (edge) 22650.4 s (vertex) (~15.4 hours) See ref No. of Community Detection AlgorithmsIGraph wins. NetworkX: only optimal modularity. IGraph: nine algorithms including optimal modularity; edge betweenness etc. Ease of ProgrammingNetworkX wins. Load a graph NetworkX can simply load a graph from a list of edge tuples. 12345edges = [(1,2),(2,3),(3,4),(4,5)]G = nx.Graph()# add edges from txt G.add_edges_from(edges) IGraph needs to first load vertices and then a list of edge tuples 1234567891011121314151617# create graphg = Graph()# in order to add edges, we have to add all the vertices first#iterate through edges and put all the vertices in a listvertex = []for edge in edges: vertex.extend(edge)g.add_vertices( list( set(vertex))) # add a list of unique vertices to the graphg.add_edges(edges) # add the edges to the graph. &quot;&quot;&quot;Note: add_edges is much quicker than add_edge. for every time add_edge is performed, the entire data structure in C has to be renewed with a new series ordering.&quot;&quot;&quot;print g Get information from existing graph NetworkX 123## get no. of neighbors of a node by node namenode_index = G.vs.index(target_node) neighbors = G.adjacency_list()[node_index] IGraph 123456789101112## get no. of neighbors of a node by node namenode_index = g.vs.find(target_node).index #find index by nodenameneighbors = []# g.incident(node_index) gives a series of edges that passes the target node index# iterate through the edges, and append the other node into neighbors listfor edge in g.es[g.incident(node_index)]: if edge.target == node_index: neighbors.append(g.vs[edge.source][&quot;name&quot;]) else: neighbors.append(g.vs[edge.target][&quot;name&quot;]) Community Detection AlgorithmsA list of algorithms available in IGraph include: Optimal Modularity Edge Betweenness (2001) Fast Greedy (2004) Walktrap (2005) Eigenvectors (2006) Spinglass (2006) Label Propagation (2007) Multi-level (2008) Info Map (2008) Summary For directed graph: go with Info Map. Else, pls continue to read. If compuational resources is not a big problem, and the graph is &lt; 700 vertices &amp; 3500 edges, go with Edge Betweenness; it yields the best result. If cares about modularity, any of the remaining algorithms will apply; If the graph is particularly small: &lt; 100 vertices, then go with optimal modularity; If you want a first try-on algorithm, go with fast greedy or walktrap If the graph is bigger than 100 vertices and not a de-generated graph, and you want something more accurate than fast greedy or walktrap, go with leading eigenvectors If you are looking for a solution that is similar to K-means clustering, then go for Spinglass ​ Optimal Modularity Definition of modularity: Modularity compares the number of edges inside a cluster with the expected number of edges that one would find in the cluster if the network were a random network with the same number of nodes and where each node keeps its degree, but edges are otherwise randomly attached. Modularity is a measure of the segmentation of a network into partitions. The higher the modularity, the denser in-group connections are and the sparser the inter-group connections are. Methodology: GNU linear programming kit. Evaluation: Better for smaller communities with less than 100 vertices for the reasons of implementation choice. Resolution limit: when the network is large enough, small communities tend to be combined even if they are well-shaped. Fast Greedy Methodology: Bottom up hierarchical decomposition process. It will merge two current communities iteratively, with the goal to achieve the maximum modularity gain at local optimal. Evaluation: Pretty fast, can merge a sparse graph at linear time. Resolution limit: when the network is large enough, small communities tend to be combined even if they are well-shaped. Walktrap Methodology: Similar to fast greedy. It is believed that when we walk some random steps, it is large likely that we are still in the same community as where we were before. This method firstly performs a random walk 3-4-5, and merge using modularity with methods similar to fast greedy. Evaluation: A bit slower than fast greedy; A bit more accurate than fast greedy. Multi-level Methodology: Similar to fast greedy, just that nodes are not combined, they move around communities to make dicision if they will contribute to the modularity score if they stay. Eigenvectors Methodology: A top down approach that seeks to maximize modularity. It concerns decomposing a modularity matrix. Evaluation: More accurate than fast greedy Slower than fast greedy Limitation: not stable on degenerated graphs (might not work!) Label Propogation Methodology: A bit like k-clustering, with initialization k different points. It uses an iterative method (again just like k-means): the target label will be assigned with the most “vote” of the lables from its neighbors; until the current label is the most frequent label. Evaluation: Very fast Like K-Means, random initialization yields different results. Therefore have to run multiple times (suggested 1000+) to achieve a consensus clustering. Edge Betweenness Definition of edge betweenness: Number of shortest path that passes the edge. It’s not difficult to imagin that, if there is an edge that connects two different groups, then that edge will has to be passed through multiple times when we count the shortest path. Therefore, by removing the edge that contains with the highest number of shortest path, we are disconnecting two groups. Methodology: Top down hierarchical decomposition process. Evalution: Generally this approach gives the most satisfying results from my experience. Pretty slow method. The computation for edge betweenness is pretty complex, and it will have to be computed again after removing each edge. Suitable for graph with less than 700 vertices and 3500 edges. It produces a dendrogram with no reminder to choose the appropriate number of communities. (But for IGraph it does a function that output the optimal count for a dendrogram). Spinglass Methodology: Complicated enough for me to ignore.. Evaluation: Not fast has a bunch of hyperparameters to tune from Infomap Methodology: It is based on information theoretic principles; it tries to build a grouping which provides the shortest description length for a random walk on the graph, where the description length is measured by the expected number of bits per vertex required to encode the path of a random walk. Evaluation: Used for directed graph analytics Reference Graph-tool performance comparison Python IGraph Manual Modularity (Networks)) Tamas’ answer to “What are the differences between community detection algorithms?”","tags":"python-packages network-analytics machine-learning"},{"title":"Learn ML from Sklearn Preprocessing","url":"/Sklearn-Preprocessing/","text":"PreprocessingEncoding Categorical Features Integer representation can not be used directly with scikit-learn estimators, as these expect continuous input, and would interpret the categories as being ordered, which is often not desired (i.e. the set of browsers was ordered arbitrarily). One possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K or one-hot encoding, which is implemented in OneHotEncoder This estimator transforms each categorical feature with m possible values into m binary features, with only one active. 12345678from sklearn import preprocessingenc = preprocessing.OneHotEncoder()X_encoded = enc.fit_transform(X) #if X contains missing categorical features, one has to explicitly set n_valuesenc = preprocessing.OneHotEncoder(n_values=[1, 2, 3, 4])X_encoded = enc.fit_transform(X) Imputation of Missing Values A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. Basic strategies for imputing missing values: either using the mean, the median or the most frequent value of the row or column in which the missing values are located. 123from sklearn.preprocessing import Imputerimp = Imputer(missing_values=&apos;NaN&apos;, strategy=&apos;mean&apos;, axis=0)X_imputed = imp.fit(X) Standardization It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features. To address this issue you can use PCA with whiten=True to further remove the linear correlation across features. Standardization Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. 12from sklearn import preprocessingX_scaled = preprocessing.scale(X) Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) should be learnt from a training set and applied to held-out data for prediction: 123456from sklearn import preprocessingX_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.4)# instead of preprocessing.scale, it&apos;s recommended to use scalerscaler = preprocessing.StandardScaler().fit(X_train)X_train_transformed = scaler.transform(X_train)X_test_transformed = scaler.transform(X_test) Scaling sparse dataSummary: do the standardization without centering. Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it can make sense to scale sparse inputs without centering, especially if features are on different scales. 1234567from sklearn import preprocessingmax_abs_scaler = preprocessing.MaxAbsScaler()X_scaled = max_abs_scaler.fit_transform(X)## alternativescaler = preprocessing.StandardScaler(with_means = False)X_scaled = scaler.fit(X) Scaling data with outliersSummary: use median instead of mean, use IQR instead of standard deviation. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results. 123from sklearn.preprocessing import RobustScalerrobust_scaler = RobustScaler()X_scaled = robust_scaler.fit_transform(X) Normalization Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples. This assumption is the base of the Vector Space Model often used in text classification and clustering contexts. 123from sklearn import preprocessingX_l2_normalized = preprocessing.normalize(X, norm=&apos;l2&apos;)X_l1_normalized = preprocessing.normalize(X, norm=&apos;l1&apos;) Binarization Feature binarization is the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution. 123from sklean import preprocessing binarizer = preprocessing.Binarizer(threshold=1.1)binarizer.transform(X)","tags":"python-packages machine-learning preprocessing"},{"title":"学医偶得 一","url":"/学医偶得/","text":"总纲次序 有的朋友每天要按摩很多穴位，还要刮痧、拔罐、练功，总觉得运用的方法越多，治疗的效果越好。其实并非如此，我们的气血就那么多，我们需集中力量，逐个解决身体的问题；切不可将气血分散各处，无的放矢，这样越治问题会越多，终将失去信心和耐心。记住：简单才有效，顺势才迅捷。 手法 酸和痛都表示经络尚通畅，但在该处狭窄或有拥堵，流通不畅快。酸多表示气血虚弱，需要补，不可采用过强手法。而刺痛则表明那地方有气血在，却堵住了，气血正在努力冲撞，此时则稍微用力度大的手法帮助疏通。 肺经时辰 人的气血在夜里3点到5点（也就是寅时）开始冲击肺经，所以此时若出现症状，我们通常要考虑到肺是不是有问题。曾治过一个妇女，每到冬季总是在凌晨4点钟左右躁热出汗，白天则畏寒怕冷。诊断她为风寒束表，心火内盛，典型的“冰包火”。但其发病的根源是肺气不足，无力助心火以驱散风寒，必借寅时肺经气盛才能发汗解表。所以我用补中益气汤补肺而助其宣发之力，顺势而为，一剂而愈。 功用 上可疏解肝经之郁结，中可运化脘腹之湿浊，下可补肾中之亏虚。 “诸气者，皆属于肺。”《内经》的话句句都是金玉良言，须仔细体悟才行。所以，气虚的培补、气逆的顺调、浊气的排放、清气的灌溉，都可以通过调节肺的功能来实现。 “肺主宣发肃降，肺是水上之源，肺开窍于鼻，肺主皮毛，诸气愤郁，皆属于肺，在志为忧悲，在液为涕，在体合皮毛，在窍为鼻。” 实例 少商：咽痛 治嗓子痛效果最佳，尤其是对急性咽喉肿痛有特效。 但是这个穴必须得强刺激才行，过去这些井穴（末梢的穴都叫井穴）通常都需要用三棱针来点刺，放一滴血，当时就会见效，但好多人就怕放血，这时不妨用指甲使劲掐一掐。 鱼际：心火 定喘的效果很好，只需按揉即可。夜里咳嗽，尤其是两三点钟咳嗽、睡不着觉，这种现象非常普遍，通常就是肝火引起的。这时候按摩鱼际穴就会缓解。 善于退热的要穴。当您心里有火、夜间爱咳嗽、比较烦热、睡不着觉时，按揉鱼际穴特别管用。 调节小孩的肠胃功能。在中医院小儿科，鱼际穴又叫板门穴。“板门”就是木板的房门，此穴是专门调理小孩不爱吃东西、肠胃功能不好的，一揉“板门”，胃口的门就打开了。 在大拇指下肉肚最高点。 太渊：补气、脉之会 有人总觉得气不够使，有吸不上气的感觉，就点揉太渊穴，此穴为肺经原穴，补气效果极佳。 喘气费劲（吸入氧气不够）、爬一会儿山甚至动一动就一头汗或者气不足、大便时老觉得没劲或使不上劲，这样的人就要补气。有的人是偏血虚，有的人是偏气虚。偏气虚的人要想直接补血是补不上的，一定要先补气，气足了血才能生。如果火气很旺，那就是气实，不需要直接补气。而气虚的人都是虚寒体质，都需要补气。 其为脉之会，就是体内所有脉都归它控制。心脏跳动异常、早搏、房颤，只要跟心血管有关系的，都是太渊穴的适应范围。静脉曲张、脉管炎这些跟血管、脉络有关系的病症，按揉太渊穴都有效果。 太渊穴正好在腕横纹上，很深。在揉它的时候，一定要把指甲剪平，也可以用大拇指内侧硌它。 经渠：气不顺 “经”是经络，“渠”是水渠。“经渠”的意思是经络到此就水到渠成了。经渠穴是治疗气不顺的。好多虚弱体质的人就是因为气乱了，而这个穴是一个慢慢调养的穴，它可以使肺气逐渐增强，最后达到一个水到渠成的目的。也就是说，每天都能给您补充一点点精力，而且对实证、虚证都管用。 按这个穴位的时候不要按到骨头上，而要按到骨头内侧缘。不要往下按，要往外按，搓着按就能找到这个穴了。平常可以经常揉一揉它，觉得气有点儿不太顺或者气接不上来都可以揉，而且无论是热性、寒性咳嗽都可以揉， 列缺：头颈不适 “头项寻列缺”，脖子落枕、感冒引起的头痛，都跟风寒有关，平时您可要多揉列缺穴。 孔最：扁桃体炎 “孔”是毛孔，“最”是最大。孔最穴的意思就是身体里所有跟孔有关的问题都归它来管理。上至鼻窍，下至肛门，都跟孔有关。对风寒感冒引起的咳嗽和扁桃体炎效果不错。有的人发烧不出汗，赶紧揉孔最穴可以帮助发汗。 尺泽：泻肺补肾 给肾以恩泽，给肾以浇灌，最好的补肾穴，通过降肺气而补肾，最适合上实下虚的人。肺属金，肾属水，而金能生水，就是肺气足了就可以补肾。所以，揉尺泽穴就能把肺经多余的能量补到肾经上去。尺泽穴又是合穴，属水，所以这种补肾方法叫做泻肺补肾法。其实，泻只是能量的一种转化，是把肺经多余的能量转换到肾经上去了，因为上焦的能量过多、淤积住了，反而让人觉得不舒服，老有火气，老想吃点儿凉的或者祛火的东西，而同时却两脚冰凉。这是火气都用到上边去了，没有留些到下面来，形成了上实下虚之证。 侠白：解气郁恐惧 “侠”是侠客，“白”是白色。肺属金，金在五行的颜色为白，因此，这里“白”代表肺的意思。而“侠白”就是有个侠客在保护肺，给我们补足肺气，让我们无所畏惧。所以它可以治疗肺气不足造成的经常恐惧、心跳过速。为什么人会恐惧呢？就是先有忧虑，忧虑解不开了就会恐惧。而且一忧虑就会气郁，常常气串两肋，所以按揉侠白穴还可以治疗肋间神经痛，即两肋痛。 天府：通鼻窍 鼻窍通于天，天府穴暗含着这个意思，就是能治鼻子的各种疾患。像过敏性鼻炎、慢性鼻炎、经常流鼻血等鼻子的疾病，揉天府穴效果非常好。 在腋下3寸，此穴可以用一种特殊的方法来找到。两臂张开，掌心相对平伸，在鼻尖上涂上一点墨水，用鼻尖点臂上，点到处就是此穴。 云门：浊气宣发 这里是一个气体宣发的地方。很多人爱生气，气完就憋在那里了，宣发不出去，于是循着肺经走到四肢，就会造成四肢烦热、特别燥、心里堵闷、掌心热等症状。这时，使劲点揉云门穴，一般就会打嗝，气就发出去了。打通经络，其中的一个主要目的就是排除浊气。好多人一揉这个肺经就老打嗝，这是非常好的现象。 中线任脉旁开６寸，锁骨下缘处。两手叉腰时，此处会有一个三角窝。 中府：调理中气 “中”指中气，就是脾肺之气，脾和肺合起来的气叫中气。如果经常觉得气不够使，喘不上气来，或者大便的时候无力，以及吃一点东西肚子就胀，这就是中气不足了。中府穴就是专门调治中气不足的。中府穴是肺经的一个募穴，也是脾肺两经交会的一个穴，这个穴调气最好。如果人体的气乱了，就爱咳嗽、哮喘、堵闷，会经常觉得上气不接下气，这时一定要多揉中府穴。 在云门下面一寸。推的时候，大拇指按着中府穴，然后向上推云门穴，一般这里会很痛。把痛的地方给推开，浊气就会散掉，您就会觉得胸里面非常舒服。 制约 肺功能不好就是肝里的浊气熏蒸肺造成的，主要的源头在肝，所以从肝上来调节才是治本的方法。 肺本是娇脏，最怕攻伐，所以“调诸脏即是治肺”实乃真知灼见。如咳喘症，也很少由肺经直接引起，多是其他脏波及。由肝火引起的叫“木火刑金”，祛肝火就好；由肾虚引起的叫“肾不纳气”，补肾气辄效；由脾虚引起的叫“痰湿蕴肺”，健脾祛湿最佳。还有外感咳嗽，多由风寒引起，那就赶走膀胱经之风寒好了。通常咳喘的病总会迁延不愈，古时便有“内科不治喘”之说，其实多是因见肺治肺，有痰化痰，宣来降去，不治根本，才成痼疾。 大肠经功用 肺与大肠相表里，所以肺脏上面有什么疾患，都可以通过大肠经来调理，它主治皮肤病，也管便秘、腹泻等肠道疾病。 皮肤病可以说是最让人心烦意乱的疾病了，荨麻疹、神经性皮炎、日光性皮炎、牛皮癣、疥疮、丹毒、疖肿、皮肤瘙痒症……都让人痛苦不堪。在百治无效之际，取大肠经刮痧，通常都会得到不同程度的缓解。大肠经为多气多血之经，阳气最盛，用刮痧和刺络的方法，最善祛体内热毒。若平日常常敲打，可清洁血液通道，预防青春痘。大肠经对现代医学所讲的淋巴系统有自然保护功能，经常刺激可增强人体免疫力，防止淋巴结核病的生成。 实例 三间：通经行气 最善通经行气，上可通达头面，治疗三叉神经痛、齿痛、目痛、喉肿痛和肩膀痛；下能通腹行气，泻泄可止，便秘可通。另外，有研究指出此穴有消炎、止痛、抗过敏的功效。 位于食指近拇指侧根部，第二掌指关节后。常用大拇指内侧指节横向硌揉此穴，效果甚佳。","tags":"肺经"},{"title":"偶有所得","url":"/偶有所得/","text":"遗言软件/网站如果我死了，我不希望我默默无名地死去。不希望该说出口的爱意仍被沉默吞没，不希望抱歉也只能随着骸骨而去，不希望我的葬礼上亲人好友想起和我最后对话的惋惜。我还想亲自定葬礼的形式呢。可是这个网站有一定风险- 怎么确定一个人死了呢？如果还没有死遗嘱就发了不是很尴尬吗？ air穿衣App。像Uber和Airbnb一样出借多余的“审美”。 Machine Learning Literature Review网站。摘录最新方向的ML进展，以知识树形式呈现。两个维度： 作者影响力：类似于PageRank 时间：纯粹时间线，或论文“族谱” （引用关系） 苏轼的一生网站。时间轴标明苏轼年份大事记，重大转折点标红。 年份旁鼠标移位可见诗作关键意象（NLP) 及 代表作。 中医版Keep健身类App。体质测试/五脏状况自测/舌苔图像匹配 。健身计划打卡 Prayers’ Circle教堂/小组社交App。 发布祷告事项，旁人可点Prayer。发布者可看到代祷人数。代祷人姓名不可见。上帝回应则划过，代祷人皆收到通知。按月/半年/年度统计基础数字，关键词。小组负责人可见组内关键主题汇总，及相关资料推荐。 灵修笔记圣经类App。可以引用经文、方便分类的灵修笔记 。日期/主题view 。","tags":"未来梦"},{"title":"Optimizing SQL Statements","url":"/Optimizing-SQL/","text":"PART I Database LevelChoice of Storage EngineThe most commonly used storage engines are: InnoDB: Default setting for MySQL. Pros: A transactional storage engine. That means it supports rollback / commit (All or Nothing), rather writing every change directly into disk (though by default auto commit is on, but you can use SET AUTOCOMMIT=0; ). Row-level locking. That means, it allows concurrent access to one table. Better to use when you will have multiple read/ write sections simultaneously. Foreign key constraints. It automatically checks foreign integrity. Supports large buffer pool for both DATA and index. While MyISAM only supports buffer for index. Cons: Can’t be compressed for fast access in system table space. But it could have compress tables in general table space or file per table space. No full text indexing MyISAM: a non-transactional storage engine. Pros: It is designed for FAST READ. In a situation that the read is frequent while the write is little (read-write-ratio&lt;15%). Especially good for extensive SELECT queries. It’s frequently used in data warehousing. Full text indexing Cons: No foreign key constraints. Non-transactional. thus no roll-back capability. Row limit: $2^{32}$, about 4.3 billion records at maximum. Besides, other available storage engines include: CSV engine: stores data in CSV. Easily integrated with other applications. Archive storage engine: optimized for high speed inserting task. To see support engines in the database, 1show storage engines; To see engines currently used for each table in the database, 1SHOW TABLE STATUS FROM $YourDatabaseName\\G Sample Output: 123456789101112131415161718192021222324mysql&gt; SHOW TABLE STATUS FROM airline\\G*************************** 1. row *************************** Name: DIM_AIRLINE Engine: InnoDB Version: 10 Row_format: Dynamic Rows: 16256 Avg_row_length: 97 Data_length: 1589248Max_data_length: 0 Index_length: 1835008 Data_free: 0 Auto_increment: NULL Create_time: 2017-03-18 21:54:52 Update_time: NULL Check_time: NULL Collation: latin1_swedish_ci Checksum: NULL Create_options: Comment: *************************** 2. row *************************** Name: DIM_AIRPORT Engine: InnoDB(and more) To alter engine, 1ALTER TABLE $YourTableName ENGINE=&apos;MyISAM&apos;; Optimizing Buffering and CachingThe caching size should be large enough to hold frequently queried data and smaller than the memory size of the instance. InnoDB: Default caching size: 128 MB Show the current caching size: 1SELECT @@innodb_buffer_pool_size/1024/1024/1024; Sample results: 123456+------------------------------------------+| @@innodb_buffer_pool_size/1024/1024/1024 |+------------------------------------------+| 0.125000000000 |+------------------------------------------+1 row in set (0.00 sec) Change the caching size: 1mysqld --innodb_buffer_pool_size=8G --innodb_buffer_pool_instances=16 MyISAM: Key cache buffer is designed for index only and allows concurrent access. Show the current key cache buffer size: 1234567mysql&gt; SHOW VARIABLES LIKE &apos;key_buffer_size&apos;;+-----------------+----------+| Variable_name | Value |+-----------------+----------+| key_buffer_size | 16777216 |+-----------------+----------+1 row in set (0.00 sec) Change the caching size: 1SET GLOBAL key_buffer_size = 26777218; ​ A suggested strategy for MyISAM is, though, to divide cache into three parts: hot cache for read intensive index, cold cache for write intensive tables, and a default warm cache for other operations. ​ MySQL Query Cache: Stores query results. Useful when data tables doesn’t change a lot and queries are similar, such as web services. Show the current query cache size: 1234567mysql&gt; SHOW VARIABLES LIKE &apos;query_cache_size&apos;;+------------------+----------+| Variable_name | Value |+------------------+----------+| query_cache_size | 16777216 |+------------------+----------+1 row in set (0.00 sec) Change the caching size: 1SET GLOBAL query_cache_size = 46777216; PART II Table CreationTable CompressionTable compression is very useful for read-intensive applications. It helps particularly when there is a lot of character string columns. Why? “Because data compression enables smaller database size, reduced I/O, and improved throughput, at the small cost of increased CPU utilization.” Two ways to use compressed table for InnoDB: Upon table creation As compression is not enabled in the system table space (that is where there contains system files and we normally create tables), we need to create a general table space first. 1234-- Create a table spaceCREATE TABLESPACE `ts2` ADD DATAFILE &apos;ts2.ibd&apos; FILE_BLOCK_SIZE = 8192 Engine=InnoDB;-- Create a compressed tableCREATE TABLE t4 (c1 INT PRIMARY KEY) TABLESPACE ts2 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8; Alter table 1Alter table t4 ROW_FORMAT=COMPRESSED; For MyISAM engine, compressed table is generated with myisampack tool. Right IndexingWith clustered index, the table is sorted when being stored on disk. A non-clustered index is a row locator (RiD), it’s good to use in small table, but it finds a row required iterating through the whole table which is very bad for large table. In default, MySQL makes primary key as clustered index. Else MySQL will find the first all not null column as index (might be pretty long though!). According to MySQL documentation, “Accessing a row through the clustered index is fast because the index search leads directly to the page with all the row data. If a table is large, the clustered index architecture often saves a disk I/O operation when compared to storage organizations that store row data using a different page from the index record. (For example, MyISAM uses one file for data rows and another for index records.)” Other than clustered indexing (primary key), we could still set secondary indexing. The indexing will then contain data for the column which it indexes to, and the primary key. As you can see, without indexing, where clause has to go over the whole table to look up value. 123456789101112131415mysql&gt; explain select avg(carrier_delay) from FACT_FLIGHT_3_SAMPLE where unique_carrier = &apos;b6&apos; group by unique_carrier\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: FACT_FLIGHT_3_SAMPLE partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 50000 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) When indexing is created for the column of interests, the where clause is executed without accessing the actual table. 12345678910111213141516171819mysql&gt; CREATE INDEX carrier_ix ON FACT_FLIGHT_3_SAMPLE (UNIQUE_CARRIER ASC);Query OK, 50000 rows affected (0.10 sec)Records: 50000 Duplicates: 0 Warnings: 0mysql&gt; explain select avg(carrier_delay) from FACT_FLIGHT_3_SAMPLE where unique_carrier = &apos;b6&apos; group by unique_carrier\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: FACT_FLIGHT_3_SAMPLE partitions: NULL type: refpossible_keys: carrier_ix key: carrier_ix key_len: 9 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) According to experiment on data table with one billion entries, the time spent with indexing is only 50% without indexing. PART III Query LevelUse EXPLAINExplain is used to show a query execution plan. It helps understand what we are trying to do: how many operations are needed and how many rows are affected. Examples could be found in previous section. Queries Isolate “Isolate and tune any part of the query, so that a function could be called once for every row in the result set rather than in the whole table.” Aggregate functions: “The most efficient way to process GROUP BY is when an index is used to directly retrieve the grouping columns. With this access method, MySQL uses the property of some index types that the keys are ordered (for example, BTREE). This property enables use of lookup groups in an index without having to consider all keys in the index that satisfy all WHERE conditions. “ Reference MySQL Storage Engines When to use MyISAM and InnoDB SQL query performance killers – understanding poor database indexing Optimization Note* All quotes are from Reference 4 Optimization - the official documentation of MySQL’s.","tags":"mysql data-warehousing"},{"title":"Kalman Filter and Its Applications","url":"/Kalman-Filter/","text":"IntroductionKalman Filter, an artificial intelligence technology, has been widely applied in driverless car navigation and robotics. It works well in presence of uncertainty information in dynamic systems. It is a linear solution based on Bayesian Inference especially for state space models. – This is the definition in the hard way. Lets skip the first paragraph and look at a little story. Source: Kalman Filter: Theories and ApplicationsA group of young men were standing, under their feet there was a twisty narrow road to a very big tree. One man walked to the tree, asking, “can any of you walk to the tree with your eyes closed?” “That’s simple, I used to serve in the army.” Mike said. He closed his eyes and walked to the tree like a drunk man. “Well, maybe I haven’t practiced for long.” He murmured. - Depending on prediction power alone. “Hey, I have GPS!!” David said. He held the GPS and closed his eyes, but he also walked like a drunk man. “That’s very bad GPS!” He shouted, “it’s not accurate!!” - Depending on measurement which has big noises “Let me try.” Simon, who also served at the army before, grabbed the GPS and then walked to the tree with his eyes closed. - Depending on both of prediction power and measurement After reaching the tree, he smiled and told everyone, “I am Kalman.” In the story above, a good representation of the walking state at time k is the velocity and position. $$ X_k = (p, v) $$ So there are two ways to measure where you are. you predict based on your own command system - it records every commands sent to you, but only some of commands are executed exactly as what they were - wheels may slip or wind may affect; you measure by your GPS system - it measures where you are based on satellite, but it can’t be as accurate as in meters and sometimes signals lost. With Kalman Filter, we will get better understanding of where you are and how fast you go than either of the prediction or measurement. That is, we update our belief of where you are and how fast you go by incorporating the two sources of predictions and measurements using Bayesian inference. Understanding Kalman FilterNote: all the knowledge and photos for this section come from Ref 2. This is a study note only. The whole idea of Kalman Filter can be represented by a single picture. It might look complicated at this moment, but we will understand everything after this article (if not, read Ref 2 - it’s a much nicer article I believe). [source: ref 2] Prediction Phase: $X_k, P_k, F_k$$X_k$: Position and VelocityRemember in our scenario, we want to know the position and velocity. We represent the state of the walking people at time k as $X_k = [position_k; velocity_k]$. $F_k$: Prediction MatrixWe have $ Position_k = Velocity_{k-1} * t + Position_{k-1} $$ Velocity_k = Velocity_{k-1} $ The above two formulas could be written as: $ \\begin{bmatrix} Position_k \\\\ Velocity_k \\end{bmatrix} = \\begin{bmatrix} 1 &amp; t \\\\ 0 &amp; 1 \\end{bmatrix} * \\begin{bmatrix} Position_{k-1} \\\\ Velocity_{k-1} \\end{bmatrix} $ We represent the prediction matrix $\\begin{bmatrix} 1 &amp; t \\\\ 0 &amp; 1 \\end{bmatrix} ​$ as $F_k​$. We have then $ X_k = F_k * X_{k-1} ​$ $P_k$: Covariance MatrixSince in our case, the faster the robot walks, the further the position might be. Therefore velocity and position should be correlated; the covariance matrix is $P_k$ . When prediction matrix updates X_k, the change will also reflect on covariance matrix. Therefore $ P_k = F_k * P_{k-1} F_k^T $ . Measurement Phase: $Z_k, H_k$$H_k$: Measurement FunctionSometimes the GPS reading is not having the same units with the prediction states. For example, we use km/h in prediction states and we use m/s in GPS reading for velocity. So we need a transformation with matrix $H_k$: $$ X_k = H_k X_k $$ $$ P_k = H_k P_kH_k^T $$ $Z_k$: MeasurementRemember the GPS reading is not very reliable and might have some variations. So it is represented as a Gaussian distribution with mean $Z_k = [PositionReading_k, VelocityReading_k]$. So in below picture the pink circle represents the prediction distribution while the green circle represents the measurement distribution. The bright bubble insight represents the belief distribution of position and velocity. [source: ref 2] External Factors: $R_k, Q_k, B_k, U_k $$R_k, Q_k$: noises$Q_k$ is the transition covariance for the prediction phase. $ P_k = F_kP_{k-1}F_k^T + Q_k $. The idea is that there would always be some uncertainty. Therefore we kept an assumption - points might move a bit outside its original path. In practice, the value is often set very slow, for example: 12delta = 1e-5Q = np.array([delta/1-delta, 0; 0, dealta/1-delta]) $R_k$ is the observation covariance for the measurement phase. $ P’_k = P’_{k-1} + R_k $. Default set as 1. $B_k, U_k$: External influenceFor example, if the people are going down from the mountain, he might walker quicker because of the gravity. Therefore we have $$ Position_k = Velocity_{k-1} * t + Position_{k-1} + gt^2 /2 $$ $$ Velocity_k = Velocity_{k-1} + gt $$ Therefore we have $$ X_k = F_k X_{k-1} + \\begin{bmatrix} t^2/2 \\\\ t \\end{bmatrix} g = F_k X_{k-1} + B_k U_k $$ where $ B_k $ is the control matrix for external influence and $U_k​$ is the control vector. The Big Boss: Kalman Gain ?We are almost done explaining all the variables in Kalman Filter, except a very important term: Kalman Gain. This is a bit complicated, but luckily, this is not something we need to calculate or input. Now let’s go back to the measurement phase once more. When we multiplying the prediction distribution and measurement distribution, the new mean and variance go like this: $$ u’ = u_0 + \\sigma_0^2 (u_1 - u_0)/ (\\sigma_0^2 + \\sigma_1^2) $$ $$ \\sigma’^2 = \\sigma_0^2 - \\sigma_0^4/(\\sigma_0^2 + \\sigma_1^2) $$ so we make: $ k = \\sigma_0^2/ (\\sigma_0^2 + \\sigma_1^2) $ where k is the kalman gain, therefore we can simplify the above equations to: $$ u’ = u_0 + k(u_1 - u_0) $$ $$ \\sigma’^2 = (1-k)\\sigma_0^2 $$ Therefore Kalman gain helps updating the new $X_k, P_k$ value after seeing the measurement. So what is an intuitive explanation of Kalman Gain? It actually calculates the uncertainty in the prediction phase to the measurement phase, so it tells how much we should trust the measurement when updating $X_k, P_k$. Python ImplementationI’d love to recommend a great post which gives applications of Kalman Filter in financial predictions with codes posted on its Jupyter Notebook. It demonstrates why we should use Kalman Filter comparing to linear regression just in one picture: [source: ref 3] Parameter MappingRecall: Kalman Filter measures uncertain information in a dynamic systems. In this case, we want to know the hidden state slope and intercept. Let’s map all the inputs from theoretical to practical settings. Prediction Phase State: $ X_k = \\begin{bmatrix} \\alpha_k \\\\ \\beta_k \\end{bmatrix} $ Prediction Matrix: $ F_k = \\begin{bmatrix} 1&amp;0 \\\\ 0&amp;1 \\end{bmatrix} $. This is because we assumes the slope and intercept aren’t correlated. Intial State Covariance $ P_0 = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 1 \\end{bmatrix} $ Measurement Phase Measurement Function $ H_k = \\begin{bmatrix}EWA &amp; 1 \\end{bmatrix} $. The measurement we have is EWC. It’s obvious that it doesn’t share the same measuring units with slope and intercept. Since we have EWC = EWA*slope + intercept, therefore the measurement function should be [EWA 1]. Measurement Mean $Z_k = EWC$. External Factors Transition Covariance $Q = \\begin{bmatrix} 1e-5/ (1-1e-5) &amp; 0 \\\\ 0 &amp;1e-5/(1-1e-5) \\end{bmatrix} $ Observation Covariance R = $1$ Note that, the selection of Q and R here means the author wants to trust more in the prediction phase rather than the measurement phase. Or, as suggested in PyKalman documentation, values for $P_0, Q, R$ could be initialized using: 12kf = KalmanFilter(em_vars=[&apos;initial_state_covariance&apos;, &apos;transition_covariance&apos;, &apos;observation_covariance&apos;])kf.em(X, n_iter=5) Implementation CodesHere is the code source. I copied it here only for easy reading. 123456789101112131415161718192021import numpy as npfrom pykalman import KalmanFilter### construct the covariance matrix Q and measurement function Hdelta = 1e-5trans_cov = delta / (1 - delta) * np.eye(2)obs_mat = np.vstack([data.EWA, np.ones(data.EWA.shape)]).T[:, np.newaxis]### construct Kalman Filterkf = KalmanFilter(n_dim_obs=1, n_dim_state=2, initial_state_mean=np.zeros(2), initial_state_covariance=np.ones((2, 2)), transition_matrices=np.eye(2), observation_matrices=obs_mat, observation_covariance=1.0, transition_covariance=trans_cov)### get resultsstate_means, _ = kf.filter(data.EWC.values)slope = state_means[:, 0]intercept = state_means[:,1] Application in Dynamic RoISimilarly, diminishing marketing RoI could be measured in this way. We always write $$ Sales = Marketing Investment * RoI + Intercept $$ However, as time past by, the RoI should also be diminishing. So with Kalman Filter, the changing RoI could be captured. Meanwhile, Intercept also composite of historical sales, industry trends, buzz news etc and could be analyzed deeper. Reference Kalman Filter: Theories and Applications How a Kalman Filter works, in pictures Online Linear Regression using a Kalman Filter Estimating the Half Life of Advertisement, Prasad Naik, 1999 How to understand Kalman Gain intuitively pykalman documentation","tags":"machine-learning kalman-filter"},{"title":"Deep Learning Review Notes","url":"/Deep-Learning-1/","text":"Key Layers in a CNN Network Convolutional neural networks make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies). - AlexNet Convolutional LayerA convolutional layer compromise multiple convolutional kernels (image kernels). Below we will introduce what is an image kernel, why there are multiple image kernels than one, some computational details, and why a smaller image kernel is preffered in most recent research and models. What is an image kernel ​ image source So above picture demonstrates perfectly how a kernel of 3*3 with stride =1 works. Basically it slides through the picture one pixel a time (thus stride = 1), and performs element wise multiplication. Why do we need this? Here is a more straightforward summary of what a kernel can do: ​ image source Besides the above demonstrations, image kernels could also retain certain colors of the image, or bottom sobel etc. Here is a link you can play with and get more understanding towards image kernels. Why there are multiple image kernels in a convolutional layer?Here is a good answer (by Prasoon Goyal) for the first question: But clearly, why would you want only one of those filters? Why not all? Or even something that is a hybrid of these? So instead of having one filter of size 5×55×5 in the convolution layer, you have kk filters of the same size in the layer. Now, each of these is independent and hopefully, they’ll converge to different fitlers after learning. Here, k is the number of output channels. kk could be taken anywhere between few tens to few thousands. How to compute?So, the input of a convolutional neural network ususally has three dimensions: long, wide, and color dimension (if grey scale then two dimensions). Here is a good demo of how things work. Explained in text, if the input source is 7*7*3, and we have 2 kernels of size 3*3*3 with stride 2: The input could be seen as 3 stacked 2d 7*7 matrices; and each kernel could be seen as 3 stacked 3*3 matrix; Each stacked layer of the kernel will slide through the corresponding stacked layer of input with step of 2 pixels each time, and the element sum will be furthur added together. The output after each kernel sliding will then be a 33 2d matrix. Here is why: (input width - kernel width + 2 zero padding)/stride + 1 = (7-3+0)/2 + 1 = 3. As we have two kernels, so the output volume will be 3\\3*2. Smaller kernel size preferredAnother important thing to know is that, smaller kernels capture more details of pictures. This helps to understand the evolvement on CNNs. A demonstration is as below: Kernel size: 3*3 Kernel size: 10*10 image source Pooling LayerPooling layer is often used after convolutional layer for down sampling. It reduces the amount of parameters carried forward while retaining the most useful information; thus it also prevents overfitting. A demonstration for max pooling could be shown in following picture: ​ Image source Some visualizations could be found below: Feature Map After Pooling image source Another example (image source): Fully Connected LayersFor fully connected layers we have $$ output = activation(dot(input, kernel) + bias) $$ Some of the key activation functions are as follows: SigmoidSigmoid function pushes large positive numbers to 1 while large negative numbers to 0. However it has two fallbacks: 1) It will kill the gradient. If the value of a neuron is either 0 or 1, the gradient for the neuron will become so closed to zero that, it will “kill” the multiplication results for all gradients in back propagation computation. 2) The sigmoid output are all positive. It will cause the gradient on weights become all positive or all negative. [source: 3] TanhTanh activation is a scaled version of sigmoid function: $$tanh(x)=2σ(2x)−1tanh⁡(x)=2σ(2x)−1$$ Therefore it is zero centered with range [-1,1]. It still have the problem of killing gradient, but generally it is preferred to sigmoid activation. ReLUShort for Rectified Linear Units. A popular choice. It threshold upon 0. $$ max (0, x) $$ Comparing to the previous two activation methods, it’s much quicker to converge and involves much less computation time due to linearity. And it doesn’t have the issue of non-zero centered. However, it should be noted, if the learning rate is set to be high, part of the neurons will “die” - they will be not activated during the whole training phase. With the learning rate set to be smaller, it won’t be much an issue. And below is a demonstration of how ReLU activation looks like: ​ image source SoftMaxA very common choice for multi-class output activation. Batch Normalization Layer This type of layer turns out to be useful when using neurons with unbounded activations (e.g. rectified linear neurons), because it permits the detection of high-frequency features with a big neuron response, while damping responses that are uniformly large in a local neighborhood. It is a type of regularizer that encourages “competition” for big activities among nearby groups of neurons.” - AlexNet Batch normalization is a common practice in deep learning. In machine learning tasks, scaling with zero mean and one standard deviation will make the performance better. However, in deep learning, even if we normalize the data at the very beginning, the data distribution will change a lot in deeper layers. Therefore with batch normalization layer, we could always do data preprocessing again. It is often used right after the fully connected layer or convolutional layer, before the non-linear layers. It makes a significant difference and becomes much more robust to bad initializations. Note that, Fei-Fei Li claims the contributions of batch normalization is minimal. And the use of local response normalization could hardly be seen in recent year models. Drop Out LayerDrop out layer is a common choice to prevent over fitting. It’s fast and effective. It will keep some neurons activated or 0 according to probabilities. Sample Architecture and CodesSample architecture for convolutional neural network is as follows: [source: 3] Sample codes for MNIST solution using keras deep learning as follows: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122'''Transfer learning toy example:1- Train a simple convnet on the MNIST dataset the first 5 digits [0..4].2- Freeze convolutional layers and fine-tune dense layers for the classification of digits [5..9].Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_transfer_cnn.pyGet to 99.8% test accuracy after 5 epochsfor the first five digits classifierand 99.2% for the last five digits after transfer + fine-tuning.'''from __future__ import print_functionimport datetimeimport kerasfrom keras.datasets import mnistfrom keras.models import Sequentialfrom keras.layers import Dense, Dropout, Activation, Flattenfrom keras.layers import Conv2D, MaxPooling2Dfrom keras import backend as Know = datetime.datetime.nowbatch_size = 128num_classes = 5epochs = 5# input image dimensionsimg_rows, img_cols = 28, 28# number of convolutional filters to usefilters = 32# size of pooling area for max poolingpool_size = 2# convolution kernel sizekernel_size = 3if K.image_data_format() == 'channels_first': input_shape = (1, img_rows, img_cols)else: input_shape = (img_rows, img_cols, 1)def train_model(model, train, test, num_classes): x_train = train[0].reshape((train[0].shape[0],) + input_shape) x_test = test[0].reshape((test[0].shape[0],) + input_shape) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # convert class vectors to binary class matrices y_train = keras.utils.to_categorical(train[1], num_classes) y_test = keras.utils.to_categorical(test[1], num_classes) model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy']) t = now() model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) print('Training time: %s' % (now() - t)) score = model.evaluate(x_test, y_test, verbose=0) print('Test score:', score[0]) print('Test accuracy:', score[1])# the data, shuffled and split between train and test sets(x_train, y_train), (x_test, y_test) = mnist.load_data()# create two datasets one with digits below 5 and one with 5 and abovex_train_lt5 = x_train[y_train &lt; 5]y_train_lt5 = y_train[y_train &lt; 5]x_test_lt5 = x_test[y_test &lt; 5]y_test_lt5 = y_test[y_test &lt; 5]x_train_gte5 = x_train[y_train &gt;= 5]y_train_gte5 = y_train[y_train &gt;= 5] - 5x_test_gte5 = x_test[y_test &gt;= 5]y_test_gte5 = y_test[y_test &gt;= 5] - 5# define two groups of layers: feature (convolutions) and classification (dense)feature_layers = [ Conv2D(filters, kernel_size, padding='valid', input_shape=input_shape), Activation('relu'), Conv2D(filters, kernel_size), Activation('relu'), MaxPooling2D(pool_size=pool_size), Dropout(0.25), Flatten(),]classification_layers = [ Dense(128), Activation('relu'), Dropout(0.5), Dense(num_classes), Activation('softmax')]# create complete modelmodel = Sequential(feature_layers + classification_layers)# train model for 5-digit classification [0..4]train_model(model, (x_train_lt5, y_train_lt5), (x_test_lt5, y_test_lt5), num_classes)# freeze feature layers and rebuild modelfor l in feature_layers: l.trainable = False# transfer: train dense layers for new classification task [5..9]train_model(model, (x_train_gte5, y_train_gte5), (x_test_gte5, y_test_gte5), num_classes) code source: keras documentation Parameter TuningLosses Regression: Mean_squared_error, Mean_absolute_error, mean_absolute_percentage_error, mean_squared_logarithmic_error Classification: two most commonly used: squared_hinge, cross entropy for softmax output1) hinge: hinge, squared_hinge2) cross entropy: categorical_crossentropy, sparse_categorical_crossentropy, binary_crossentropy Optimizers Batch Size: It means the number of training examples in one forward-backward training phase. If the batch size is small, it requires less memory and the network trains faster (the parameters will be updated once a batch). If the batch size is large, the training takes more time but will be more accurate. Learning Rate: If the learning rate is small, it will takes so long to reach the optimal solution; if the learning rate is large, it will stuck at some points and fail to reach optimal. So the best practice is to use a time based learning rate - it will decrease after each epoch. How? Use parameter decay - a common choice is 1e-2. $$ lr = self.lr (1. / (1. + self.decay self.iterations)) $$ Momentum (used in SGD optimizer): It helps accelerating convergence and avoid local optimal. A typical value is 0.9 . Comparison between Deep Learning Frameworks Theano: a pretty old deep learning framework written in Java. Raw Theano might not be perfect but It has many easy-to-use APIs built on top of it, such as Keras and Lasagne. (+) RNN fits nicely (-) Long compile time for large models (-) Single GPU support (-) Many Bugs on AWS TensorFlow: a newly created machine learning framework to replace Theano - but TensorFlow and Theano share some amount of the same creators so they are pretty similar. (+) Supports more than deep learning tasks - can do reinforcement learning (+) Faster model compile time than Theano (+) Supports multiple GPU for data and model parallelism (-) Computational graph is written in Python, thus pretty slow Caffe: mainly used for visual recognition tasks. (+) Large amount of existing models (+) CNN fits nicely (+) Good for image processing (+) Easy to tune or train models (-) needs to write extra codes for GPU models (-) RNN doesn’t fit well so not good for text or sound applications Deeplearning4J: a deep learning library written in Java. It includes distributed version for Hadoop and Spark. (+) Supports distributed parallel computing (+) CNN fits nicely (-) Takes 4X computation time than the other three frameworks Keras: An easy-to-use Wrapper API for Theano, TensorFlow and Deeplearning4J. It supports all the functionality that TensorFlow supports! Pre-trained ModelsWhat are Pre-trained Models?Pre-trained models are those models that people train on a very large datasets, such as ImageNet (it has 1.2 million images and 1000 categories). We could either use it as a start point for the deep learning tasks to raise accuracy, or use them as a feature extraction tool and feed the features generated with pre-train models into other machine learning models (e.g. SVM). Pre-trained Models: A ComparisonSome of the pre-trained models for image tasks include: ResNet, VGG, AlexNet, GoogLeNet. We use top-1-error and top-5-error to represent the accuracy on ImageNet. Top-1-error is just 1- accuracy; top-5-error measures if the true label resides in the 5-most-probable labels predicted. Release Model Name Top-1 Error Top-5 Error Images per second 2015 ResNet 50 24.6 7.7 396.3 2015 ResNet 101 23.4 7.0 247.3 2015 ResNet 152 23.0 6.7 172.5 2014 VGG 19 28.7 9.9 166.2 2014 VGG 16 28.5 9.9 200.2 2014 GoogLeNet 34.2 12.9 770.6 2012 AlexNet 42.6 19.6 1379.8 Tensorflow has a recent package Slim that implements more advanced models including Inception V4 etc. Right now the lowest top-1-error is 19.6% by Inception-Resnet-V2. Reference Comparing Frameworks: Deeplearning4j, Torch, Theano, TensorFlow, Caffe, Paddle, MxNet, Keras &amp; CNTK Deep Learning with Theano, Torch, Caffe, TensorFlow, and Deeplearning4J: Which One Is the Best in Speed and Accuracy? CS231n Convolutional Neural Networks for Visual Recognition Pretrained Models An overview of gradient descent optimization algorithms Keras: Deep Learning library for TensorFlow and Theano An Intuitive Explanation of Convolutional Neural Networks ​","tags":"machine-learning tensorflow cnn"},{"title":"DS Interview Preparation: Capital One","url":"/Interview-Capital-One/","text":"The interviewer will cover 2-4 technical questions covering various topics relating to data science and Big data. This includes both abstract questions about mathematical concepts and problem solving/initiative, as well as experimental design and model building, and MapReduce. Credit Risk Related Suppose you were given two years of transaction history. What features would you use to predict credit risk? Transaction amount, Transaction count, Transaction frequency, transaction category: bar, grocery, jwery etc. transaction channels: credit card, debit card, international wire transfer etc. distance between transaction address and mailing address, fraud/ risk score. ​ Are false positives or false negatives more important? False negative. The cost of false positive is just inspections and some calls to check with customer, but the cost of false negative will be thousands of dollars. ​ How would you build a model to predict credit card fraud? What are potential Issues. Inputs &amp; Outputs.: We will map the inputs to the output - in this case zero or one, or the probabilities of credit card fraud. If given years of transactions, the inputs can be the ones in Q1. If given payment information, the inputs can be: Past due: 0-30 days, 30-60 days, over 90 days. Number of occurance. Credit usage. Debt ratio. Monthly income. Age. Number of dependents. Feature Engineering.: Deal with outliers, impute missing data. Model for imbalanced data.: reference Data level: We can either oversample/ undersample, Algorithm level: bagging based, or boosting based.or use tree ensembling methods. Model metrics.: to maximize true positive rate and to minimize false negative rate. If we use accuracy, we might come across accuracy paradox. ​ Source Image Model interpretation.: Oversample the fraud data or undersample the non-fraud data, and then apply decision tree and logistics regression. ML Questions How do you handle missing or bad data? For Missing Data: Dropping rows Impute data, average, mediean (more robust for high magnitude), most frequent value Build model to predict if there is high correlation exists. For Data with Outliers: 1234567891011121314151617181920212223242526272829303132333435363738394041424344&apos;&apos;&apos;Code reference:https://github.com/IdoZehori/Credit-Score/blob/master/Credit%20score.ipynb&apos;&apos;&apos; def mad_based_outlier(points, thresh=3.5): if len(points.shape) == 1: points = points[:,None] median = np.median(points, axis=0) diff = np.sum((points - median)**2, axis=-1) diff = np.sqrt(diff) med_abs_deviation = np.median(diff) modified_z_score = 0.6745 * diff / med_abs_deviation return modified_z_score &gt; threshdef percentile_based_outlier(data, threshold=95): diff = (100 - threshold) / 2.0 (minval, maxval) = np.percentile(data, [diff, 100 - diff]) return ((data &lt; minval) | (data &gt; maxval))def std_div(data, threshold=3): std = data.std() mean = data.mean() isOutlier = [] for val in data: if val/std &gt; threshold: isOutlier.append(True) else: isOutlier.append(False) return isOutlierdef outlierVote(data): x = percentile_based_outlier(data) y = mad_based_outlier(data) z = std_div(data) temp = zip(data.index, x, y, z) final = [] for i in range(len(temp)): if temp[i].count(False) &gt;= 2: final.append(False) else: final.append(True) return final ​ How would you use existing features to add new features? Transfer learning Transaction frequency, amount sum, times ​ If you’re attempting to predict a customer’s gender, and you only have 100 data points, what problems could arise? Overfitting. We might learn too much into some particular patterns within this small sample set so we lose generalization abilities on other datasets. ​ Explain the bias-variance tradeoff. Reference We call a model high bias if it’s robust to dataset, but it’s too simple so it can’t really get the prediction right. We call a model high variance if it can get things right but it’s too complicated so it’s super specific to datasets so lose the generalization ability to other datasets. Somewhere in the middle of too simple and too complicated we can find the right model, but going along one way or another will increase either bias or variance while decrease another one. ​ What does regularization do? Regularization penalizes complex models. It prevents the model from overfitting. ​ Difference between random forest and gradient boosted tree. Bagging and boosting are both ensemble methods to turn weak classifiers into a strong one. Bagging: to train a bunch of same weak learners from a subset of training data, and then take majority votes. The subset is taken using bootstrap method (sampling with replacement), and could be a subset of all training examples or a subset of all features. Boosting: to train a bunch of different weak learners from the whole set of training data. The whole idea is to iteratively add weights to examples that were wrongly classified by previous classifier combinations. the above two images are from The Down Low on Boosting ​ Design an AI program for Tic-tac-toehttps://en.wikipedia.org/wiki/Tic-tac-toe Statistics Interpret this ANOVA table. What is VIF (in regression output)? It is a measure of how much the variance of the estimated regression coefficient bk is “inflated” by the existence of correlation among the predictor variables in the model. A VIF of 1 means that there is no correlation among the kth predictor and the remaining predictor variables, and hence the variance of bk is not inflated at all. The general rule of thumb is that VIFs exceeding 4 warrant further investigation, while VIFs exceeding 10 are signs of serious multicollinearity requiring correction. Reference Engineering Write pseudocode for map reduce 123456789101112map(String key, String value)// key: document name// value: document contents for each word w in value EmitIntermediate(w, &quot;1&quot;)reduce(String key, Iterator values):// key: word// values: a list of counts for each v in values: result += ParseInt(v); Emit(AsString(result)); ​ 1234567891011121314151617181920212223public class WordCount &#123; public static class Map extends MapReduceBase implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException &#123; String line = value.toString(); StringTokenizer tokenizer = new StringTokenizer(line); while (tokenizer.hasMoreTokens()) &#123; word.set(tokenizer.nextToken()); output.collect(word, one); &#125; &#125; &#125;public static class Reduce extends MapReduceBase implements Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; public void reduce(Text key, Iterator&lt;IntWritable&gt; values, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException &#123; int sum = 0; while (values.hasNext()) &#123; sum += values.next().get(); &#125; output.collect(key, new IntWritable(sum)); &#125; &#125; ​ ##Behavioral/ Project Walkthrough Describe a time you worked on a team. How do you learn something new? Tell me about your experience in neural network? Tell me about a time that you had to persuade somebody","tags":"machine-learning data-science-interviews"},{"title":"This is my prayer","url":"/my-prayer/","text":"Open my eyes that I may see wonderful things in your law. Psalms 119:18 If any of you lacks wisdom, you should ask God, who generously give to all without finding fault, and it will be given to you. James 1:5 Blessed are the pure in heart, for they will see God. Matthew 5:8","tags":""},{"title":"DS Interview: Classification","url":"/Interview-Classification/","text":"Definition Explain what logistic regression is. How do we train a logistic regression model? How do we interpret its coefficients? Logistic Regression is mapping inputs to probabilities. The sigmoid function turns a non-differentiable cost function to a convex one. So training a logistic regression model is a matter of optimization problem, we can deploy gradient descent ( cost function) or gradient ascent ( likelihood function). The coefficient means unit of increase of the log odd [log P(y=1) - log P(y=0)] for one unit increase of a predictor, given other predictors constant. How do random forests work (in layman’s terms)? Random forest works in the following way: First, the random forests compromise of decision trees (that’s why it is a random forest!). The decision trees learn and VOTE; output with majority vote will be taken. Second, the random forests takes an tree bagging (boostrap aggregation) process: each of the decision trees are built with only a subset of the original train data - it samples with replacement. Third, random forests takes a feature bagging process: each of the decision trees will only train on a subset of all features. What is the maximal margin classifier? How this margin can be achieved and why is it beneficial? How do we train SVM? Data with different labels are separated by a hyperplane. Maximal margin classifier minimizes the total distance from the points to the hyperplane. Maximal margin classifier tolerate more noises thus more robust to overfitting. We can train it either using linear programming or gradient descent. What is a kernel? How to choose kernel? Explain the kernel trick. Kernel is legal definition of dot product. Choosing kernel requires domain knowledge, or we can simply apply cross validation. When the data are not seperable in the current feature space, a common approach is to map them into high-dimensional space and then find the hyperplane. The kernel trick is, we don’t have to compute the mapping coordinates of those features in high dimensional space - we only perform inner product multiplication in the current space and then raise to the power of n. Thus it saves a lot of computational resources. Why is naive Bayes so bad? How would you improve a spam detection algorithm that uses naive Bayes? Naive bayes has its name because it assumes all the features to be independent from each other. This is not the case, for example in NLP tasks. Ways to improve Naive Bayes: Complement Naive Bayes: also count words NOT IN the category; Feature engineering, such as retaining words with high kappa, stemming etc; PCA / covariance matrix could be employed. What is an Artificial Neural Network? What is back propagation?It is a classification method that model brain neuras to solve complex classification problems. Back propagation is a “learn from mistake” algorithm for neural network training. Back propagation, in its nature, is gradient descent. It compares the output with the label, and “propagate” the error back onto the previous layers and adjust the weights accordingly. Model Comparison What are the advantages of different classification algorithms? (Decision tree, Naive Bayes, logistic regression, SVM, Random Forest) Decision Tree: Pros: 1, Easy to interpret; 2, more robust to outliers; 3, works for non-linear separable data Cons: 1, Tree structures change; 2, Easy to overfit; 3, Performs relatively worse in linear separable tasks Naive Bayes:Pros: 1, Easy to train, converge quickly; 2, Gives generative story; Cons: 1, Constraint: Independence assumption; 2, Easily affected by outliers Logistic Regression: Pros: 1, Can apply regularization to prevent overfitting; 2, Easy to interpret with probabilities; 3, Well-performed; Cons: 1, Linear separable; 2, Complex for multi-class problem SVM: Pros: 1, Good performance; 2, Can apply different kernel (linear/ non-linear separable) Cons: 1, Slow to train, memory intensive (especially with cross validation for kernel selection); 2, complicated for multi-class problem Random Forests:Pros: 1, Very good performance; 2, Handles large numbers of features; 3, Tells which features matter more; 4, Deals well with missing data Cons: 1, Unlike decision tree, the process is hard to interpret; 2, Was observed to be overfitting on some datasets; 3, Slow to predict How can I classify supervised data that is probabilistic rather than deterministic? Either logistic regression, or linear regression with normalization. Metrics How to define/select metrics? What are benefits and weaknesses of various binary classification metrics? What is the best out-of-sample accuracy test for a logistic regression and why? Provide examples when false positives are more important than false negatives, false negatives are more important than false positives False positive is more important during early stage health scanning. At this moment the cost of delaying small problems are not big as false alarms bringing unnecessary worries to many patients. False negative is more important during cancer related examinations. Misses will cost lives. Cross validation What is cross-validation? Cross validation is a technique to assess how well the model perform on unseen data. It is used for either, model selection, or model performance estimation. Basically it randomly seperates the data into k-folds and train on k-1 folds and test on the last fold. k is always chosen to be 3, 5, 10. What are the pitfalls on relying on cross-validation to select models? Generally we use Cross validation for two purpose, model selection&amp; estimate model accuracy for future use. For model selection, cross validation actually measures “accuracy”. Then it will easily falls into accuracy paradox - that increasing accuracy doesn’t lead to a more desired model (the email spam filter could increase accuracy by setting the rule “no email is spammed”, as TP &lt; FP). So CV is not robust in this sense. For estimate model accuracy - normally it is training and test data falls in the same pattern, but the future data might not have the same pattern as your training/ test data. Potentially data shift exists.","tags":"machine-learning data-science-interviews"},{"title":"DS Interview: Linear Regression","url":"/Interview-Linear-Regression/","text":"Definition How would linear regression be described and explained in layman’s terms? Linear regression is to find the best fitting line given a bunch of points. The distance to the line is the “error” - because ideally we prefer every point is on that line. Mathematically, we want a line that produces as small total error as possible. That’s what we do in linear regression. What is an intuitive explanation of a multivariate regression? Normally regression only has one outcome (Y), and several predictor variables (X). In Multivariate regression, there are more than one outcomes (Y). What is gradient descent method? Will gradient descent methods always converge to the same point?Gradient descent is an interative algorithm to find optimal solution. Sometimes, for example, in k-means clustering it will converge to different local optimals with different initializations. In regression, yes, it will always converge to the global optimal point. What is the difference between linear regression and least squares? Linear regression is a statistical inference problem / machine learning model; and least squares describes one way to achieve the solution to the problem/ model. Alternatively, we can use MAE as measurement and gradient descent to find the solution to linear regression model. Statistical Perspective What are the assumptions required for linear regression? What if some of these assumptions are violated? First, variables are normal distributed. Can check with histogram/ QQ plot. Second, relationship is linear between independent and dependent variables (not polynomial etc.). Scatter plot is helpful in two dimensional case. Solution: change the model to include polynomial terms. Third, no linear dependency between predictors (no multi-collinearity). Use correlation/ covariance matrix to detect. Solution: dimensional reduction (PCA), select one particular variable from highly correlated variable set, or ridge regression (without expecting the coefficient of one single variable explain much) Fourth, there is no correlation between error terms (no autocorrelation). This means, dependent variables (y) are independent of each other. Use residual plot or Durbin – Watson test. Solution: time-series modelling Last, homoscedasticity. It means the residual remains same as x changes. Scatter plot with x,y,fitting line, residuals is a good measurement. Solution: log transformation, box-cox transformation What is collinearity and what to do with it? How to remove multicollinearity? This means there are at least two predictors are highly correlated. It will make interpretation of coefficients, lead to overfitting, or even failure of inverting the matrix.To remove multicollinearity, there are generally four ways:1, drop affected variables; 2, dimensional reduction (PCA); 3, ridge regression; 4, partial least square regression. Can you derive the ordinary least square regression formula? Regularization What is an intuitive explanation of regularization? Regularization is a technique to prevent overfitting. It enables the model to learn just the right amount of information about the data. Basically it penalizes the model if it goes too detailed that even learn specific patterns that will not necessarily present in the future. What is the difference between L1 and L2 regularization? L1 regularization gives sparse estimation by giving a Lapace prior for coefficients. It will force most of coefficients to be zero.L2 regularization will retain all predictors with Gaussian prior for coefficients. Some of the coefficients get larger and some get smaller. It’s easier to compute. What are the benefits and drawbacks of specific methods, such as ridge regression and lasso? Ridge regression is faster than lasso; lasso regression is a feature selection method itself. But we can’t claim which method to use without observing the distribution of data. Model Tuning/ Selection How do you choose the right number of predictors? One lazy method is lasso regression. We can also go through a feature selection process: First we decide the metric, such as MSE or R square. Then we can use greedy search and cross validation to build models with different combinations of predictors; and evaluate the models with test set. How to check if the regression model fits the data well? Adjusted R square, MSE What is the difference between squared error and absolute error? and Are there instances where root mean squared error might be used rather than mean absolute error? and How would a model change if we minimized absolute error instead of squared error? What about the other way around? Squared error/ root mean squared error - penalize more on big errors; while MAE is more robust to outliers. This is because taking square of a small item (&lt; 1) is even smaller, square of a large item is even larger. Others What are the drawbacks of linear model? Are you familiar with alternatives (Lasso, ridge regression, boosted trees)? Drawbacks: Rigid assumptions; overfitting problem; linearity; see previous regularization part; boosted trees turn a series of weak prediction models into a strong prediction by voting. Assume you need to generate a predictive model using multiple regression. Explain how you intend to validate this modelCross validation; R^2 or MSE; Residual analysis (see statistical part) Do we always need the intercept term in a regression model? Yes. Intuitively speaking, without the constant, the regression line has to cross the origin, but it’s not always the case. Statistically speaking, it allows the mean of residuals to be 0. Provide three examples of relevant phenomena that have long tails. Why are they important in classification and regression problems? Natural language (Zipf’s law); customer purchase; wealth. The problem associated include it violates the normal distribution and linearity assumption for linear model and create accuracy paradox in classification problem. We will need to perform log transformation for long tailed data.","tags":"machine-learning data-science-interviews"},{"title":"Install Pentaho with GUI on EC2 Linux Instance","url":"/Pentaho/","text":"In this post I will introduce how to install and visit the GUI interface from VNC client. I have been sitting here for 4 hours and were so frustrated for all the BUGS I saw.. Hopefully this post will help you get this done within one hour. Now let’s get started!! Security GroupWhen you run an instance, please edit the inbound traffic rules of security group:Protocol: Customed TCP/IP, Port 5901. This step is critical for setting up VNC server which we will introduce later. See ref. Install JavaPentaho runs on Java. Installation instruction goes here.Please, please check you’ve installed the SAME version of JDK and JRE! If you get error message “Unsupported major.minor version 52.0” then it’s very likely the version doesn’t match. At least it was the problem for me. Well it’s also possible that there is something wrong with yout /etc/environment : either you didn’t set up JAVA_HOME and Path; or you forgot to run source /etc/environment to confirm your setup. Download the binFile is available at here . If you can’t download with wget; try transfer downloads to your instance using filezilla. It took only half an hour to download and transfer to instance.After downloading (&amp; transfering), runchmod a+x pentaho-business-analytics-7.0.0-x64.bin./pentaho-business-analytics-7.0.0-x64.bin Install VNC server on server sideSteps as in Reference:12345678910sudo useradd -m pentahosudo passwd pentahosudo usermod -aG admin pentahosudo apt-get updatesudo apt-get install ubuntu-desktopsudo apt-get install vnc4serversu - pentahovncservervncserver -kill :1vim /home/pentaho/.vnc/xstartup and then change the document xstartup tp:12345678910111213141516#!/bin/shexport XKL_XMODMAP_DISABLE=1unset SESSION_MANAGERunset DBUS_SESSION_BUS_ADDRESS[ -x /etc/vnc/xstartup ] &amp;&amp; exec /etc/vnc/xstartup[ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresourcesxsetroot -solid greyvncconfig -iconic &amp;gnome-panel &amp;gnome-settings-daemon &amp;metacity &amp;nautilus &amp;gnome-terminal &amp; start VNC server again. Very Important: user name to start VNC server should be the same as username to run ./spoon.sh. Else will encounter org.eclipse.swt.SWTError. Download VNC clientI found RealVNC simple and good to use. After installation, open:${your public DNS}:1 Open PentahoIn VNC client, open terminal.Navigate usingcd /computer/home/pentaho/Pentaho/design-tools/data-integration/Final step: run./spoon.sh DONE!!!!","tags":"data-warehousing linux"},{"title":"Yoga! Yoga!","url":"/Yoga/","text":"这学期报了一个Yoga班。坦白来说，不过是为了学费回本并应付罗尼先生的“要运动要出汗”的要求，但每周有三天时间能荒废一小时在无用却美好的事情上，却觉得很值。虽然大腿战战，时常力竭，但身体似乎有好转！不知道是因为难得的脑子不动肢体却动的时光；还是真的如我猜测：瑜伽是一门作用于筋与脊柱的运动。而肝主筋，肾主骨，从中医的角度，瑜伽恰恰是对身体很好的一门锻炼。 文下记载课上常用Routine。有些衔接动作搜索不到相应图片，那就略过。因博主脑容量小，常常记忆紊乱，此帖长期更新。","tags":"yoga"},{"title":"categories","url":"/categories/index.html","text":"","tags":""},{"title":"tags","url":"/tags/index.html","text":"","tags":""}]}