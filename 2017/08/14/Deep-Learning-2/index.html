<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="XxyEUfc4yR7vI102EbbgOtuIXP1d7ew6TTq6J2wMlK8" />













  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Machine learning,CNN," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="Let’s start with the best tutorials for deep learning and CNNs.  Genreal Tutorials: An Intuitive Explanation of Convolutional Neural Networks by Ujjwal Karn Unsupervised Feature Learning &amp;amp; Deep Le">
<meta name="keywords" content="Machine learning,CNN">
<meta property="og:type" content="article">
<meta property="og:title" content="Learn CNN from Giants">
<meta property="og:url" content="http://yoursite.com/2017/08/14/Deep-Learning-2/index.html">
<meta property="og:site_name" content="Yoyo in Wanderland">
<meta property="og:description" content="Let’s start with the best tutorials for deep learning and CNNs.  Genreal Tutorials: An Intuitive Explanation of Convolutional Neural Networks by Ujjwal Karn Unsupervised Feature Learning &amp;amp; Deep Le">
<meta property="og:image" content="https://adeshpande3.github.io/assets/AlexNet.png">
<meta property="og:image" content="https://adeshpande3.github.io/assets/VGGNet.png">
<meta property="og:image" content="https://qph.ec.quoracdn.net/main-qimg-34686eb9aa41d84ec784164601174be5">
<meta property="og:image" content="https://adeshpande3.github.io/assets/GoogLeNet2.png">
<meta property="og:image" content="https://adeshpande3.github.io/assets/GoogLeNet3.png">
<meta property="og:image" content="https://adeshpande3.github.io/assets/ResNet.gif">
<meta property="og:image" content="https://adeshpande3.github.io/assets/ResNet.png">
<meta property="og:updated_time" content="2017-08-29T15:33:04.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Learn CNN from Giants">
<meta name="twitter:description" content="Let’s start with the best tutorials for deep learning and CNNs.  Genreal Tutorials: An Intuitive Explanation of Convolutional Neural Networks by Ujjwal Karn Unsupervised Feature Learning &amp;amp; Deep Le">
<meta name="twitter:image" content="https://adeshpande3.github.io/assets/AlexNet.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/08/14/Deep-Learning-2/"/>





  <title> Learn CNN from Giants | Yoyo in Wanderland </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  














  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yoyo in Wanderland</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/14/Deep-Learning-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yoyo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yoyo in Wanderland">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Learn CNN from Giants
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-14T10:47:49-04:00">
                2017-08-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Let’s start with the best tutorials for deep learning and CNNs.</p>
<ul>
<li>Genreal Tutorials:<ul>
<li><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="external">An Intuitive Explanation of Convolutional Neural Networks</a> by Ujjwal Karn</li>
<li><a href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial" target="_blank" rel="external">Unsupervised Feature Learning &amp; Deep Learning Tutorial</a> by Andrew NG</li>
<li><a href="http://cs231n.github.io" target="_blank" rel="external">CS231n Convolutional Neural Network for Visual Recognition</a> by Feifei Li</li>
<li><a href="http://deeplearning.net/tutorial/contents.html#" target="_blank" rel="external">Deep Learning Tutorial</a> by Theano Development Team</li>
</ul>
</li>
<li>Classic Papers:<ul>
<li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">AlexNet</a> by Alex Krizhevsky, Ilya Sutskever&amp; Geoffrey Hinton from University of Toronto</li>
<li><a href="https://arxiv.org/pdf/1409.1556v6.pdf" target="_blank" rel="external">VGG: Very Deep Convolutional Neural Networks for Large-Scale Image Recognition</a> by Visual Geometry Group, University of Oxford</li>
<li><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" target="_blank" rel="external">GoogLeNet: Going Deeper with Convolutions</a> by Google Inc</li>
<li><a href="https://arxiv.org/pdf/1512.03385v1.pdf" target="_blank" rel="external">ResNet: Deep Residual Learning for Image Recognition</a> by Microsoft</li>
</ul>
</li>
</ul>
<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p><a href="https://adeshpande3.github.io/adeshpande3.github.io/about/" target="_blank" rel="external">Adit</a> has a good summary of its importance:</p>
<blockquote>
<p>The one that started it all.</p>
<p>2012 marked the first year where a CNN was used to achieve a top 5 test error rate of 15.4% (Top 5 error is the rate at which, given an image, the model does not output the correct label with its top 5 predictions). The next best entry achieved an error of 26.2%, which was an astounding improvement that pretty much shocked the computer vision community. Safe to say, CNNs became household names in the competition from then on out.</p>
</blockquote>
<h3 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h3><ul>
<li>Year: 2012</li>
<li>Data: 1.2 million images from ImageNet LSVRC-2010</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>Top 1 Error</th>
<th>Top 5 Error</th>
</tr>
</thead>
<tbody>
<tr>
<td>AlexNet</td>
<td>37.5%</td>
<td>17.0%</td>
</tr>
</tbody>
</table>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p><img src="https://adeshpande3.github.io/assets/AlexNet.png" alt="AlexNet">                                    </p>
<p>​                                <a href="https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html" target="_blank" rel="external">Image Source</a> </p>
<table>
<thead>
<tr>
<th></th>
<th>Layers</th>
<th>Remarks</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>Convolutional</strong>: 96 kernels of 11*11*3, stride 4</td>
<td>response normalized &amp; pooled</td>
</tr>
<tr>
<td>2</td>
<td><strong>Convolutional</strong>: 256 kernels of 5*5*48</td>
<td>response normalized &amp; pooled</td>
</tr>
<tr>
<td>3</td>
<td><strong>Convolutional</strong>: 384 kernels of 3*3*256</td>
<td></td>
</tr>
<tr>
<td>4</td>
<td><strong>Convolutional</strong>: 384 kernels of 3*3*192</td>
<td></td>
</tr>
<tr>
<td>5</td>
<td><strong>Convolutional:</strong> 256 kernels of 3*3*192</td>
<td></td>
</tr>
<tr>
<td>6</td>
<td><strong>Fully connected</strong>: 4096 neurons</td>
<td></td>
</tr>
<tr>
<td>7</td>
<td><strong>Fully connected</strong>: 4096 neurons</td>
<td></td>
</tr>
<tr>
<td>8</td>
<td><strong>Fully connected</strong>: 4096 neurons</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Output layers: 1000, softmax</td>
</tr>
</tbody>
</table>
<p>Note that, later in the more successful version <a href="https://arxiv.org/pdf/1311.2901v3.pdf" target="_blank" rel="external">ZF Net</a>, the <strong>size of kernel is modified from 11*11*3 to 7*7*3</strong> to capture more details in the image.</p>
<ul>
<li>training schema</li>
</ul>
<blockquote>
<p>We use stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005.</p>
<p>We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and ﬁfth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates the early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron biases in the remaining layers with the constant 0.</p>
</blockquote>
<h3 id="Main-Take-aways"><a href="#Main-Take-aways" class="headerlink" title="Main Take-aways"></a>Main Take-aways</h3><table>
<thead>
<tr>
<th></th>
<th>Save Time</th>
<th>Increase Accuracy</th>
<th>Reduce Overfitting</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ReLU</strong></td>
<td><strong>6 times faster</strong> than Tahn/ Sigmoid activations</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Multiple GPUs</strong></td>
<td>☑️</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Drop Out layer</strong></td>
<td><strong>1/2 of time</strong> required to converge</td>
<td></td>
<td>☑️</td>
</tr>
<tr>
<td><strong>Local Response Normalization</strong></td>
<td></td>
<td>Top1: <strong>1.4%</strong>; Top5: <strong>1.2%</strong></td>
<td></td>
</tr>
<tr>
<td><strong>Overlapping Pooling</strong></td>
<td></td>
<td>Top1: <strong>0.4%</strong>; Top5: <strong>0.3%</strong></td>
<td></td>
</tr>
<tr>
<td><strong>Data Augmentation</strong></td>
<td></td>
<td></td>
<td>☑️</td>
</tr>
</tbody>
</table>
<ul>
<li><p><strong>Data Augmentation Methodologies</strong></p>
<blockquote>
<p>Extracting random 224 × 224 patches (and their horizontal reﬂections) from the 256×256 images and training our network on these extracted patches 4.</p>
<p>Altering the intensities of the RGB channels in training images…. We perform PCA on the set of RGB pixel values, To each training image, we add multiples of the found principal components, with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from a Gaussian with mean zero and standard deviation 0.1.</p>
</blockquote>
</li>
</ul>
<h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><p>VGG is the first paper that discusses about the depth of CNN architecture. It extends the number of layers to 19 and uses very small (3*3) convolutional filters. It also states that VGG model could be used as a part in other machine learning pipeline as deep features.</p>
<h3 id="Statistics-1"><a href="#Statistics-1" class="headerlink" title="Statistics"></a>Statistics</h3><ul>
<li>Year: 2014</li>
<li>Data: 1.3 million images from ImageNet ILSVRC-2012</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>Top 1 Error</th>
<th>Top 5 Error</th>
</tr>
</thead>
<tbody>
<tr>
<td>AlexNet</td>
<td>37.5%</td>
<td>17.0%</td>
</tr>
<tr>
<td><strong>VGG</strong></td>
<td><strong>23.7%</strong></td>
<td><strong>7.3%</strong></td>
</tr>
</tbody>
</table>
<h3 id="Architecture-1"><a href="#Architecture-1" class="headerlink" title="Architecture"></a>Architecture</h3><p><img src="https://adeshpande3.github.io/assets/VGGNet.png" alt="vgg"></p>
<p>​                                <a href="https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html" target="_blank" rel="external">Image Source</a></p>
<p>Note that, All hidden layers are equipped with the rectiﬁcation (ReLU (Krizhevsky et al., 2012)) non-linearity. the ReLU activation function is not shown for brevity. </p>
<ul>
<li><p><strong>Changes from AlexNet</strong></p>
<p>|                                  | AlexNet                       | VGG                                      |<br>| ——————————– | —————————– | —————————————- |<br>| <strong>Layers</strong>                       | 5 Conv, 3 FC layers           | 16 Conv, 3 FC layers                     |<br>| <strong>Convolutional Filters</strong>        | 11*11; stride = 4            | a stack of conv layers with 3*3 kernels, stide = |<br>| <strong>Padding</strong>                      | None                          | 1                                        |<br>| <strong>Pooling</strong>                      | Overlapping Max Pooling       | Non-overlapping Max Pooling              |<br>| <strong>Local Response Normalization</strong> | Yes - it improves performance | No - it doesn’t improve performance but increase memory &amp; computation time |</p>
</li>
<li><p><strong>Training Schema</strong></p>
<p>|                    | AlexNet             | VGG                     |<br>| —————— | ——————- | ———————– |<br>| <strong>Batch size</strong>     | 128                 | <strong>256</strong>                 |<br>| <strong>Momentum</strong>       | 0.9                 | 0.9                     |<br>| <strong>Weight decay</strong>   | 0.0005              | 0.0005                  |<br>| <strong>Initialization</strong> | N(0,0.01); bias = 1 | N(0,0.01); <strong>bias = 0</strong> |</p>
</li>
</ul>
<h3 id="Main-Take-aways-1"><a href="#Main-Take-aways-1" class="headerlink" title="Main Take-aways"></a>Main Take-aways</h3><ul>
<li><p><strong>Deep CNN!</strong></p>
</li>
<li><p>Why would we use stack of <strong>multiple 3*3 Convolutional layers</strong> (without spatial pooling in between) instead of larger one layer kernel?</p>
<ul>
<li><p><strong>Same receptive filed.</strong></p>
<p>Receptive filed is well explained by <a href="https://www.quora.com/profile/Novel-Martis" target="_blank" rel="external">Novel Martis</a> in <a href="https://www.quora.com/What-is-a-CNN’s-receptive-field" target="_blank" rel="external">this post</a>. For example, in the below image, the input for B(2,2) is A(1:3, 1:3). The input for C(3,3) is B(2:4, 2:4) -&gt; A(1:5,1:5). The receptive field for 2 convolutional layers will be 5*5, and 3 convolutional layers will be 7*7.</p>
<p><img src="https://qph.ec.quoracdn.net/main-qimg-34686eb9aa41d84ec784164601174be5" alt="receptive field"></p>
<p>​                                <a href="https://www.quora.com/What-is-a-CNN’s-receptive-field" target="_blank" rel="external">Image Source</a></p>
</li>
<li><p><strong>More discriminative function.</strong></p>
<p>There are three ReLU layers instead of one.</p>
</li>
<li><p><strong>Less parameters.</strong></p>
<p>Suppose we have C channels. Stack of three 3*3 kernels will have $3<em>(3^2) = 27$ parameters, and one layer of 7\</em>7 kernel will have $7^2 = 49$ parameters; which is 81% more.</p>
</li>
</ul>
</li>
</ul>
<h2 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h2><p>Inception goes beyond the idea “we need to go deeper” but comes up with a “network-in-network” inception module. There are two drawbacks of the previous most popular deeper and wider neural network -  that it might easily go overfitting especially when there’re no enough training examples, and that it takes up too much computational resources. The authors are motivated to build more efficient yet accurate (or more accurate) algorithms by replacing the fully connected layers with dense structures.</p>
<h3 id="Statistics-2"><a href="#Statistics-2" class="headerlink" title="Statistics"></a>Statistics</h3><ul>
<li><p>Year: 2015</p>
</li>
<li><p>Data: 1.3 million images from ImageNet ILSVRC 2014</p>
<p>|               | Top 1 Error | Top 5 Error |<br>| ————- | ———– | ———– |<br>| VGG           | 23.7%       | 7.3%        |<br>| <strong>Inception</strong> |             | <strong>6.7%</strong>    |</p>
</li>
</ul>
<h3 id="Architecture-2"><a href="#Architecture-2" class="headerlink" title="Architecture"></a>Architecture</h3><p><img src="https://adeshpande3.github.io/assets/GoogLeNet2.png" alt="Inception"></p>
<p>​                                <a href="https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html" target="_blank" rel="external">Image source</a></p>
<ul>
<li><p><strong>Inception Module:</strong></p>
<p>So the green box above is an inception module, which could be presented as the picture below. The idea behind is that the authors try to find a dense structure that can best approximate the optimal local sparse structure. So the idea is well outlined in <a href="https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html" target="_blank" rel="external">Adit’s blog</a>: previously in stacked CNNs, different sizes of convolutional layers and max pooling layers are to be chosen; here we have them all. For example, if inside one picture, there is a person stands nearer the camera while there is a cat that is far away from the camera, it would be beneficial to have both of a larger image kernel to capture the nearer person and a smaller kernel to capture the cat. Therefore with the inception module, parameters are less, yet more powerful than simply stacked convolution.</p>
<p><img src="https://adeshpande3.github.io/assets/GoogLeNet3.png" alt="inception-block"></p>
<p>​                                <a href="https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html" target="_blank" rel="external">Image source</a></p>
</li>
</ul>
<h3 id="Main-Take-Aways"><a href="#Main-Take-Aways" class="headerlink" title="Main Take-Aways"></a>Main Take-Aways</h3><ul>
<li><p>The idea of CNN doesn’t have to be stacked up sequentially.</p>
</li>
<li><p>Get rid of fully connected layer (use average pooling instead) thus saves a lot of parameters and computational time.</p>
</li>
<li><p>The massive usage of <strong>1*1 convoluational kernel</strong>:</p>
<ul>
<li><p>Dimensional reduction</p>
<p>The 1*1 kernel is reducing a great amount of image  dimensions. For example, if there is 224*224*60 input that goes through a 1*1*10 image kernel, then the output size will just be 224*224*10. </p>
</li>
<li><p>Less parameters, less chance of overfitting</p>
</li>
</ul>
</li>
</ul>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>ResNet aims to solve the problem of <strong><em>degradation</em></strong>. </p>
<p>Basically, the authors of ReNet found that with increased number of layers, the accuracy get saturated thus degradation occurs. Previously the degragation was thought to be overfitting, but it isn’t: The training error <u>increase</u> rather than <u>decrease</u>. This is counterintuitive. The authors believe that, “the degradation problem (of training accuracy) suggests that the solvers might have difﬁculties in approximating identity mappings by multiple nonlinear layers.” Therefore he is motivated to create an easier way to optimize the deep CNNs.</p>
<h3 id="Statistics-3"><a href="#Statistics-3" class="headerlink" title="Statistics"></a>Statistics</h3><ul>
<li><p>Year: 2015</p>
</li>
<li><p>Data: ILSVRC 2015</p>
<p>|            | Top 5 Error                              |<br>| ———- | —————————————- |<br>| Inception  | 6.7%                                     |<br>| <strong>ResNet</strong> | <strong>3.6%</strong>  beats human recognition: 5%-10% |</p>
</li>
</ul>
<h3 id="Architecture-3"><a href="#Architecture-3" class="headerlink" title="Architecture"></a>Architecture</h3><p><img src="https://adeshpande3.github.io/assets/ResNet.gif" alt="ResNet"></p>
<p>​                                <a href="https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html" target="_blank" rel="external">Image Source</a></p>
<ul>
<li><p><strong>Residual Block</strong></p>
<p><img src="https://adeshpande3.github.io/assets/ResNet.png" alt="residual block"></p>
<p><a href="https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html" target="_blank" rel="external">Image Source</a></p>
<p>It performs <strong><em>shortcut  identity mapping</em></strong>. I will reference Adit’s wonderful explanation here:</p>
<blockquote>
<p>The idea behind a residual block is that you have your input x go through conv-relu-conv series. This will give you some F(x). That result is then added to the original input x. Let’s call that H(x) = F(x) + x. In traditional CNNs, your H(x) would just be equal to F(x) right? So, instead of just computing that transformation (straight from x to F(x)), we’re computing the term that you have to <em>add</em>, F(x), to your input, x. Basically, the mini module shown below is computing a “delta” or a slight change to the original input x to get a slightly altered representation (When we think of traditional CNNs, we go from x to F(x) which is a completely new representation that doesn’t keep any information about the original x). The authors believe that “it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping”.</p>
<p>Another reason for why this residual block might be effective is that during the backward pass of backpropagation, the gradient will flow easily through the graph because we have addition operations, which distributes the gradient.</p>
</blockquote>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html" target="_blank" rel="external">The 9 Deep Learning Papers You Need to Know About</a></li>
<li><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="external">An Intuitive Explanation of Convolutional Neural Networks</a></li>
<li><a href="http://cs231n.github.io" target="_blank" rel="external">CS231n Convolutional Neural Network for Visual Recognition</a> by Feifei Li</li>
<li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">AlexNet</a> by Alex Krizhevsky, Ilya Sutskever&amp; Geoffrey Hinton from University of Toronto</li>
<li><a href="https://arxiv.org/pdf/1409.1556v6.pdf" target="_blank" rel="external">VGG: Very Deep Convolutional Neural Networks for Large-Scale Image Recognition</a> by Visual Geometry Group, University of Oxford</li>
<li><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" target="_blank" rel="external">GoogLeNet: Going Deeper with Convolutions</a> by Google Inc</li>
<li><a href="https://arxiv.org/pdf/1512.03385v1.pdf" target="_blank" rel="external">ResNet: Deep Residual Learning for Image Recognition</a> by Microsoft</li>
<li><a href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4" target="_blank" rel="external">A Brief History of CNNs in Image Segmentation: from R-CNN to Mask R-CNN</a></li>
<li><a href="https://courses.cs.washington.edu/courses/cse590v/14au/cse590v_wk1_rcnn.pdf" target="_blank" rel="external">R-CNN for Object Detection</a></li>
</ul>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-learning/" rel="tag"># Machine learning</a>
          
            <a href="/tags/CNN/" rel="tag"># CNN</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/08/16/Sklearn-CV/" rel="next" title="Learn ML from Sklearn: Cross Validation">
                <i class="fa fa-chevron-left"></i> Learn ML from Sklearn: Cross Validation
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/08/08/Community-Detection/" rel="prev" title="Community Detection in Python">
                Community Detection in Python <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Yoyo" />
          <p class="site-author-name" itemprop="name">Yoyo</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">21</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">16</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#AlexNet"><span class="nav-number">1.</span> <span class="nav-text">AlexNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Statistics"><span class="nav-number">1.1.</span> <span class="nav-text">Statistics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Architecture"><span class="nav-number">1.2.</span> <span class="nav-text">Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Main-Take-aways"><span class="nav-number">1.3.</span> <span class="nav-text">Main Take-aways</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VGG"><span class="nav-number">2.</span> <span class="nav-text">VGG</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Statistics-1"><span class="nav-number">2.1.</span> <span class="nav-text">Statistics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Architecture-1"><span class="nav-number">2.2.</span> <span class="nav-text">Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Main-Take-aways-1"><span class="nav-number">2.3.</span> <span class="nav-text">Main Take-aways</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inception"><span class="nav-number">3.</span> <span class="nav-text">Inception</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Statistics-2"><span class="nav-number">3.1.</span> <span class="nav-text">Statistics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Architecture-2"><span class="nav-number">3.2.</span> <span class="nav-text">Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Main-Take-Aways"><span class="nav-number">3.3.</span> <span class="nav-text">Main Take-Aways</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ResNet"><span class="nav-number">4.</span> <span class="nav-text">ResNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Statistics-3"><span class="nav-number">4.1.</span> <span class="nav-text">Statistics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Architecture-3"><span class="nav-number">4.2.</span> <span class="nav-text">Architecture</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">5.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yoyo</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  




	





  





  





  






  





  

  

  

  

</body>
</html>
